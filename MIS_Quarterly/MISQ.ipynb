{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb5005a",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install selenium pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d07d1f8",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1003917",
      "metadata": {},
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "# Initialize the chrome webdriver\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# Starting URL - browse by year page\n",
        "START_URL = 'https://misq.umn.edu/misq/issue/browse-by-year'\n",
        "\n",
        "# Years to scrape (2010 to 2025)\n",
        "START_YEAR = 2010\n",
        "END_YEAR = 2025\n",
        "\n",
        "# Save CSV file in the same directory as this notebook (MIS_Quarterly folder)\n",
        "OUT_FILE = os.path.join(os.getcwd(), 'MISQ_Issues.csv')\n",
        "print(f\"CSV file will be saved to: {OUT_FILE}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\\n\")\n",
        "data = []\n",
        "\n",
        "def write_to_csv(rows):\n",
        "    file_exists = os.path.exists(OUT_FILE) and os.path.getsize(OUT_FILE) > 0\n",
        "    with open(OUT_FILE, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"Title\", \"URL\", \"Volume Issue\", \"Vol Issue Year\"])\n",
        "            print(f\"Created CSV file: {OUT_FILE}\")\n",
        "        writer.writerows(rows)\n",
        "        file.flush()  # Ensure data is written immediately\n",
        "    print(f\"  ✓ Saved {len(rows)} articles to {OUT_FILE}\")\n",
        "\n",
        "def scrape_issue_page(issue_url, vol_issue, year):\n",
        "    \"\"\"Scrape articles from a single issue page\"\"\"\n",
        "    try:\n",
        "        driver.get(issue_url)\n",
        "        driver.implicitly_wait(15)\n",
        "        time.sleep(2)\n",
        "        \n",
        "        print(f\"  Page loaded: {driver.title}\")\n",
        "        \n",
        "        # Find all article links - try multiple selectors\n",
        "        articles = []\n",
        "        \n",
        "        # Try different selectors for article links\n",
        "        selectors = [\n",
        "            'a[href*=\"/misq/vol\"]',\n",
        "            'a[href*=\"/article\"]',\n",
        "            '.article-title a',\n",
        "            '.article a',\n",
        "            'h3 a',\n",
        "            'h4 a',\n",
        "            '.title a',\n",
        "            'article a',\n",
        "            '.entry-title a',\n",
        "            'li a[href*=\"/article\"]',\n",
        "            'div.article a',\n",
        "            'table a[href*=\"/article\"]'\n",
        "        ]\n",
        "        \n",
        "        print(\"  Searching for articles...\")\n",
        "        for selector in selectors:\n",
        "            try:\n",
        "                found = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                if found:\n",
        "                    print(f\"    Selector '{selector}': Found {len(found)} links\")\n",
        "                    # Filter for article links (not issue links)\n",
        "                    filtered = [a for a in found if a.get_attribute('href') and \n",
        "                               ('/article' in a.get_attribute('href') or \n",
        "                                ('/misq/vol' in a.get_attribute('href') and '/issue' not in a.get_attribute('href')))]\n",
        "                    if filtered:\n",
        "                        print(f\"      → {len(filtered)} are article links\")\n",
        "                        articles.extend(filtered)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        \n",
        "        # Remove duplicates\n",
        "        seen_urls = set()\n",
        "        unique_articles = []\n",
        "        for article in articles:\n",
        "            try:\n",
        "                url = article.get_attribute('href')\n",
        "                if url and url not in seen_urls:\n",
        "                    seen_urls.add(url)\n",
        "                    unique_articles.append(article)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        if not unique_articles:\n",
        "            # Fallback: find all links and filter\n",
        "            print(\"  Trying fallback: checking all links...\")\n",
        "            all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "            for link in all_links:\n",
        "                try:\n",
        "                    url = link.get_attribute('href') or ''\n",
        "                    if url and ('/article' in url or ('/misq/vol' in url and '/issue' not in url)) and url not in seen_urls:\n",
        "                        seen_urls.add(url)\n",
        "                        unique_articles.append(link)\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        print(f\"  Total unique articles found: {len(unique_articles)}\")\n",
        "        \n",
        "        rows = []\n",
        "        for article in unique_articles:\n",
        "            try:\n",
        "                article_url = article.get_attribute('href')\n",
        "                if not article_url:\n",
        "                    continue\n",
        "                    \n",
        "                # Make sure URL is absolute\n",
        "                if article_url.startswith('/'):\n",
        "                    article_url = 'https://misq.umn.edu' + article_url\n",
        "                \n",
        "                if not article_url.startswith('http'):\n",
        "                    continue\n",
        "                \n",
        "                # Get article title\n",
        "                article_title = article.text.strip()\n",
        "                if not article_title or len(article_title) < 5:\n",
        "                    # Try to get title from parent or nearby element\n",
        "                    try:\n",
        "                        parent = article.find_element(By.XPATH, './..')\n",
        "                        article_title = parent.text.strip()\n",
        "                    except:\n",
        "                        try:\n",
        "                            # Try sibling or nearby heading\n",
        "                            heading = article.find_element(By.XPATH, './preceding-sibling::h3 | ./preceding-sibling::h4 | ./following-sibling::h3 | ./following-sibling::h4')\n",
        "                            article_title = heading.text.strip()\n",
        "                        except:\n",
        "                            article_title = \"N/A\"\n",
        "                \n",
        "                if article_url and article_title and article_title != \"N/A\" and len(article_title) > 5:\n",
        "                    rows.append([article_title, article_url, vol_issue, year])\n",
        "                    print(f\"    ✓ {article_title[:60]}...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"    Error extracting article: {e}\")\n",
        "                continue\n",
        "        \n",
        "        if rows:\n",
        "            write_to_csv(rows)\n",
        "            return len(rows)\n",
        "        else:\n",
        "            print(f\"  ⚠ No articles found on this page\")\n",
        "            # Debug: show page structure\n",
        "            try:\n",
        "                page_text = driver.find_element(By.TAG_NAME, 'body').text[:300]\n",
        "                print(f\"  Page content preview: {page_text[:200]}...\")\n",
        "            except:\n",
        "                pass\n",
        "            return 0\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error scraping issue page: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 0\n",
        "\n",
        "def scrape_year_page(year_url, year):\n",
        "    \"\"\"Scrape all issues from a year page\"\"\"\n",
        "    try:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Navigating to year page: {year_url}\")\n",
        "        driver.get(year_url)\n",
        "        driver.implicitly_wait(15)\n",
        "        time.sleep(3)  # Give page more time to load\n",
        "        \n",
        "        print(f\"Page title: {driver.title}\")\n",
        "        print(f\"Current URL: {driver.current_url}\")\n",
        "        \n",
        "        # Debug: Print some page content to understand structure\n",
        "        try:\n",
        "            page_text = driver.find_element(By.TAG_NAME, 'body').text[:500]\n",
        "            print(f\"Page content preview: {page_text}...\")\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Find all issue links - try comprehensive approach\n",
        "        issue_links = []\n",
        "        \n",
        "        # Try different selectors for issue links\n",
        "        selectors = [\n",
        "            'a[href*=\"/misq/vol\"]',\n",
        "            'a[href*=\"/vol\"]',\n",
        "            '.issue-link a',\n",
        "            '.issue a',\n",
        "            'h2 a',\n",
        "            'h3 a',\n",
        "            'h4 a',\n",
        "            'li a',\n",
        "            '.volume a',\n",
        "            'article a',\n",
        "            'div a[href*=\"/vol\"]',\n",
        "            'table a[href*=\"/vol\"]'\n",
        "        ]\n",
        "        \n",
        "        print(\"\\nTrying to find issue links...\")\n",
        "        for selector in selectors:\n",
        "            try:\n",
        "                links = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                if links:\n",
        "                    print(f\"  Selector '{selector}': Found {len(links)} links\")\n",
        "                    # Filter for actual issue links\n",
        "                    filtered = [l for l in links if l.get_attribute('href') and ('/misq/vol' in l.get_attribute('href') or '/vol' in l.get_attribute('href'))]\n",
        "                    if filtered:\n",
        "                        print(f\"    → {len(filtered)} are issue links\")\n",
        "                        issue_links.extend(filtered)\n",
        "            except Exception as e:\n",
        "                print(f\"  Selector '{selector}': Error - {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Remove duplicates\n",
        "        seen_urls = set()\n",
        "        unique_issue_links = []\n",
        "        for link in issue_links:\n",
        "            try:\n",
        "                url = link.get_attribute('href')\n",
        "                if url and url not in seen_urls:\n",
        "                    seen_urls.add(url)\n",
        "                    unique_issue_links.append(link)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        print(f\"\\nTotal unique issue links found: {len(unique_issue_links)}\")\n",
        "        \n",
        "        if not unique_issue_links:\n",
        "            # Fallback: find ALL links and filter\n",
        "            print(\"Trying fallback: checking all links on page...\")\n",
        "            all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "            print(f\"Total links on page: {len(all_links)}\")\n",
        "            \n",
        "            for link in all_links[:50]:  # Check first 50 links\n",
        "                try:\n",
        "                    url = link.get_attribute('href') or ''\n",
        "                    text = link.text.strip()\n",
        "                    if url and ('/misq/vol' in url or '/vol' in url) and url not in seen_urls:\n",
        "                        print(f\"  Found issue link: {text[:50]} -> {url}\")\n",
        "                        seen_urls.add(url)\n",
        "                        unique_issue_links.append(link)\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        # Extract unique issue URLs with metadata\n",
        "        unique_issues = {}\n",
        "        for link in unique_issue_links:\n",
        "            try:\n",
        "                url = link.get_attribute('href')\n",
        "                if not url:\n",
        "                    continue\n",
        "                    \n",
        "                # Make sure URL is absolute\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://misq.umn.edu' + url\n",
        "                \n",
        "                # Extract volume/issue from URL or text\n",
        "                link_text = link.text.strip()\n",
        "                \n",
        "                # Try to extract from URL\n",
        "                match = re.search(r'vol[^\\d]*(\\d+)[^\\d]*issue[^\\d]*(\\d+)', url, re.I)\n",
        "                if match:\n",
        "                    vol_issue = f\"Vol {match.group(1)}, Issue {match.group(2)}\"\n",
        "                elif link_text and len(link_text) > 3:\n",
        "                    vol_issue = link_text\n",
        "                else:\n",
        "                    # Extract from URL path\n",
        "                    parts = url.split('/')\n",
        "                    vol_issue = parts[-1] if parts else f\"Vol {year}\"\n",
        "                \n",
        "                unique_issues[url] = vol_issue\n",
        "                print(f\"  Issue: {vol_issue} -> {url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing link: {e}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing {len(unique_issues)} issues for year {year}...\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        if len(unique_issues) == 0:\n",
        "            print(f\"WARNING: No issues found for year {year}!\")\n",
        "            print(\"Page HTML snippet:\")\n",
        "            try:\n",
        "                html_snippet = driver.page_source[:1000]\n",
        "                print(html_snippet)\n",
        "            except:\n",
        "                pass\n",
        "            return 0\n",
        "        \n",
        "        total_articles = 0\n",
        "        for issue_url, vol_issue in unique_issues.items():\n",
        "            print(f\"\\n{'─'*60}\")\n",
        "            print(f\"Scraping: {vol_issue}\")\n",
        "            print(f\"URL: {issue_url}\")\n",
        "            count = scrape_issue_page(issue_url, vol_issue, str(year))\n",
        "            total_articles += count\n",
        "            print(f\"  → Found {count} articles\")\n",
        "            time.sleep(1)  # Be respectful\n",
        "        \n",
        "        return total_articles\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping year page {year_url}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 0\n",
        "\n",
        "# Main scraping logic\n",
        "try:\n",
        "    driver.get(START_URL)\n",
        "    driver.implicitly_wait(15)\n",
        "    time.sleep(2)\n",
        "    \n",
        "    print(\"Starting MIS Quarterly scraper (2010-2025)...\")\n",
        "    print(f\"Browse page: {START_URL}\\n\")\n",
        "    \n",
        "    # Find year links for 2010-2025\n",
        "    year_links = {}\n",
        "    \n",
        "    print(\"Searching for year links on browse-by-year page...\")\n",
        "    print(f\"Page title: {driver.title}\")\n",
        "    print(f\"Current URL: {driver.current_url}\\n\")\n",
        "    \n",
        "    # Get all links and filter by year\n",
        "    all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "    print(f\"Total links found on page: {len(all_links)}\")\n",
        "    \n",
        "    # First pass: look for links with year in URL or text\n",
        "    for link in all_links:\n",
        "        try:\n",
        "            url = link.get_attribute('href') or ''\n",
        "            text = link.text.strip()\n",
        "            \n",
        "            # Make URL absolute if relative\n",
        "            if url.startswith('/'):\n",
        "                url = 'https://misq.umn.edu' + url\n",
        "            \n",
        "            # Check if link contains year in URL or text\n",
        "            for year in range(START_YEAR, END_YEAR + 1):\n",
        "                year_str = str(year)\n",
        "                if (year_str in url or year_str in text) and url.startswith('http'):\n",
        "                    if year not in year_links:\n",
        "                        year_links[year] = url\n",
        "                        print(f\"  ✓ Found year {year}: {text[:40]} -> {url}\")\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\nFound {len(year_links)} year links directly from page\")\n",
        "    \n",
        "    # If we didn't find enough year links, try to construct URLs\n",
        "    if len(year_links) < (END_YEAR - START_YEAR + 1) / 2:  # If less than half found\n",
        "        print(\"\\nTrying to construct year URLs...\")\n",
        "        # Common patterns for year pages\n",
        "        base_patterns = [\n",
        "            'https://misq.umn.edu/misq/issue/browse-by-year/{}',\n",
        "            'https://misq.umn.edu/misq/issue/{}',\n",
        "            'https://misq.umn.edu/misq/vol/{}'\n",
        "        ]\n",
        "        \n",
        "        for year in range(START_YEAR, END_YEAR + 1):\n",
        "            if year in year_links:\n",
        "                continue  # Skip if already found\n",
        "                \n",
        "            for pattern in base_patterns:\n",
        "                test_url = pattern.format(year)\n",
        "                try:\n",
        "                    driver.get(test_url)\n",
        "                    time.sleep(1)\n",
        "                    if '404' not in driver.title.lower() and 'not found' not in driver.title.lower():\n",
        "                        year_links[year] = test_url\n",
        "                        print(f\"  ✓ Constructed year {year}: {test_url}\")\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "    \n",
        "    # Go back to browse page\n",
        "    driver.get(START_URL)\n",
        "    time.sleep(2)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Total year links found: {len(year_links)}\")\n",
        "    print(f\"Years: {sorted(year_links.keys())}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Scrape each year\n",
        "    total_articles_scraped = 0\n",
        "    for year in sorted(year_links.keys()):\n",
        "        if START_YEAR <= year <= END_YEAR:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Scraping year {year}\")\n",
        "            print(f\"{'='*60}\")\n",
        "            count = scrape_year_page(year_links[year], year)\n",
        "            total_articles_scraped += count\n",
        "            print(f\"Year {year}: {count} articles scraped\")\n",
        "            time.sleep(2)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Scraping complete!\")\n",
        "    print(f\"Total articles scraped: {total_articles_scraped}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Exception: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a1ad51d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2: Scraping MISQ article details (ONE ROW PER AUTHOR)\n",
            "Reading from: /Users/keerthisagi/Documents/Journals/MIS_Quarterly/MISQ_Issues.csv\n",
            "Writing to: /Users/keerthisagi/Documents/Journals/MIS_Quarterly/MISQ_article_data.csv\n",
            "Total input rows: 1273\n",
            "Processing: 0 -> 5\n",
            "\n",
            "[1/5] ✓ Information Systems Innovation for Environmental Sustainabil... (1 author(s))\n",
            "[2/5] ✓ Information Systems and Environmentally Sustainable Developm... (1 author(s))\n",
            "[3/5] ✓ An Empirical Analysis of the Impact of Information Capabilit... (1 author(s))\n",
            "[4/5] ✓ Chasing the Hottest IT: Effects of Information Technology Fa... (1 author(s))\n",
            "[5/5] ✓ Toward Agile: An Integrated Analysis of Quantitative and Qua... (1 author(s))\n",
            "\n",
            "DONE. Saved to: /Users/keerthisagi/Documents/Journals/MIS_Quarterly/MISQ_article_data.csv\n"
          ]
        }
      ],
      "source": [
        "# ========== STEP 2: Scrape detailed data for each article ==========\n",
        "# Reads MISQ_Issues.csv, visits each URL, extracts title/abstract/keywords/authors.\n",
        "# Writes MISQ_article_data.csv with ONE ROW PER AUTHOR.\n",
        "# JUPYTER-SAFE: NO input() prompts. If verification shows up, solve it in Chrome and code auto-continues.\n",
        "\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "# -----------------------\n",
        "# CONFIG\n",
        "# -----------------------\n",
        "START_INDEX = 0\n",
        "END_INDEX = 5  # set to len(journals_data) for all\n",
        "WAIT_SEC = 20\n",
        "\n",
        "# pacing (faster -> more bot; slower -> fewer bot)\n",
        "SLEEP_MIN = 2.5\n",
        "SLEEP_MAX = 5.5\n",
        "\n",
        "# bot wait loop (you solve manually in browser; code waits/polls)\n",
        "BOT_MAX_WAIT_SEC = 15 * 60   # 15 minutes\n",
        "BOT_POLL_SEC = 5\n",
        "\n",
        "CSV_PATH = os.path.join(os.getcwd(), \"MISQ_Issues.csv\")\n",
        "OUT_FILE = os.path.join(os.getcwd(), \"MISQ_article_data.csv\")\n",
        "\n",
        "# -----------------------\n",
        "# LOAD INPUT\n",
        "# -----------------------\n",
        "journals_data = pd.read_csv(CSV_PATH)\n",
        "n_total = len(journals_data)\n",
        "END_INDEX = min(END_INDEX, n_total)\n",
        "\n",
        "print(\"Step 2: Scraping MISQ article details (ONE ROW PER AUTHOR)\")\n",
        "print(f\"Reading from: {CSV_PATH}\")\n",
        "print(f\"Writing to: {OUT_FILE}\")\n",
        "print(f\"Total input rows: {n_total}\")\n",
        "print(f\"Processing: {START_INDEX} -> {END_INDEX}\\n\")\n",
        "\n",
        "# -----------------------\n",
        "# OUTPUT HEADER\n",
        "# -----------------------\n",
        "if not os.path.exists(OUT_FILE) or os.path.getsize(OUT_FILE) == 0:\n",
        "    with open(OUT_FILE, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\n",
        "            \"URL\",\"Journal_Title\",\"Article_Title\",\"Volume_Issue\",\"Month_Year\",\n",
        "            \"Abstract\",\"Keywords\",\"Author_name\",\"Author_email\",\"Author_Address\"\n",
        "        ])\n",
        "\n",
        "# -----------------------\n",
        "# CHROME SETUP (single session + persistent profile)\n",
        "# -----------------------\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--user-data-dir=/tmp/misq_profile\")\n",
        "chrome_options.add_argument(\"--profile-directory=Default\")\n",
        "\n",
        "# Lighter loads\n",
        "prefs = {\n",
        "    \"profile.managed_default_content_settings.images\": 2,\n",
        "    \"profile.managed_default_content_settings.stylesheets\": 2,\n",
        "    \"profile.managed_default_content_settings.fonts\": 2,\n",
        "}\n",
        "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "wait = WebDriverWait(driver, WAIT_SEC)\n",
        "\n",
        "# -----------------------\n",
        "# HELPERS\n",
        "# -----------------------\n",
        "def is_bot_page(driver) -> bool:\n",
        "    \"\"\"\n",
        "    Only treat as bot/verification page when title or URL clearly indicate it.\n",
        "    Do NOT check full page_source - normal article pages often contain words\n",
        "    like 'verify', 'cloudflare', 'verification' in scripts/footers and cause false positives.\n",
        "    \"\"\"\n",
        "    title = (driver.title or \"\").lower()\n",
        "    url = (driver.current_url or \"\").lower()\n",
        "    # Only check title and URL to avoid false \"bot detected\" on real article pages\n",
        "    signals = [\n",
        "        \"captcha\",\n",
        "        \"are you human\",\n",
        "        \"checking your browser\",\n",
        "        \"access denied\",\n",
        "        \"security check\",\n",
        "        \"please enable cookies\",\n",
        "        \"crawlprevention\",\n",
        "        \"blocked\",\n",
        "        \"challenge\"\n",
        "    ]\n",
        "    if any(s in title for s in signals) or any(s in url for s in signals):\n",
        "        return True\n",
        "    # Optional: only if page is clearly a minimal challenge page (very short body)\n",
        "    try:\n",
        "        body_text = (driver.find_element(By.TAG_NAME, \"body\").text or \"\").lower()\n",
        "        if len(body_text) < 500 and (\"verify\" in body_text or \"human\" in body_text or \"security check\" in body_text):\n",
        "            return True\n",
        "    except Exception:\n",
        "        pass\n",
        "    return False\n",
        "\n",
        "def wait_until_unblocked(driver, max_wait_sec=BOT_MAX_WAIT_SEC, poll_sec=BOT_POLL_SEC) -> bool:\n",
        "    \"\"\"\n",
        "    If verification page appears, you solve it manually in the open Chrome window.\n",
        "    This function polls until verification is gone or timeout.\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    warned = False\n",
        "    while is_bot_page(driver):\n",
        "        if not warned:\n",
        "            print(\"⚠ Verification detected. Please complete it in the Chrome window.\")\n",
        "            print(\"   Waiting automatically (no ENTER needed)...\")\n",
        "            warned = True\n",
        "        if time.time() - start > max_wait_sec:\n",
        "            return False\n",
        "        time.sleep(poll_sec)\n",
        "    return True\n",
        "\n",
        "def first_text(driver, selectors, min_len=3, default=\"N/A\"):\n",
        "    for sel in selectors:\n",
        "        try:\n",
        "            el = driver.find_element(By.CSS_SELECTOR, sel)\n",
        "            txt = (el.text or \"\").strip()\n",
        "            if txt and len(txt) >= min_len:\n",
        "                return txt\n",
        "        except:\n",
        "            continue\n",
        "    return default\n",
        "\n",
        "def extract_keywords_str(driver):\n",
        "    selectors = [\n",
        "        \".keyword\",\n",
        "        \".keywords span\",\n",
        "        \".keywords a\",\n",
        "        \".tag\",\n",
        "        \"[class*='keyword'] span\",\n",
        "        \"[class*='keyword'] a\"\n",
        "    ]\n",
        "    kws = []\n",
        "    seen = set()\n",
        "    for sel in selectors:\n",
        "        try:\n",
        "            for el in driver.find_elements(By.CSS_SELECTOR, sel):\n",
        "                t = (el.text or \"\").strip()\n",
        "                if not t:\n",
        "                    continue\n",
        "                k = t.lower()\n",
        "                if k not in seen:\n",
        "                    seen.add(k)\n",
        "                    kws.append(t)\n",
        "        except:\n",
        "            continue\n",
        "    return \"; \".join(kws) if kws else \"N/A\"\n",
        "\n",
        "def _split_author_block(text):\n",
        "    \"\"\"Split a byline like 'Name1, Name2, and Name3' into individual author names.\"\"\"\n",
        "    if not text or len(text) < 3:\n",
        "        return []\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", str(text).strip())\n",
        "\n",
        "    # Strip common label prefixes\n",
        "    text = re.sub(r\"^(authors?|by|by:)\\s*:?\\s*\", \"\", text, flags=re.I)\n",
        "\n",
        "    # Split on common separators: comma, and, newline-ish bullets\n",
        "    parts = re.split(r\",\\s*|\\s+and\\s+|\\s*[•·\\|/]\\s*|\\s*;\\s*\", text, flags=re.I)\n",
        "\n",
        "    names = []\n",
        "    for p in parts:\n",
        "        p = \" \".join(p.split()).strip()\n",
        "        if not p:\n",
        "            continue\n",
        "        # Remove trailing footnote/affiliation markers like 1, *, †, ‡\n",
        "        p = re.sub(r\"[\\s\\d\\*†‡]+$\", \"\", p).strip()\n",
        "        if 2 <= len(p) <= 150 and not p.lower().startswith((\"author\", \"authors\", \"article\", \"additional details\")):\n",
        "            names.append(p)\n",
        "\n",
        "    # Dedupe while preserving order\n",
        "    out = []\n",
        "    seen_local = set()\n",
        "    for n in names:\n",
        "        low = n.lower()\n",
        "        if low not in seen_local:\n",
        "            seen_local.add(low)\n",
        "            out.append(n)\n",
        "\n",
        "    return out\n",
        "\n",
        "def get_authors_per_author(driver):\n",
        "    \"\"\"\n",
        "    ONE ROW PER AUTHOR.\n",
        "    Returns list of [Author_name, Author_email, Author_Address].\n",
        "\n",
        "    Notes for MISQ:\n",
        "    - Emails are often NOT published; we can only capture them if they appear as mailto links or visible text.\n",
        "    - \"Address\" is treated as affiliation/institution text when available.\n",
        "    \"\"\"\n",
        "\n",
        "    email_re = re.compile(r\"[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\", re.I)\n",
        "\n",
        "    def clean_name(s: str) -> str:\n",
        "        s = re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
        "        s = re.sub(r\"[\\s\\d\\*†‡]+$\", \"\", s).strip()\n",
        "        return s\n",
        "\n",
        "    def extract_email(scope_el=None) -> str:\n",
        "        try:\n",
        "            root = scope_el if scope_el is not None else driver\n",
        "            for a in root.find_elements(By.CSS_SELECTOR, \"a[href^='mailto:']\"):\n",
        "                href = (a.get_attribute(\"href\") or \"\").strip()\n",
        "                if href.lower().startswith(\"mailto:\"):\n",
        "                    addr = href.split(\"mailto:\", 1)[1].split(\"?\", 1)[0].strip()\n",
        "                    if addr and email_re.search(addr):\n",
        "                        return addr\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            txt = (scope_el.text if scope_el is not None else driver.find_element(By.TAG_NAME, \"body\").text) or \"\"\n",
        "            m = email_re.search(txt)\n",
        "            return m.group(0) if m else \"N/A\"\n",
        "        except Exception:\n",
        "            return \"N/A\"\n",
        "\n",
        "    def extract_affiliation(scope_el=None) -> str:\n",
        "        sels = [\n",
        "            \".affiliation\",\n",
        "            \"[class*='affil']\",\n",
        "            \"[itemprop='affiliation']\",\n",
        "            \"[class*='institution']\",\n",
        "            \".author-affiliation\",\n",
        "            \".contributor-affiliation\",\n",
        "            \".hlFld-affiliation\",\n",
        "        ]\n",
        "        texts = []\n",
        "        seen_local = set()\n",
        "        try:\n",
        "            root = scope_el if scope_el is not None else driver\n",
        "            for sel in sels:\n",
        "                for el in root.find_elements(By.CSS_SELECTOR, sel):\n",
        "                    t = clean_name(el.text)\n",
        "                    if t and len(t) >= 3:\n",
        "                        low = t.lower()\n",
        "                        if low not in seen_local:\n",
        "                            seen_local.add(low)\n",
        "                            texts.append(t)\n",
        "        except Exception:\n",
        "            pass\n",
        "        return \" | \".join(texts) if texts else \"N/A\"\n",
        "\n",
        "    skip = {\n",
        "        \"n/a\", \"author\", \"authors\", \"author & article information\",\n",
        "        \"author information\", \"article information\", \"view article\", \"abstract\", \"pdf\",\n",
        "        \"additional details\", \"published\", \"received\", \"accepted\"\n",
        "    }\n",
        "\n",
        "    # Strategy A: author containers (best chance to map name ↔ email ↔ affiliation)\n",
        "    container_selectors = [\n",
        "        \".loa-author\",\n",
        "        \"li.loa-author\",\n",
        "        \"li.author\",\n",
        "        \".author\",\n",
        "        \".contributor\",\n",
        "        \"[itemprop='author']\",\n",
        "        \"[class*='author'] li\",\n",
        "        \"[class*='authors'] li\",\n",
        "    ]\n",
        "    author_rows = []\n",
        "    seen_names = set()\n",
        "    for sel in container_selectors:\n",
        "        try:\n",
        "            containers = driver.find_elements(By.CSS_SELECTOR, sel)\n",
        "            for c in containers:\n",
        "                # try to get name from child first\n",
        "                name = \"\"\n",
        "                for nsel in [\".author-name\", \".contributor-name\", \"[itemprop='name']\", \"a[rel='author']\", \"a\"]:\n",
        "                    try:\n",
        "                        t = clean_name(c.find_element(By.CSS_SELECTOR, nsel).text)\n",
        "                        if 2 <= len(t) <= 150:\n",
        "                            name = t\n",
        "                            break\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                if not name:\n",
        "                    # fallback: take first plausible name from container text\n",
        "                    candidates = _split_author_block(c.text)\n",
        "                    name = clean_name(candidates[0]) if candidates else \"\"\n",
        "\n",
        "                low = name.lower()\n",
        "                if not name or low in skip or low in seen_names:\n",
        "                    continue\n",
        "\n",
        "                email = extract_email(c)\n",
        "                affil = extract_affiliation(c)\n",
        "\n",
        "                seen_names.add(low)\n",
        "                author_rows.append([name, email if email else \"N/A\", affil if affil else \"N/A\"])\n",
        "\n",
        "            if author_rows:\n",
        "                return author_rows\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    # Strategy B: names first (byline/table), then best-effort global email/affiliation (may be N/A)\n",
        "    seen = set()\n",
        "\n",
        "    one_per_author_selectors = [\n",
        "        \".author-name\",\n",
        "        \".contributor-name\",\n",
        "        \"[itemprop='author']\",\n",
        "        \".byline a\",\n",
        "        \".authors a\",\n",
        "        \".author a\",\n",
        "        \".contributor a\",\n",
        "        \"a[rel='author']\",\n",
        "        \".citation-author\",\n",
        "        \".loa-author span\",\n",
        "        \".author-list span\",\n",
        "        \".hlFld-author\",\n",
        "    ]\n",
        "\n",
        "    names = []\n",
        "    for sel in one_per_author_selectors:\n",
        "        try:\n",
        "            els = driver.find_elements(By.CSS_SELECTOR, sel)\n",
        "            for el in els:\n",
        "                txt = clean_name(el.text)\n",
        "                if not txt or len(txt) < 2 or len(txt) > 500:\n",
        "                    continue\n",
        "\n",
        "                split_names = _split_author_block(txt)\n",
        "                if len(split_names) > 1:\n",
        "                    for n in split_names:\n",
        "                        n = clean_name(n)\n",
        "                        low = n.lower()\n",
        "                        if not n or low in skip or low in seen:\n",
        "                            continue\n",
        "                        seen.add(low)\n",
        "                        names.append(n)\n",
        "                    continue\n",
        "\n",
        "                low = txt.lower()\n",
        "                if low in skip or low in seen:\n",
        "                    continue\n",
        "                seen.add(low)\n",
        "                names.append(txt)\n",
        "\n",
        "            if names:\n",
        "                break\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if not names:\n",
        "        # table 'Author(s)' row\n",
        "        try:\n",
        "            for row in driver.find_elements(By.CSS_SELECTOR, \"tr\"):\n",
        "                cells = row.find_elements(By.TAG_NAME, \"td\")\n",
        "                if len(cells) < 2:\n",
        "                    continue\n",
        "                label = clean_name(cells[0].text).lower()\n",
        "                if \"author\" not in label:\n",
        "                    continue\n",
        "                value = clean_name(cells[1].text)\n",
        "                for n in _split_author_block(value):\n",
        "                    n = clean_name(n)\n",
        "                    low = n.lower()\n",
        "                    if not n or low in skip or low in seen:\n",
        "                        continue\n",
        "                    seen.add(low)\n",
        "                    names.append(n)\n",
        "                if names:\n",
        "                    break\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if not names:\n",
        "        return [[\"N/A\", \"N/A\", \"N/A\"]]\n",
        "\n",
        "    # Best-effort: capture page-level emails/affiliations (mapping per author usually not available)\n",
        "    page_email = extract_email(None)\n",
        "    page_affil = extract_affiliation(None)\n",
        "    return [[n, page_email, page_affil] for n in names]\n",
        "\n",
        "def _safe_get(row, *keys):\n",
        "    \"\"\"Get first existing column value (handles CSV header variations).\"\"\"\n",
        "    for k in keys:\n",
        "        if k in row.index:\n",
        "            val = row.get(k, \"N/A\")\n",
        "            return \"N/A\" if pd.isna(val) else str(val).strip()\n",
        "    return \"N/A\"\n",
        "\n",
        "def _write_article_rows(out_path, url, journal, title, vol_issue, month_year, abstract, keywords_str, authors):\n",
        "    \"\"\"Write one row per author to CSV; always at least one row.\"\"\"\n",
        "    final_data = [url, journal, title, vol_issue, month_year, abstract, keywords_str]\n",
        "    if not authors:\n",
        "        authors = [[\"N/A\", \"N/A\", \"N/A\"]]\n",
        "    with open(out_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        for arow in authors:\n",
        "            w.writerow(final_data + arow)\n",
        "        f.flush()\n",
        "\n",
        "# -----------------------\n",
        "# MAIN LOOP\n",
        "# -----------------------\n",
        "try:\n",
        "    for pos, (index, row) in enumerate(journals_data.iloc[START_INDEX:END_INDEX].iterrows(), start=1):\n",
        "\n",
        "        url = str(row.get(\"URL\", \"\") or \"\").strip()\n",
        "        month_year = _safe_get(row, \"Vol Issue Year\", \"Year\")\n",
        "        volume_issue = _safe_get(row, \"Volume Issue\")\n",
        "\n",
        "        if not url or not url.startswith(\"http\"):\n",
        "            print(f\"[{pos}] Skipping row (no URL)\")\n",
        "            continue\n",
        "\n",
        "        # polite pacing (helps bot + avoids hammering)\n",
        "        time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))\n",
        "\n",
        "        try:\n",
        "            driver.get(url)\n",
        "\n",
        "            # basic load\n",
        "            try:\n",
        "                wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
        "            except:\n",
        "                pass\n",
        "            time.sleep(1)\n",
        "\n",
        "            # if bot, wait until you solve (only when title/URL clearly indicate challenge)\n",
        "            if is_bot_page(driver):\n",
        "                ok = wait_until_unblocked(driver)\n",
        "                if not ok:\n",
        "                    print(f\"[{pos}] Timed out on verification. Writing placeholder row: {url}\")\n",
        "                    _write_article_rows(OUT_FILE, url, \"MIS Quarterly\", \"N/A\", volume_issue, month_year, \"N/A\", \"N/A\", [[\"N/A\", \"N/A\", \"N/A\"]])\n",
        "                    continue\n",
        "\n",
        "                driver.get(url)\n",
        "                time.sleep(2)\n",
        "\n",
        "                if is_bot_page(driver):\n",
        "                    print(f\"[{pos}] Still blocked. Writing placeholder row: {url}\")\n",
        "                    _write_article_rows(OUT_FILE, url, \"MIS Quarterly\", \"N/A\", volume_issue, month_year, \"N/A\", \"N/A\", [[\"N/A\", \"N/A\", \"N/A\"]])\n",
        "                    continue\n",
        "\n",
        "            # wait for content-ish\n",
        "            try:\n",
        "                wait.until(\n",
        "                    EC.any_of(\n",
        "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"h1\")),\n",
        "                        EC.presence_of_element_located((By.CSS_SELECTOR, \".article-title\")),\n",
        "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"article h1\"))\n",
        "                    )\n",
        "                )\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            title = first_text(driver, [\"h1\", \".article-title\", \"article h1\", \".title\", \".citation-title\", \"h1.citation-title\"], min_len=3, default=\"N/A\")\n",
        "\n",
        "            abstract = first_text(\n",
        "                driver,\n",
        "                [\".abstract\", \"#abstract\", \".article-abstract\", \"section.abstract\", \"[class*='abstract']\"],\n",
        "                min_len=20,\n",
        "                default=\"N/A\"\n",
        "            )\n",
        "\n",
        "            keywords_str = extract_keywords_str(driver)\n",
        "            authors = get_authors_per_author(driver)\n",
        "\n",
        "            _write_article_rows(OUT_FILE, url, \"MIS Quarterly\", title, volume_issue, month_year, abstract, keywords_str, authors)\n",
        "\n",
        "            print(f\"[{pos}/{END_INDEX-START_INDEX}] ✓ {title[:60]}... ({len(authors)} author(s))\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[{pos}] ✗ Error on {url}: {e}\")\n",
        "            _write_article_rows(OUT_FILE, url, \"MIS Quarterly\", \"N/A\", volume_issue, month_year, \"N/A\", \"N/A\", [[\"N/A\", \"N/A\", \"N/A\"]])\n",
        "\n",
        "finally:\n",
        "    driver.quit()\n",
        "    print(f\"\\nDONE. Saved to: {OUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77b091bc",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0decfb17",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
