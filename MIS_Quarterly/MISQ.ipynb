{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb5005a",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install selenium pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d07d1f8",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0decfb17",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e5d2eec4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2: Scraping article details (one row per author)...\n",
            "Reading from: /Users/keerthisagi/Documents/Journals/MIS_Quarterly/MISQ_Issues.csv\n",
            "Writing to: /Users/keerthisagi/Documents/Journals/MIS_Quarterly/MISQ_article_data.csv\n",
            "\n",
            "Total articles: 1273\n",
            "Processing range: 0 to 5\n",
            "\n",
            "[1] Bot detection - waiting 30s...\n",
            "[1/5] Information Systems Innovation for Environmental S... (1 author(s))\n",
            "[2/5] Information Systems and Environmentally Sustainabl... (3 author(s))\n",
            "[3/5] An Empirical Analysis of the Impact of Information... (3 author(s))\n",
            "[4/5] Chasing the Hottest IT: Effects of Information Tec... (1 author(s))\n",
            "[5/5] Toward Agile: An Integrated Analysis of Quantitati... (2 author(s))\n",
            "\n",
            "Step 2 complete. Data saved to /Users/keerthisagi/Documents/Journals/MIS_Quarterly/MISQ_article_data.csv\n"
          ]
        }
      ],
      "source": [
        "# ========== STEP 2: Scrape detailed data for each article ==========\n",
        "# Reads MISQ_Issues.csv, visits each URL, extracts title/abstract/keywords/authors.\n",
        "# Writes MISQ_article_data.csv with ONE ROW PER AUTHOR (same article info, author name changes).\n",
        "# Run Step 1 first to create MISQ_Issues.csv.\n",
        "\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "START_INDEX = 0\n",
        "END_INDEX = 5  # Process in batches; use len(journals_data) for all\n",
        "\n",
        "OUT_DIR = os.getcwd()\n",
        "CSV_PATH = os.path.join(OUT_DIR, 'MISQ_Issues.csv')\n",
        "OUT_FILE = os.path.join(OUT_DIR, 'MISQ_article_data.csv')\n",
        "\n",
        "print(\"Step 2: Scraping article details (one row per author)...\")\n",
        "print(f\"Reading from: {CSV_PATH}\")\n",
        "print(f\"Writing to: {OUT_FILE}\\n\")\n",
        "\n",
        "journals_data = pd.read_csv(CSV_PATH)\n",
        "n_total = len(journals_data)\n",
        "print(f\"Total articles: {n_total}\")\n",
        "print(f\"Processing range: {START_INDEX} to {min(END_INDEX, n_total)}\\n\")\n",
        "\n",
        "if not os.path.exists(OUT_FILE) or os.path.getsize(OUT_FILE) == 0:\n",
        "    with open(OUT_FILE, 'a', newline='', encoding='utf-8') as f:\n",
        "        csv.writer(f).writerow(['URL','Journal_Title','Article_Title','Volume_Issue','Month_Year','Abstract','Keywords','Author_name','Author_email','Author_Address'])\n",
        "\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "\n",
        "def is_bot_page(driver):\n",
        "    title = (driver.title or '').lower()\n",
        "    url = (driver.current_url or '').lower()\n",
        "    return 'validate' in title or 'bot' in title or 'captcha' in title or 'crawlprevention' in url\n",
        "\n",
        "def get_authors(driver):\n",
        "    \"\"\"One row per author. Splits combined text (e.g. 'A1\\\\nA2\\\\nA3') into individual names.\"\"\"\n",
        "    authdata = []\n",
        "    seen = set()\n",
        "    skip = {'n/a', 'author', 'authors', 'author & article information', 'author information', 'article information'}\n",
        "    try:\n",
        "        selectors = ['.author-name', '.author .name', '[itemprop=\"author\"]', '.byline .author', '.contributor-name',\n",
        "                     '.author, .contributor, [class*=\"author\"], .byline span']\n",
        "        all_parts = []\n",
        "        for sel in selectors:\n",
        "            try:\n",
        "                for el in driver.find_elements(By.CSS_SELECTOR, sel):\n",
        "                    text = (el.text or '').strip()\n",
        "                    if not text or len(text) < 3:\n",
        "                        continue\n",
        "                    for part in re.split(r'[\\n;]+', text):\n",
        "                        part = part.strip()\n",
        "                        if part and 3 <= len(part) <= 80:\n",
        "                            all_parts.append(part)\n",
        "                if all_parts:\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "        for name in all_parts:\n",
        "            name = ' '.join(name.split()).replace(';', '').strip()\n",
        "            if not name or len(name) < 3 or len(name) > 80 or name.lower() in skip or name.startswith('Author') or name.startswith('Article'):\n",
        "                continue\n",
        "            key = name.lower()\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                authdata.append([name, \"N/A\", \"N/A\"])\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting authors: {e}\")\n",
        "    return authdata if authdata else [[\"N/A\", \"N/A\", \"N/A\"]]\n",
        "\n",
        "try:\n",
        "    for index, row in journals_data.iloc[START_INDEX:END_INDEX].iterrows():\n",
        "        url = str(row.get('URL', '')).strip()\n",
        "        article_date = row.get('Vol Issue Year', None)\n",
        "        article_vol = row.get('Volume Issue', 'N/A')\n",
        "        if not url or not url.startswith('http'):\n",
        "            continue\n",
        "\n",
        "        title, abstract, keyword_list = \"N/A\", None, []\n",
        "\n",
        "        try:\n",
        "            time.sleep(random.uniform(3, 7))\n",
        "            driver.get(url)\n",
        "            driver.implicitly_wait(10)\n",
        "            time.sleep(random.uniform(1, 3))\n",
        "\n",
        "            if is_bot_page(driver):\n",
        "                print(f\"[{index-START_INDEX+1}] Bot detection - waiting 30s...\")\n",
        "                time.sleep(30)\n",
        "                driver.get(url)\n",
        "                time.sleep(3)\n",
        "                if is_bot_page(driver):\n",
        "                    print(\"  Still blocked. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "            for sel in ['h1', '.article-title', '.title', 'article h1']:\n",
        "                try:\n",
        "                    title = driver.find_element(By.CSS_SELECTOR, sel).text.strip()\n",
        "                    if title and len(title) > 3:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            for sel in ['.abstract', '#abstract', '.article-abstract', 'section.abstract', '[class*=\"abstract\"]']:\n",
        "                try:\n",
        "                    abstract = driver.find_element(By.CSS_SELECTOR, sel).text.strip()\n",
        "                    if abstract and len(abstract) > 20:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            try:\n",
        "                keyword_list = [k.text.strip() for k in driver.find_elements(By.CSS_SELECTOR, '.keyword, .keywords span, .tag, [class*=\"keyword\"] span') if k.text.strip()]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            auth_data = get_authors(driver)\n",
        "            final_data = [url, \"MIS Quarterly\", title, article_vol, article_date, abstract, keyword_list]\n",
        "\n",
        "            with open(OUT_FILE, 'a', newline='', encoding='utf-8') as f:\n",
        "                writer = csv.writer(f)\n",
        "                for auth in auth_data:\n",
        "                    writer.writerow(final_data + auth)\n",
        "                f.flush()\n",
        "\n",
        "            print(f\"[{index-START_INDEX+1}/{min(END_INDEX, n_total)-START_INDEX}] {title[:50]}... ({len(auth_data)} author(s))\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[{index-START_INDEX+1}] Error: {e}\")\n",
        "            with open(OUT_FILE, 'a', newline='', encoding='utf-8') as f:\n",
        "                csv.writer(f).writerow([url, \"MIS Quarterly\", \"N/A\", article_vol, article_date, None, [], \"N/A\", \"N/A\", \"N/A\"])\n",
        "                f.flush()\n",
        "\n",
        "finally:\n",
        "    driver.quit()\n",
        "    print(f\"\\nStep 2 complete. Data saved to {OUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98440492",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "START_INDEX = 0\n",
        "END_INDEX = 200  # set to len(journals_data) for all\n",
        "\n",
        "journals_data = pd.read_csv('MISQ_Issues.csv')\n",
        "OUT_FILE = 'MISQ_article_data.csv'\n",
        "\n",
        "WAIT_SEC = 15\n",
        "\n",
        "def is_bot_page(driver):\n",
        "    title = (driver.title or '').lower()\n",
        "    url = (driver.current_url or '').lower()\n",
        "    src = (driver.page_source or '').lower()\n",
        "    signals = ['validate', 'bot', 'captcha', 'crawlprevention', 'verification', 'are you human', 'cloudflare']\n",
        "    return any(s in title for s in signals) or any(s in url for s in signals) or any(s in src for s in signals)\n",
        "\n",
        "def getAuthorsData(driver):\n",
        "    authdata = []\n",
        "    seen = set()\n",
        "    skip = {\n",
        "        'n/a', 'author', 'authors', 'author & article information',\n",
        "        'author information', 'article information'\n",
        "    }\n",
        "\n",
        "    selectors = [\n",
        "        '.author-name',\n",
        "        '.author .name',\n",
        "        '[itemprop=\"author\"]',\n",
        "        '.byline .author',\n",
        "        '.contributor-name',\n",
        "        '.author, .contributor, [class*=\"author\"], .byline span'\n",
        "    ]\n",
        "\n",
        "    all_parts = []\n",
        "    for sel in selectors:\n",
        "        try:\n",
        "            els = driver.find_elements(By.CSS_SELECTOR, sel)\n",
        "            for el in els:\n",
        "                text = (el.text or '').strip()\n",
        "                if not text or len(text) < 3:\n",
        "                    continue\n",
        "                for part in re.split(r'[\\n;]+', text):\n",
        "                    part = part.strip()\n",
        "                    if part and 3 <= len(part) <= 80:\n",
        "                        all_parts.append(part)\n",
        "            if all_parts:\n",
        "                break\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    for name in all_parts:\n",
        "        name = ' '.join(name.split()).replace(';', '').strip()\n",
        "        if not name or len(name) < 3 or len(name) > 80:\n",
        "            continue\n",
        "        low = name.lower()\n",
        "        if low in skip:\n",
        "            continue\n",
        "        if name.startswith('Author') or name.startswith('Article'):\n",
        "            continue\n",
        "        if low not in seen:\n",
        "            seen.add(low)\n",
        "            authdata.append([name, \"N/A\", \"N/A\"])\n",
        "\n",
        "    return authdata if authdata else [[\"N/A\", \"N/A\", \"N/A\"]]\n",
        "\n",
        "# Write header once\n",
        "if not os.path.exists(OUT_FILE) or os.path.getsize(OUT_FILE) == 0:\n",
        "    with open(OUT_FILE, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\n",
        "            'URL','Journal_Title','Article_Title','Volume_Issue','Month_Year',\n",
        "            'Abstract','Keywords','Author_name','Author_email','Author_Address'\n",
        "        ])\n",
        "\n",
        "# Chrome options: faster loads (block images/fonts/styles)\n",
        "opts = Options()\n",
        "prefs = {\n",
        "    \"profile.managed_default_content_settings.images\": 2,\n",
        "    \"profile.managed_default_content_settings.stylesheets\": 2,\n",
        "    \"profile.managed_default_content_settings.fonts\": 2,\n",
        "}\n",
        "opts.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "driver = webdriver.Chrome(options=opts)\n",
        "wait = WebDriverWait(driver, WAIT_SEC)\n",
        "\n",
        "# Optional: batch writes\n",
        "buffer_rows = []\n",
        "BUFFER_SIZE = 50\n",
        "\n",
        "def flush_buffer():\n",
        "    global buffer_rows\n",
        "    if not buffer_rows:\n",
        "        return\n",
        "    with open(OUT_FILE, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerows(buffer_rows)\n",
        "        file.flush()\n",
        "    buffer_rows = []\n",
        "\n",
        "try:\n",
        "    for idx, row in journals_data.iloc[START_INDEX:END_INDEX].iterrows():\n",
        "        url = str(row.get('URL', '')).strip()\n",
        "        article_date = row.get('Vol Issue Year', None)\n",
        "        article_vol = row.get('Volume Issue', 'N/A')\n",
        "\n",
        "        if not url.startswith('http'):\n",
        "            continue\n",
        "\n",
        "        title = \"N/A\"\n",
        "        abstract = None\n",
        "        keyword_list = []\n",
        "\n",
        "        # Smaller jitter (faster than 3–7 seconds)\n",
        "        time.sleep(random.uniform(0.8, 1.8))\n",
        "\n",
        "        driver.get(url)\n",
        "\n",
        "        # Bot handling (manual)\n",
        "        if is_bot_page(driver):\n",
        "            print(f\"⚠ Bot page hit. Solve in browser, then press ENTER. URL: {url}\")\n",
        "            input()\n",
        "            driver.get(url)\n",
        "\n",
        "        # Wait for page to load enough (title)\n",
        "        try:\n",
        "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Title\n",
        "        for sel in ['h1', '.article-title', '.title', 'article h1']:\n",
        "            try:\n",
        "                title = driver.find_element(By.CSS_SELECTOR, sel).text.strip()\n",
        "                if title and len(title) > 3:\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Abstract\n",
        "        for sel in ['.abstract', '#abstract', '.article-abstract', 'section.abstract', '[class*=\"abstract\"]']:\n",
        "            try:\n",
        "                abstract = driver.find_element(By.CSS_SELECTOR, sel).text.strip()\n",
        "                if abstract and len(abstract) > 20:\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Keywords\n",
        "        try:\n",
        "            keyword_list = [\n",
        "                k.text.strip()\n",
        "                for k in driver.find_elements(By.CSS_SELECTOR, '.keyword, .keywords span, .tag, [class*=\"keyword\"] span')\n",
        "                if k.text.strip()\n",
        "            ]\n",
        "        except:\n",
        "            keyword_list = []\n",
        "\n",
        "        final_data = [url, \"MIS Quarterly\", title, article_vol, article_date, abstract, keyword_list]\n",
        "\n",
        "        auth_data = getAuthorsData(driver)\n",
        "        for arow in auth_data:\n",
        "            buffer_rows.append(final_data + arow)\n",
        "\n",
        "        if len(buffer_rows) >= BUFFER_SIZE:\n",
        "            flush_buffer()\n",
        "\n",
        "        print(f\"✓ {title[:60]}... ({len(auth_data)} author(s))\")\n",
        "\n",
        "finally:\n",
        "    flush_buffer()\n",
        "    driver.quit()\n",
        "    print(\"DONE:\", OUT_FILE)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
