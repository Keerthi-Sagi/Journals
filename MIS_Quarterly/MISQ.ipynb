{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6fb5005a",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstall selenium pandas\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\IPython\\core\\interactiveshell.py:2511\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2509\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2510\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2511\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2513\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2514\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2515\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\IPython\\core\\magics\\packaging.py:105\u001b[39m, in \u001b[36mPackagingMagics.pip\u001b[39m\u001b[34m(self, line)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    103\u001b[39m     python = shlex.quote(python)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpython\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNote: you may need to restart the kernel to use updated packages.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\ipykernel\\zmqshell.py:786\u001b[39m, in \u001b[36mZMQInteractiveShell.system_piped\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    785\u001b[39m             cmd = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpushd \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m &&\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcmd\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m         \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    788\u001b[39m     \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = system(\u001b[38;5;28mself\u001b[39m.var_expand(cmd, depth=\u001b[32m1\u001b[39m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\IPython\\utils\\_process_win32.py:138\u001b[39m, in \u001b[36msystem\u001b[39m\u001b[34m(cmd)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    137\u001b[39m     cmd = \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpushd \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m &&\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m % (path, cmd)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m res = \u001b[43mprocess_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_system_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\IPython\\utils\\_process_common.py:86\u001b[39m, in \u001b[36mprocess_handler\u001b[39m\u001b[34m(cmd, callback, stderr)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shell \u001b[38;5;129;01mand\u001b[39;00m os.name == \u001b[33m'\u001b[39m\u001b[33mposix\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mSHELL\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m     85\u001b[39m     executable = os.environ[\u001b[33m'\u001b[39m\u001b[33mSHELL\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m p = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     94\u001b[39m     out = callback(p)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\t276m996\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\subprocess.py:1038\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1034\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_mode:\n\u001b[32m   1035\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1036\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1048\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m.stdin, \u001b[38;5;28mself\u001b[39m.stdout, \u001b[38;5;28mself\u001b[39m.stderr)):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\t276m996\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\subprocess.py:1552\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[39m\n\u001b[32m   1550\u001b[39m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[32m   1551\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1552\u001b[39m     hp, ht, pid, tid = \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[32m   1554\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1559\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1561\u001b[39m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[32m   1562\u001b[39m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1565\u001b[39m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[32m   1566\u001b[39m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[32m   1567\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_pipe_fds(p2cread, p2cwrite,\n\u001b[32m   1568\u001b[39m                          c2pread, c2pwrite,\n\u001b[32m   1569\u001b[39m                          errread, errwrite)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "pip install selenium pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d07d1f8",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1003917",
      "metadata": {},
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "# Initialize the chrome webdriver\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# Starting URL - browse by year page\n",
        "START_URL = 'https://misq.umn.edu/misq/issue/browse-by-year'\n",
        "\n",
        "# Years to scrape (2010 to 2025)\n",
        "START_YEAR = 2010\n",
        "END_YEAR = 2025\n",
        "\n",
        "# Save CSV file in the same directory as this notebook (MIS_Quarterly folder)\n",
        "OUT_FILE = os.path.join(os.getcwd(), 'MISQ_Issues.csv')\n",
        "print(f\"CSV file will be saved to: {OUT_FILE}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\\n\")\n",
        "data = []\n",
        "\n",
        "def write_to_csv(rows):\n",
        "    file_exists = os.path.exists(OUT_FILE) and os.path.getsize(OUT_FILE) > 0\n",
        "    with open(OUT_FILE, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"Title\", \"URL\", \"Volume Issue\", \"Vol Issue Year\"])\n",
        "            print(f\"Created CSV file: {OUT_FILE}\")\n",
        "        writer.writerows(rows)\n",
        "        file.flush()  # Ensure data is written immediately\n",
        "    print(f\"  ✓ Saved {len(rows)} articles to {OUT_FILE}\")\n",
        "\n",
        "def scrape_issue_page(issue_url, vol_issue, year):\n",
        "    \"\"\"Scrape articles from a single issue page\"\"\"\n",
        "    try:\n",
        "        driver.get(issue_url)\n",
        "        driver.implicitly_wait(15)\n",
        "        time.sleep(2)\n",
        "        \n",
        "        print(f\"  Page loaded: {driver.title}\")\n",
        "        \n",
        "        # Find all article links - try multiple selectors\n",
        "        articles = []\n",
        "        \n",
        "        # Try different selectors for article links\n",
        "        selectors = [\n",
        "            'a[href*=\"/misq/vol\"]',\n",
        "            'a[href*=\"/article\"]',\n",
        "            '.article-title a',\n",
        "            '.article a',\n",
        "            'h3 a',\n",
        "            'h4 a',\n",
        "            '.title a',\n",
        "            'article a',\n",
        "            '.entry-title a',\n",
        "            'li a[href*=\"/article\"]',\n",
        "            'div.article a',\n",
        "            'table a[href*=\"/article\"]'\n",
        "        ]\n",
        "        \n",
        "        print(\"  Searching for articles...\")\n",
        "        for selector in selectors:\n",
        "            try:\n",
        "                found = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                if found:\n",
        "                    print(f\"    Selector '{selector}': Found {len(found)} links\")\n",
        "                    # Filter for article links (not issue links)\n",
        "                    filtered = [a for a in found if a.get_attribute('href') and \n",
        "                               ('/article' in a.get_attribute('href') or \n",
        "                                ('/misq/vol' in a.get_attribute('href') and '/issue' not in a.get_attribute('href')))]\n",
        "                    if filtered:\n",
        "                        print(f\"      → {len(filtered)} are article links\")\n",
        "                        articles.extend(filtered)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        \n",
        "        # Remove duplicates\n",
        "        seen_urls = set()\n",
        "        unique_articles = []\n",
        "        for article in articles:\n",
        "            try:\n",
        "                url = article.get_attribute('href')\n",
        "                if url and url not in seen_urls:\n",
        "                    seen_urls.add(url)\n",
        "                    unique_articles.append(article)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        if not unique_articles:\n",
        "            # Fallback: find all links and filter\n",
        "            print(\"  Trying fallback: checking all links...\")\n",
        "            all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "            for link in all_links:\n",
        "                try:\n",
        "                    url = link.get_attribute('href') or ''\n",
        "                    if url and ('/article' in url or ('/misq/vol' in url and '/issue' not in url)) and url not in seen_urls:\n",
        "                        seen_urls.add(url)\n",
        "                        unique_articles.append(link)\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        print(f\"  Total unique articles found: {len(unique_articles)}\")\n",
        "        \n",
        "        rows = []\n",
        "        for article in unique_articles:\n",
        "            try:\n",
        "                article_url = article.get_attribute('href')\n",
        "                if not article_url:\n",
        "                    continue\n",
        "                    \n",
        "                # Make sure URL is absolute\n",
        "                if article_url.startswith('/'):\n",
        "                    article_url = 'https://misq.umn.edu' + article_url\n",
        "                \n",
        "                if not article_url.startswith('http'):\n",
        "                    continue\n",
        "                \n",
        "                # Get article title\n",
        "                article_title = article.text.strip()\n",
        "                if not article_title or len(article_title) < 5:\n",
        "                    # Try to get title from parent or nearby element\n",
        "                    try:\n",
        "                        parent = article.find_element(By.XPATH, './..')\n",
        "                        article_title = parent.text.strip()\n",
        "                    except:\n",
        "                        try:\n",
        "                            # Try sibling or nearby heading\n",
        "                            heading = article.find_element(By.XPATH, './preceding-sibling::h3 | ./preceding-sibling::h4 | ./following-sibling::h3 | ./following-sibling::h4')\n",
        "                            article_title = heading.text.strip()\n",
        "                        except:\n",
        "                            article_title = \"N/A\"\n",
        "                \n",
        "                if article_url and article_title and article_title != \"N/A\" and len(article_title) > 5:\n",
        "                    rows.append([article_title, article_url, vol_issue, year])\n",
        "                    print(f\"    ✓ {article_title[:60]}...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"    Error extracting article: {e}\")\n",
        "                continue\n",
        "        \n",
        "        if rows:\n",
        "            write_to_csv(rows)\n",
        "            return len(rows)\n",
        "        else:\n",
        "            print(f\"  ⚠ No articles found on this page\")\n",
        "            # Debug: show page structure\n",
        "            try:\n",
        "                page_text = driver.find_element(By.TAG_NAME, 'body').text[:300]\n",
        "                print(f\"  Page content preview: {page_text[:200]}...\")\n",
        "            except:\n",
        "                pass\n",
        "            return 0\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error scraping issue page: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 0\n",
        "\n",
        "def scrape_year_page(year_url, year):\n",
        "    \"\"\"Scrape all issues from a year page\"\"\"\n",
        "    try:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Navigating to year page: {year_url}\")\n",
        "        driver.get(year_url)\n",
        "        driver.implicitly_wait(15)\n",
        "        time.sleep(3)  # Give page more time to load\n",
        "        \n",
        "        print(f\"Page title: {driver.title}\")\n",
        "        print(f\"Current URL: {driver.current_url}\")\n",
        "        \n",
        "        # Debug: Print some page content to understand structure\n",
        "        try:\n",
        "            page_text = driver.find_element(By.TAG_NAME, 'body').text[:500]\n",
        "            print(f\"Page content preview: {page_text}...\")\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Find all issue links - try comprehensive approach\n",
        "        issue_links = []\n",
        "        \n",
        "        # Try different selectors for issue links\n",
        "        selectors = [\n",
        "            'a[href*=\"/misq/vol\"]',\n",
        "            'a[href*=\"/vol\"]',\n",
        "            '.issue-link a',\n",
        "            '.issue a',\n",
        "            'h2 a',\n",
        "            'h3 a',\n",
        "            'h4 a',\n",
        "            'li a',\n",
        "            '.volume a',\n",
        "            'article a',\n",
        "            'div a[href*=\"/vol\"]',\n",
        "            'table a[href*=\"/vol\"]'\n",
        "        ]\n",
        "        \n",
        "        print(\"\\nTrying to find issue links...\")\n",
        "        for selector in selectors:\n",
        "            try:\n",
        "                links = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                if links:\n",
        "                    print(f\"  Selector '{selector}': Found {len(links)} links\")\n",
        "                    # Filter for actual issue links\n",
        "                    filtered = [l for l in links if l.get_attribute('href') and ('/misq/vol' in l.get_attribute('href') or '/vol' in l.get_attribute('href'))]\n",
        "                    if filtered:\n",
        "                        print(f\"    → {len(filtered)} are issue links\")\n",
        "                        issue_links.extend(filtered)\n",
        "            except Exception as e:\n",
        "                print(f\"  Selector '{selector}': Error - {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Remove duplicates\n",
        "        seen_urls = set()\n",
        "        unique_issue_links = []\n",
        "        for link in issue_links:\n",
        "            try:\n",
        "                url = link.get_attribute('href')\n",
        "                if url and url not in seen_urls:\n",
        "                    seen_urls.add(url)\n",
        "                    unique_issue_links.append(link)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        print(f\"\\nTotal unique issue links found: {len(unique_issue_links)}\")\n",
        "        \n",
        "        if not unique_issue_links:\n",
        "            # Fallback: find ALL links and filter\n",
        "            print(\"Trying fallback: checking all links on page...\")\n",
        "            all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "            print(f\"Total links on page: {len(all_links)}\")\n",
        "            \n",
        "            for link in all_links[:50]:  # Check first 50 links\n",
        "                try:\n",
        "                    url = link.get_attribute('href') or ''\n",
        "                    text = link.text.strip()\n",
        "                    if url and ('/misq/vol' in url or '/vol' in url) and url not in seen_urls:\n",
        "                        print(f\"  Found issue link: {text[:50]} -> {url}\")\n",
        "                        seen_urls.add(url)\n",
        "                        unique_issue_links.append(link)\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        # Extract unique issue URLs with metadata\n",
        "        unique_issues = {}\n",
        "        for link in unique_issue_links:\n",
        "            try:\n",
        "                url = link.get_attribute('href')\n",
        "                if not url:\n",
        "                    continue\n",
        "                    \n",
        "                # Make sure URL is absolute\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://misq.umn.edu' + url\n",
        "                \n",
        "                # Extract volume/issue from URL or text\n",
        "                link_text = link.text.strip()\n",
        "                \n",
        "                # Try to extract from URL\n",
        "                match = re.search(r'vol[^\\d]*(\\d+)[^\\d]*issue[^\\d]*(\\d+)', url, re.I)\n",
        "                if match:\n",
        "                    vol_issue = f\"Vol {match.group(1)}, Issue {match.group(2)}\"\n",
        "                elif link_text and len(link_text) > 3:\n",
        "                    vol_issue = link_text\n",
        "                else:\n",
        "                    # Extract from URL path\n",
        "                    parts = url.split('/')\n",
        "                    vol_issue = parts[-1] if parts else f\"Vol {year}\"\n",
        "                \n",
        "                unique_issues[url] = vol_issue\n",
        "                print(f\"  Issue: {vol_issue} -> {url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing link: {e}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing {len(unique_issues)} issues for year {year}...\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        if len(unique_issues) == 0:\n",
        "            print(f\"WARNING: No issues found for year {year}!\")\n",
        "            print(\"Page HTML snippet:\")\n",
        "            try:\n",
        "                html_snippet = driver.page_source[:1000]\n",
        "                print(html_snippet)\n",
        "            except:\n",
        "                pass\n",
        "            return 0\n",
        "        \n",
        "        total_articles = 0\n",
        "        for issue_url, vol_issue in unique_issues.items():\n",
        "            print(f\"\\n{'─'*60}\")\n",
        "            print(f\"Scraping: {vol_issue}\")\n",
        "            print(f\"URL: {issue_url}\")\n",
        "            count = scrape_issue_page(issue_url, vol_issue, str(year))\n",
        "            total_articles += count\n",
        "            print(f\"  → Found {count} articles\")\n",
        "            time.sleep(1)  # Be respectful\n",
        "        \n",
        "        return total_articles\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping year page {year_url}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 0\n",
        "\n",
        "# Main scraping logic\n",
        "try:\n",
        "    driver.get(START_URL)\n",
        "    driver.implicitly_wait(15)\n",
        "    time.sleep(2)\n",
        "    \n",
        "    print(\"Starting MIS Quarterly scraper (2010-2025)...\")\n",
        "    print(f\"Browse page: {START_URL}\\n\")\n",
        "    \n",
        "    # Find year links for 2010-2025\n",
        "    year_links = {}\n",
        "    \n",
        "    print(\"Searching for year links on browse-by-year page...\")\n",
        "    print(f\"Page title: {driver.title}\")\n",
        "    print(f\"Current URL: {driver.current_url}\\n\")\n",
        "    \n",
        "    # Get all links and filter by year\n",
        "    all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "    print(f\"Total links found on page: {len(all_links)}\")\n",
        "    \n",
        "    # First pass: look for links with year in URL or text\n",
        "    for link in all_links:\n",
        "        try:\n",
        "            url = link.get_attribute('href') or ''\n",
        "            text = link.text.strip()\n",
        "            \n",
        "            # Make URL absolute if relative\n",
        "            if url.startswith('/'):\n",
        "                url = 'https://misq.umn.edu' + url\n",
        "            \n",
        "            # Check if link contains year in URL or text\n",
        "            for year in range(START_YEAR, END_YEAR + 1):\n",
        "                year_str = str(year)\n",
        "                if (year_str in url or year_str in text) and url.startswith('http'):\n",
        "                    if year not in year_links:\n",
        "                        year_links[year] = url\n",
        "                        print(f\"  ✓ Found year {year}: {text[:40]} -> {url}\")\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\nFound {len(year_links)} year links directly from page\")\n",
        "    \n",
        "    # If we didn't find enough year links, try to construct URLs\n",
        "    if len(year_links) < (END_YEAR - START_YEAR + 1) / 2:  # If less than half found\n",
        "        print(\"\\nTrying to construct year URLs...\")\n",
        "        # Common patterns for year pages\n",
        "        base_patterns = [\n",
        "            'https://misq.umn.edu/misq/issue/browse-by-year/{}',\n",
        "            'https://misq.umn.edu/misq/issue/{}',\n",
        "            'https://misq.umn.edu/misq/vol/{}'\n",
        "        ]\n",
        "        \n",
        "        for year in range(START_YEAR, END_YEAR + 1):\n",
        "            if year in year_links:\n",
        "                continue  # Skip if already found\n",
        "                \n",
        "            for pattern in base_patterns:\n",
        "                test_url = pattern.format(year)\n",
        "                try:\n",
        "                    driver.get(test_url)\n",
        "                    time.sleep(1)\n",
        "                    if '404' not in driver.title.lower() and 'not found' not in driver.title.lower():\n",
        "                        year_links[year] = test_url\n",
        "                        print(f\"  ✓ Constructed year {year}: {test_url}\")\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "    \n",
        "    # Go back to browse page\n",
        "    driver.get(START_URL)\n",
        "    time.sleep(2)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Total year links found: {len(year_links)}\")\n",
        "    print(f\"Years: {sorted(year_links.keys())}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Scrape each year\n",
        "    total_articles_scraped = 0\n",
        "    for year in sorted(year_links.keys()):\n",
        "        if START_YEAR <= year <= END_YEAR:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Scraping year {year}\")\n",
        "            print(f\"{'='*60}\")\n",
        "            count = scrape_year_page(year_links[year], year)\n",
        "            total_articles_scraped += count\n",
        "            print(f\"Year {year}: {count} articles scraped\")\n",
        "            time.sleep(2)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Scraping complete!\")\n",
        "    print(f\"Total articles scraped: {total_articles_scraped}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Exception: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a1ad51d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== STEP 2: Scrape detailed data for each article ==========\n",
        "# Uses SINGLE browser session to avoid bot detection\n",
        "# Output: MISQ_article_data.csv\n",
        "\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "\n",
        "START_INDEX = 0\n",
        "END_INDEX = 5 #Process in batches; use len(articles) for all\n",
        "\n",
        "# Paths\n",
        "OUT_DIR = os.getcwd()\n",
        "CSV_PATH = os.path.join(OUT_DIR, 'MISQ_Issues.csv')\n",
        "OUT_FILE = os.path.join(OUT_DIR, 'MISQ_article_data.csv')\n",
        "\n",
        "print(f\"Step 2: Scraping article details...\")\n",
        "print(f\"Reading from: {CSV_PATH}\")\n",
        "print(f\"Writing to: {OUT_FILE}\\n\")\n",
        "\n",
        "journals_data = pd.read_csv(CSV_PATH)\n",
        "n_total = len(journals_data)\n",
        "print(f\"Total articles: {n_total}\")\n",
        "print(f\"Processing range: {START_INDEX} to {min(END_INDEX, n_total)}\\n\")\n",
        "\n",
        "# Create output file with headers\n",
        "if not os.path.exists(OUT_FILE) or os.path.getsize(OUT_FILE) == 0:\n",
        "    with open(OUT_FILE, mode='a', newline='', encoding='utf-8') as f:\n",
        "        csv.writer(f).writerow(['URL','Journal_Title','Article_Title','Volume_Issue','Month_Year','Abstract','Keywords','Author_name','Author_email','Author_Address'])\n",
        "\n",
        "# Setup Chrome with anti-detection options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
        "\n",
        "# Create SINGLE driver for all articles\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "\n",
        "def is_bot_page(driver):\n",
        "    \"\"\"Check if page is showing bot detection\"\"\"\n",
        "    title = driver.title.lower()\n",
        "    url = driver.current_url.lower()\n",
        "    return 'validate' in title or 'bot' in title or 'captcha' in title or 'crawlprevention' in url\n",
        "\n",
        "def get_authors_single(driver):\n",
        "    \"\"\"Extract all author names as one string (one row per article, no duplicates).\"\"\"\n",
        "    seen = set()\n",
        "    names = []\n",
        "    try:\n",
        "        for el in driver.find_elements(By.CSS_SELECTOR, '.author, .contributor, [class*=\"author\"], .byline span'):\n",
        "            name = (el.text or '').strip()\n",
        "            # Skip empty, labels, or too long (likely not a name)\n",
        "            if not name or len(name) > 80 or name.lower() in ('n/a', 'author', 'author & article information', 'authors'):\n",
        "                continue\n",
        "            # Deduplicate\n",
        "            key = name.lower().strip()\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                names.append(name)\n",
        "    except:\n",
        "        pass\n",
        "    author_str = \"; \".join(names) if names else \"N/A\"\n",
        "    return [author_str, \"N/A\", \"N/A\"]\n",
        "\n",
        "try:\n",
        "    for index, row in journals_data.iloc[START_INDEX:END_INDEX].iterrows():\n",
        "        url = str(row.get('URL', '')).strip()\n",
        "        article_date = row.get('Vol Issue Year', None)\n",
        "        article_vol = row.get('Volume Issue', 'N/A')\n",
        "\n",
        "        if not url or not url.startswith('http'):\n",
        "            continue\n",
        "\n",
        "        title = \"N/A\"\n",
        "        abstract = None\n",
        "        keyword_list = []\n",
        "\n",
        "        try:\n",
        "            # Random delay between requests (3-7 seconds)\n",
        "            time.sleep(random.uniform(3, 7))\n",
        "            \n",
        "            driver.get(url)\n",
        "            driver.implicitly_wait(10)\n",
        "            time.sleep(random.uniform(1, 3))  # Random page load wait\n",
        "\n",
        "            # Check for bot detection\n",
        "            if is_bot_page(driver):\n",
        "                print(f\"[{index-START_INDEX+1}] ⚠ Bot detection page - waiting 30s...\")\n",
        "                time.sleep(30)\n",
        "                driver.get(url)\n",
        "                time.sleep(3)\n",
        "                if is_bot_page(driver):\n",
        "                    print(f\"  Still blocked. Skipping {url}\")\n",
        "                    continue\n",
        "\n",
        "            # Title\n",
        "            for sel in ['h1', '.article-title', '.title', 'article h1']:\n",
        "                try:\n",
        "                    title = driver.find_element(By.CSS_SELECTOR, sel).text.strip()\n",
        "                    if title and len(title) > 3:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Abstract\n",
        "            for sel in ['.abstract', '#abstract', '.article-abstract', 'section.abstract', '[class*=\"abstract\"]']:\n",
        "                try:\n",
        "                    abstract = driver.find_element(By.CSS_SELECTOR, sel).text.strip()\n",
        "                    if abstract and len(abstract) > 20:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Keywords\n",
        "            try:\n",
        "                keywords = driver.find_elements(By.CSS_SELECTOR, '.keyword, .keywords span, .tag, [class*=\"keyword\"] span')\n",
        "                keyword_list = [k.text.strip() for k in keywords if k.text.strip()]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            author_row = get_authors_single(driver)\n",
        "            final_data = [url, \"MIS Quarterly\", title, article_vol, article_date, abstract, keyword_list]\n",
        "\n",
        "            with open(OUT_FILE, mode='a', newline='', encoding='utf-8') as f:\n",
        "                csv.writer(f).writerow(final_data + author_row)\n",
        "                f.flush()\n",
        "\n",
        "            print(f\"[{index-START_INDEX+1}/{min(END_INDEX, n_total)-START_INDEX}] ✓ {title[:50]}...\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"[{index-START_INDEX+1}] ✗ Error {url}: {e}\")\n",
        "            with open(OUT_FILE, mode='a', newline='', encoding='utf-8') as f:\n",
        "                csv.writer(f).writerow([url, \"MIS Quarterly\", \"N/A\", article_vol, article_date, None, [], \"N/A\", \"N/A\", \"N/A\"])\n",
        "                f.flush()\n",
        "\n",
        "finally:\n",
        "    driver.quit()\n",
        "    print(f\"\\nStep 2 complete. Data saved to {OUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77b091bc",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0decfb17",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
