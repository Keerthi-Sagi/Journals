{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6fb5005a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /opt/anaconda3/lib/python3.13/site-packages (4.40.0)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (2.2.3)\n",
            "Requirement already satisfied: certifi>=2026.1.4 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (2026.1.4)\n",
            "Requirement already satisfied: trio<1.0,>=0.31.0 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (0.32.0)\n",
            "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: trio-typing>=0.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (0.10.0)\n",
            "Requirement already satisfied: types-certifi>=2021.10.8.3 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (2021.10.8.3)\n",
            "Requirement already satisfied: types-urllib3>=1.26.25.14 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (1.26.25.14)\n",
            "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (4.15.0)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.6.3 in /opt/anaconda3/lib/python3.13/site-packages (from urllib3[socks]<3.0,>=2.6.3->selenium) (2.6.3)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (24.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (3.7)\n",
            "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.13/site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.13/site-packages (from urllib3[socks]<3.0,>=2.6.3->selenium) (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.2 in /opt/anaconda3/lib/python3.13/site-packages (from trio-typing>=0.10.0->selenium) (1.0.0)\n",
            "Requirement already satisfied: async-generator in /opt/anaconda3/lib/python3.13/site-packages (from trio-typing>=0.10.0->selenium) (1.10)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.13/site-packages (from trio-typing>=0.10.0->selenium) (24.2)\n",
            "Requirement already satisfied: importlib-metadata in /opt/anaconda3/lib/python3.13/site-packages (from trio-typing>=0.10.0->selenium) (8.5.0)\n",
            "Requirement already satisfied: h11<1,>=0.16.0 in /opt/anaconda3/lib/python3.13/site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/lib/python3.13/site-packages (from importlib-metadata->trio-typing>=0.10.0->selenium) (3.21.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install selenium pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d07d1f8",
      "metadata": {},
      "source": [
        "# MIS Quarterly Scraper - 2-Step Pipeline\n",
        "\n",
        "Same pattern as Information Systems:\n",
        "- **Step 1**: Collect all article URLs only → `MISQ_Issues.csv` (Title, URL, Volume Issue, Vol Issue Year)\n",
        "- **Step 2**: Scrape detailed data for each URL → `MISQ_article_data.csv` (abstract, keywords, authors)\n",
        "\n",
        "Uses direct issue URLs (`contents-{vol}-{issue}`) for Vol 34–49 (2010–2025).  \n",
        "If you see \"Validate User\", open https://misq.umn.edu in a browser and complete validation first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aa9dd72",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file will be saved to: /Users/keerthisagi/Documents/Journals/MIS_Quarterly/MISQ_Issues.csv\n",
            "Current working directory: /Users/keerthisagi/Documents/Journals/MIS_Quarterly\n",
            "\n",
            "Starting MIS Quarterly scraper (2010-2025)...\n",
            "Browse page: https://misq.umn.edu/misq/issue/browse-by-year\n",
            "\n",
            "Searching for year links on browse-by-year page...\n",
            "Page title: Validate User\n",
            "Current URL: https://misq.umn.edu/crawlprevention/governor?content=%2fmisq%2fissue%2fbrowse-by-year\n",
            "\n",
            "Total links found on page: 138\n",
            "  ✓ Found year 2010: 2010 -> https://misq.umn.edu/misq/issue/browse-by-year/2010\n",
            "  ✓ Found year 2011: 2011 -> https://misq.umn.edu/misq/issue/browse-by-year/2011\n",
            "  ✓ Found year 2012: 2012 -> https://misq.umn.edu/misq/issue/browse-by-year/2012\n",
            "  ✓ Found year 2013: 2013 -> https://misq.umn.edu/misq/issue/browse-by-year/2013\n",
            "  ✓ Found year 2014: 2014 -> https://misq.umn.edu/misq/issue/browse-by-year/2014\n",
            "  ✓ Found year 2015: 2015 -> https://misq.umn.edu/misq/issue/browse-by-year/2015\n",
            "  ✓ Found year 2016: 2016 -> https://misq.umn.edu/misq/issue/browse-by-year/2016\n",
            "  ✓ Found year 2017: 2017 -> https://misq.umn.edu/misq/issue/browse-by-year/2017\n",
            "  ✓ Found year 2018: 2018 -> https://misq.umn.edu/misq/issue/browse-by-year/2018\n",
            "  ✓ Found year 2019: 2019 -> https://misq.umn.edu/misq/issue/browse-by-year/2019\n",
            "  ✓ Found year 2020: 2020 -> https://misq.umn.edu/misq/issue/browse-by-year/2020\n",
            "  ✓ Found year 2021: 2021 -> https://misq.umn.edu/misq/issue/browse-by-year/2021\n",
            "  ✓ Found year 2022: 2022 -> https://misq.umn.edu/misq/issue/browse-by-year/2022\n",
            "  ✓ Found year 2023: 2023 -> https://misq.umn.edu/misq/issue/browse-by-year/2023\n",
            "  ✓ Found year 2024: 2024 -> https://misq.umn.edu/misq/issue/browse-by-year/2024\n",
            "  ✓ Found year 2025: 2025 -> https://misq.umn.edu/misq/issue/browse-by-year/2025\n",
            "\n",
            "Found 16 year links directly from page\n",
            "\n",
            "============================================================\n",
            "Total year links found: 16\n",
            "Years: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "Scraping year 2010\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Navigating to year page: https://misq.umn.edu/misq/issue/browse-by-year/2010\n",
            "Page title: Management Information Systems Quarterly | MIS Quarterly | Browse By Year\n",
            "Current URL: https://misq.umn.edu/misq/issue/browse-by-year/2010\n",
            "Page content preview: All Content\n",
            "Management Information Systems Quarterly\n",
            "                             Advanced Search\n",
            "Register\n",
            "Sign In\n",
            "PUBLICATIONS\n",
            "COLLECTIONS\n",
            "FORTHCOMING\n",
            "EDITORIAL\n",
            "RESOURCES\n",
            "CONTACT\n",
            "Management Information Systems Quarterly | MIS Quarterly\n",
            "Volume 34, Number 1\n",
            "Volume 34, Number 2\n",
            "Volume 34, Number 3\n",
            "Volume 34, Number 4\n",
            "Online ISSN 2162-9730 Print ISSN 0276-7783\n",
            "About MISQ\n",
            "Overview\n",
            "Editorial Board\n",
            "Contact Us\n",
            "Subscribe\n",
            "Social Links\n",
            "LinkedIn\n",
            "Facebook\n",
            "MISQ Insider\n",
            "AIS eLibrary\n",
            "Alerts\n",
            "Sign Up for Emails\n",
            "...\n",
            "\n",
            "Trying to find issue links...\n"
          ]
        }
      ],
      "source": [
        "# ========== STEP 1: Collect all article URLs only ==========\n",
        "# Output: MISQ_Issues.csv (Title, URL, Volume Issue, Vol Issue Year)\n",
        "# Uses direct issue URLs: misq.umn.edu/contents-{vol}-{issue}\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "\n",
        "driver = webdriver.Chrome()\n",
        "driver.implicitly_wait(15)\n",
        "\n",
        "# 2010 = Vol 34, 2025 = Vol 49 (MISQ publishes quarterly)\n",
        "START_VOL = 34\n",
        "END_VOL = 49\n",
        "\n",
        "# Save in same folder as notebook\n",
        "OUT_DIR = os.getcwd()\n",
        "OUT_FILE = os.path.join(OUT_DIR, 'MISQ_Issues.csv')\n",
        "print(f\"Step 1: Collecting article URLs...\")\n",
        "print(f\"Output: {OUT_FILE}\\n\")\n",
        "\n",
        "def write_to_csv(rows):\n",
        "    file_exists = os.path.exists(OUT_FILE) and os.path.getsize(OUT_FILE) > 0\n",
        "    with open(OUT_FILE, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"Title\", \"URL\", \"Volume Issue\", \"Vol Issue Year\"])\n",
        "        writer.writerows(rows)\n",
        "        file.flush()\n",
        "\n",
        "def scrape_issue(vol, issue):\n",
        "    \"\"\"Scrape article URLs from one issue page. Returns list of [title, url, vol_issue, year].\"\"\"\n",
        "    url = f\"https://misq.umn.edu/contents-{vol}-{issue}\"\n",
        "    vol_issue = f\"Vol {vol}, Issue {issue}\"\n",
        "    year = 1976 + vol  # approximate year\n",
        "    \n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        \n",
        "        # Skip if page not found\n",
        "        if \"page not found\" in driver.title.lower() or \"404\" in driver.title.lower():\n",
        "            return []\n",
        "        \n",
        "        rows = []\n",
        "        # Find article links (MISQ format: /misq/article/ or /article/)\n",
        "        all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "        seen = set()\n",
        "        \n",
        "        for link in all_links:\n",
        "            try:\n",
        "                href = link.get_attribute('href') or ''\n",
        "                if not href or href in seen:\n",
        "                    continue\n",
        "                # Article URLs contain /article/\n",
        "                if '/article/' in href and 'misq.umn.edu' in href:\n",
        "                    seen.add(href)\n",
        "                    title = link.text.strip()\n",
        "                    if not title or len(title) < 5:\n",
        "                        title = \"N/A\"\n",
        "                    rows.append([title, href, vol_issue, str(year)])\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        return rows\n",
        "    except Exception as e:\n",
        "        print(f\"  Error {vol}-{issue}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Scrape all volumes and issues\n",
        "total = 0\n",
        "for vol in range(START_VOL, END_VOL + 1):\n",
        "    year = 1976 + vol\n",
        "    print(f\"Vol {vol} ({year}):\")\n",
        "    for issue in range(1, 5):  # 4 issues per year\n",
        "        rows = scrape_issue(vol, issue)\n",
        "        if rows:\n",
        "            write_to_csv(rows)\n",
        "            total += len(rows)\n",
        "            print(f\"  Issue {issue}: {len(rows)} articles\")\n",
        "        time.sleep(1)\n",
        "    print()\n",
        "\n",
        "print(f\"Step 1 complete. Total: {total} articles → {OUT_FILE}\")\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e18b8e",
      "metadata": {},
      "source": [
        "## Step 2: Scrape detailed data for each article\n",
        "\n",
        "Reads `MISQ_Issues.csv` from Step 1, visits each URL, extracts title, abstract, keywords, authors.  \n",
        "Saves to `MISQ_article_data.csv`. Set `START_INDEX` and `END_INDEX` to process in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d55bc5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading articles from: /Users/keerthisagi/Documents/Journals/MIS_Quarterly/MISQ_Issues.csv\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Users/keerthisagi/Documents/Journals/MIS_Quarterly/MISQ_Issues.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMISQ_Issues.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading articles from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m journals_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Save detailed article data in the same directory\u001b[39;00m\n\u001b[1;32m     20\u001b[0m OUT_FILE \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMISQ_article_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/keerthisagi/Documents/Journals/MIS_Quarterly/MISQ_Issues.csv'"
          ]
        }
      ],
      "source": [
        "# ========== STEP 2: Scrape detailed data for each article ==========\n",
        "# Output: MISQ_article_data.csv\n",
        "# Run Step 1 first to create MISQ_Issues.csv\n",
        "\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "\n",
        "START_INDEX = 0\n",
        "END_INDEX = 100  # Process in batches; use len(articles) for all\n",
        "\n",
        "# Paths - run from MIS_Quarterly folder (or set CSV_PATH manually)\n",
        "OUT_DIR = os.getcwd()\n",
        "CSV_PATH = os.path.join(OUT_DIR, 'MISQ_Issues.csv')\n",
        "OUT_FILE = os.path.join(OUT_DIR, 'MISQ_article_data.csv')\n",
        "\n",
        "print(f\"Step 2: Scraping article details...\")\n",
        "print(f\"Reading from: {CSV_PATH}\")\n",
        "print(f\"Writing to: {OUT_FILE}\\n\")\n",
        "\n",
        "journals_data = pd.read_csv(CSV_PATH)\n",
        "n_total = len(journals_data)\n",
        "print(f\"Total articles: {n_total}\")\n",
        "print(f\"Processing range: {START_INDEX} to {min(END_INDEX, n_total)}\\n\")\n",
        "\n",
        "# Create output file with headers\n",
        "if not os.path.exists(OUT_FILE) or os.path.getsize(OUT_FILE) == 0:\n",
        "    with open(OUT_FILE, mode='a', newline='', encoding='utf-8') as f:\n",
        "        csv.writer(f).writerow(['URL','Journal_Title','Article_Title','Volume_Issue','Month_Year','Abstract','Keywords','Author_name','Author_email','Author_Address'])\n",
        "\n",
        "def get_authors(driver):\n",
        "    \"\"\"Extract author info from page.\"\"\"\n",
        "    authdata = []\n",
        "    try:\n",
        "        authors = driver.find_elements(By.CSS_SELECTOR, '.author, .contributor, [class*=\"author\"], .byline span')\n",
        "        for auth in authors:\n",
        "            name = auth.text.strip() or \"N/A\"\n",
        "            authdata.append([name, \"N/A\", \"N/A\"])\n",
        "    except:\n",
        "        pass\n",
        "    return authdata if authdata else [[\"N/A\", \"N/A\", \"N/A\"]]\n",
        "\n",
        "for index, row in journals_data.iloc[START_INDEX:END_INDEX].iterrows():\n",
        "    driver = webdriver.Chrome()\n",
        "    url = str(row.get('URL', '')).strip()\n",
        "    article_date = row.get('Vol Issue Year', None)\n",
        "    article_vol = row.get('Volume Issue', 'N/A')\n",
        "\n",
        "    if not url or not url.startswith('http'):\n",
        "        driver.quit()\n",
        "        continue\n",
        "\n",
        "    title = \"N/A\"\n",
        "    abstract = None\n",
        "    keyword_list = []\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        driver.implicitly_wait(10)\n",
        "        time.sleep(1)\n",
        "\n",
        "        # Title\n",
        "        for sel in ['h1', '.article-title', '.title', 'article h1']:\n",
        "            try:\n",
        "                title = driver.find_element(By.CSS_SELECTOR, sel).text.strip()\n",
        "                if title and len(title) > 3:\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Abstract\n",
        "        for sel in ['.abstract', '#abstract', '.article-abstract', 'section.abstract', '[class*=\"abstract\"]']:\n",
        "            try:\n",
        "                abstract = driver.find_element(By.CSS_SELECTOR, sel).text.strip()\n",
        "                if abstract and len(abstract) > 20:\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Keywords\n",
        "        try:\n",
        "            keywords = driver.find_elements(By.CSS_SELECTOR, '.keyword, .keywords span, .tag, [class*=\"keyword\"] span')\n",
        "            keyword_list = [k.text.strip() for k in keywords if k.text.strip()]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        auth_data = get_authors(driver)\n",
        "        final_data = [url, \"MIS Quarterly\", title, article_vol, article_date, abstract, keyword_list]\n",
        "\n",
        "        with open(OUT_FILE, mode='a', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "            for auth in auth_data:\n",
        "                writer.writerow(final_data + auth)\n",
        "            f.flush()\n",
        "\n",
        "        print(f\"[{index-START_INDEX+1}] {title[:50]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error {url}: {e}\")\n",
        "        with open(OUT_FILE, mode='a', newline='', encoding='utf-8') as f:\n",
        "            csv.writer(f).writerow([url, \"MIS Quarterly\", \"N/A\", article_vol, article_date, None, [], \"N/A\", \"N/A\", \"N/A\"])\n",
        "            f.flush()\n",
        "\n",
        "    driver.quit()\n",
        "    time.sleep(2)\n",
        "\n",
        "print(f\"\\nStep 2 complete. Data saved to {OUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caa794cc",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
