{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb5005a",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install selenium pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d07d1f8",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1003917",
      "metadata": {},
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "# Initialize the chrome webdriver\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# Starting URL - browse by year page\n",
        "START_URL = 'https://misq.umn.edu/misq/issue/browse-by-year'\n",
        "\n",
        "# Years to scrape (2010 to 2025)\n",
        "START_YEAR = 2010\n",
        "END_YEAR = 2025\n",
        "\n",
        "# Save CSV file in the same directory as this notebook (MIS_Quarterly folder)\n",
        "OUT_FILE = os.path.join(os.getcwd(), 'MISQ_Issues.csv')\n",
        "print(f\"CSV file will be saved to: {OUT_FILE}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\\n\")\n",
        "data = []\n",
        "\n",
        "def write_to_csv(rows):\n",
        "    file_exists = os.path.exists(OUT_FILE) and os.path.getsize(OUT_FILE) > 0\n",
        "    with open(OUT_FILE, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"Title\", \"URL\", \"Volume Issue\", \"Vol Issue Year\"])\n",
        "            print(f\"Created CSV file: {OUT_FILE}\")\n",
        "        writer.writerows(rows)\n",
        "        file.flush()  # Ensure data is written immediately\n",
        "    print(f\"  ✓ Saved {len(rows)} articles to {OUT_FILE}\")\n",
        "\n",
        "def scrape_issue_page(issue_url, vol_issue, year):\n",
        "    \"\"\"Scrape articles from a single issue page\"\"\"\n",
        "    try:\n",
        "        driver.get(issue_url)\n",
        "        driver.implicitly_wait(15)\n",
        "        time.sleep(2)\n",
        "        \n",
        "        print(f\"  Page loaded: {driver.title}\")\n",
        "        \n",
        "        # Find all article links - try multiple selectors\n",
        "        articles = []\n",
        "        \n",
        "        # Try different selectors for article links\n",
        "        selectors = [\n",
        "            'a[href*=\"/misq/vol\"]',\n",
        "            'a[href*=\"/article\"]',\n",
        "            '.article-title a',\n",
        "            '.article a',\n",
        "            'h3 a',\n",
        "            'h4 a',\n",
        "            '.title a',\n",
        "            'article a',\n",
        "            '.entry-title a',\n",
        "            'li a[href*=\"/article\"]',\n",
        "            'div.article a',\n",
        "            'table a[href*=\"/article\"]'\n",
        "        ]\n",
        "        \n",
        "        print(\"  Searching for articles...\")\n",
        "        for selector in selectors:\n",
        "            try:\n",
        "                found = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                if found:\n",
        "                    print(f\"    Selector '{selector}': Found {len(found)} links\")\n",
        "                    # Filter for article links (not issue links)\n",
        "                    filtered = [a for a in found if a.get_attribute('href') and \n",
        "                               ('/article' in a.get_attribute('href') or \n",
        "                                ('/misq/vol' in a.get_attribute('href') and '/issue' not in a.get_attribute('href')))]\n",
        "                    if filtered:\n",
        "                        print(f\"      → {len(filtered)} are article links\")\n",
        "                        articles.extend(filtered)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        \n",
        "        # Remove duplicates\n",
        "        seen_urls = set()\n",
        "        unique_articles = []\n",
        "        for article in articles:\n",
        "            try:\n",
        "                url = article.get_attribute('href')\n",
        "                if url and url not in seen_urls:\n",
        "                    seen_urls.add(url)\n",
        "                    unique_articles.append(article)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        if not unique_articles:\n",
        "            # Fallback: find all links and filter\n",
        "            print(\"  Trying fallback: checking all links...\")\n",
        "            all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "            for link in all_links:\n",
        "                try:\n",
        "                    url = link.get_attribute('href') or ''\n",
        "                    if url and ('/article' in url or ('/misq/vol' in url and '/issue' not in url)) and url not in seen_urls:\n",
        "                        seen_urls.add(url)\n",
        "                        unique_articles.append(link)\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        print(f\"  Total unique articles found: {len(unique_articles)}\")\n",
        "        \n",
        "        rows = []\n",
        "        for article in unique_articles:\n",
        "            try:\n",
        "                article_url = article.get_attribute('href')\n",
        "                if not article_url:\n",
        "                    continue\n",
        "                    \n",
        "                # Make sure URL is absolute\n",
        "                if article_url.startswith('/'):\n",
        "                    article_url = 'https://misq.umn.edu' + article_url\n",
        "                \n",
        "                if not article_url.startswith('http'):\n",
        "                    continue\n",
        "                \n",
        "                # Get article title\n",
        "                article_title = article.text.strip()\n",
        "                if not article_title or len(article_title) < 5:\n",
        "                    # Try to get title from parent or nearby element\n",
        "                    try:\n",
        "                        parent = article.find_element(By.XPATH, './..')\n",
        "                        article_title = parent.text.strip()\n",
        "                    except:\n",
        "                        try:\n",
        "                            # Try sibling or nearby heading\n",
        "                            heading = article.find_element(By.XPATH, './preceding-sibling::h3 | ./preceding-sibling::h4 | ./following-sibling::h3 | ./following-sibling::h4')\n",
        "                            article_title = heading.text.strip()\n",
        "                        except:\n",
        "                            article_title = \"N/A\"\n",
        "                \n",
        "                if article_url and article_title and article_title != \"N/A\" and len(article_title) > 5:\n",
        "                    rows.append([article_title, article_url, vol_issue, year])\n",
        "                    print(f\"    ✓ {article_title[:60]}...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"    Error extracting article: {e}\")\n",
        "                continue\n",
        "        \n",
        "        if rows:\n",
        "            write_to_csv(rows)\n",
        "            return len(rows)\n",
        "        else:\n",
        "            print(f\"  ⚠ No articles found on this page\")\n",
        "            # Debug: show page structure\n",
        "            try:\n",
        "                page_text = driver.find_element(By.TAG_NAME, 'body').text[:300]\n",
        "                print(f\"  Page content preview: {page_text[:200]}...\")\n",
        "            except:\n",
        "                pass\n",
        "            return 0\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error scraping issue page: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 0\n",
        "\n",
        "def scrape_year_page(year_url, year):\n",
        "    \"\"\"Scrape all issues from a year page\"\"\"\n",
        "    try:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Navigating to year page: {year_url}\")\n",
        "        driver.get(year_url)\n",
        "        driver.implicitly_wait(15)\n",
        "        time.sleep(3)  # Give page more time to load\n",
        "        \n",
        "        print(f\"Page title: {driver.title}\")\n",
        "        print(f\"Current URL: {driver.current_url}\")\n",
        "        \n",
        "        # Debug: Print some page content to understand structure\n",
        "        try:\n",
        "            page_text = driver.find_element(By.TAG_NAME, 'body').text[:500]\n",
        "            print(f\"Page content preview: {page_text}...\")\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Find all issue links - try comprehensive approach\n",
        "        issue_links = []\n",
        "        \n",
        "        # Try different selectors for issue links\n",
        "        selectors = [\n",
        "            'a[href*=\"/misq/vol\"]',\n",
        "            'a[href*=\"/vol\"]',\n",
        "            '.issue-link a',\n",
        "            '.issue a',\n",
        "            'h2 a',\n",
        "            'h3 a',\n",
        "            'h4 a',\n",
        "            'li a',\n",
        "            '.volume a',\n",
        "            'article a',\n",
        "            'div a[href*=\"/vol\"]',\n",
        "            'table a[href*=\"/vol\"]'\n",
        "        ]\n",
        "        \n",
        "        print(\"\\nTrying to find issue links...\")\n",
        "        for selector in selectors:\n",
        "            try:\n",
        "                links = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                if links:\n",
        "                    print(f\"  Selector '{selector}': Found {len(links)} links\")\n",
        "                    # Filter for actual issue links\n",
        "                    filtered = [l for l in links if l.get_attribute('href') and ('/misq/vol' in l.get_attribute('href') or '/vol' in l.get_attribute('href'))]\n",
        "                    if filtered:\n",
        "                        print(f\"    → {len(filtered)} are issue links\")\n",
        "                        issue_links.extend(filtered)\n",
        "            except Exception as e:\n",
        "                print(f\"  Selector '{selector}': Error - {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Remove duplicates\n",
        "        seen_urls = set()\n",
        "        unique_issue_links = []\n",
        "        for link in issue_links:\n",
        "            try:\n",
        "                url = link.get_attribute('href')\n",
        "                if url and url not in seen_urls:\n",
        "                    seen_urls.add(url)\n",
        "                    unique_issue_links.append(link)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        print(f\"\\nTotal unique issue links found: {len(unique_issue_links)}\")\n",
        "        \n",
        "        if not unique_issue_links:\n",
        "            # Fallback: find ALL links and filter\n",
        "            print(\"Trying fallback: checking all links on page...\")\n",
        "            all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "            print(f\"Total links on page: {len(all_links)}\")\n",
        "            \n",
        "            for link in all_links[:50]:  # Check first 50 links\n",
        "                try:\n",
        "                    url = link.get_attribute('href') or ''\n",
        "                    text = link.text.strip()\n",
        "                    if url and ('/misq/vol' in url or '/vol' in url) and url not in seen_urls:\n",
        "                        print(f\"  Found issue link: {text[:50]} -> {url}\")\n",
        "                        seen_urls.add(url)\n",
        "                        unique_issue_links.append(link)\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        # Extract unique issue URLs with metadata\n",
        "        unique_issues = {}\n",
        "        for link in unique_issue_links:\n",
        "            try:\n",
        "                url = link.get_attribute('href')\n",
        "                if not url:\n",
        "                    continue\n",
        "                    \n",
        "                # Make sure URL is absolute\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://misq.umn.edu' + url\n",
        "                \n",
        "                # Extract volume/issue from URL or text\n",
        "                link_text = link.text.strip()\n",
        "                \n",
        "                # Try to extract from URL\n",
        "                match = re.search(r'vol[^\\d]*(\\d+)[^\\d]*issue[^\\d]*(\\d+)', url, re.I)\n",
        "                if match:\n",
        "                    vol_issue = f\"Vol {match.group(1)}, Issue {match.group(2)}\"\n",
        "                elif link_text and len(link_text) > 3:\n",
        "                    vol_issue = link_text\n",
        "                else:\n",
        "                    # Extract from URL path\n",
        "                    parts = url.split('/')\n",
        "                    vol_issue = parts[-1] if parts else f\"Vol {year}\"\n",
        "                \n",
        "                unique_issues[url] = vol_issue\n",
        "                print(f\"  Issue: {vol_issue} -> {url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing link: {e}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing {len(unique_issues)} issues for year {year}...\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        if len(unique_issues) == 0:\n",
        "            print(f\"WARNING: No issues found for year {year}!\")\n",
        "            print(\"Page HTML snippet:\")\n",
        "            try:\n",
        "                html_snippet = driver.page_source[:1000]\n",
        "                print(html_snippet)\n",
        "            except:\n",
        "                pass\n",
        "            return 0\n",
        "        \n",
        "        total_articles = 0\n",
        "        for issue_url, vol_issue in unique_issues.items():\n",
        "            print(f\"\\n{'─'*60}\")\n",
        "            print(f\"Scraping: {vol_issue}\")\n",
        "            print(f\"URL: {issue_url}\")\n",
        "            count = scrape_issue_page(issue_url, vol_issue, str(year))\n",
        "            total_articles += count\n",
        "            print(f\"  → Found {count} articles\")\n",
        "            time.sleep(1)  # Be respectful\n",
        "        \n",
        "        return total_articles\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping year page {year_url}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 0\n",
        "\n",
        "# Main scraping logic\n",
        "try:\n",
        "    driver.get(START_URL)\n",
        "    driver.implicitly_wait(15)\n",
        "    time.sleep(2)\n",
        "    \n",
        "    print(\"Starting MIS Quarterly scraper (2010-2025)...\")\n",
        "    print(f\"Browse page: {START_URL}\\n\")\n",
        "    \n",
        "    # Find year links for 2010-2025\n",
        "    year_links = {}\n",
        "    \n",
        "    print(\"Searching for year links on browse-by-year page...\")\n",
        "    print(f\"Page title: {driver.title}\")\n",
        "    print(f\"Current URL: {driver.current_url}\\n\")\n",
        "    \n",
        "    # Get all links and filter by year\n",
        "    all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "    print(f\"Total links found on page: {len(all_links)}\")\n",
        "    \n",
        "    # First pass: look for links with year in URL or text\n",
        "    for link in all_links:\n",
        "        try:\n",
        "            url = link.get_attribute('href') or ''\n",
        "            text = link.text.strip()\n",
        "            \n",
        "            # Make URL absolute if relative\n",
        "            if url.startswith('/'):\n",
        "                url = 'https://misq.umn.edu' + url\n",
        "            \n",
        "            # Check if link contains year in URL or text\n",
        "            for year in range(START_YEAR, END_YEAR + 1):\n",
        "                year_str = str(year)\n",
        "                if (year_str in url or year_str in text) and url.startswith('http'):\n",
        "                    if year not in year_links:\n",
        "                        year_links[year] = url\n",
        "                        print(f\"  ✓ Found year {year}: {text[:40]} -> {url}\")\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\nFound {len(year_links)} year links directly from page\")\n",
        "    \n",
        "    # If we didn't find enough year links, try to construct URLs\n",
        "    if len(year_links) < (END_YEAR - START_YEAR + 1) / 2:  # If less than half found\n",
        "        print(\"\\nTrying to construct year URLs...\")\n",
        "        # Common patterns for year pages\n",
        "        base_patterns = [\n",
        "            'https://misq.umn.edu/misq/issue/browse-by-year/{}',\n",
        "            'https://misq.umn.edu/misq/issue/{}',\n",
        "            'https://misq.umn.edu/misq/vol/{}'\n",
        "        ]\n",
        "        \n",
        "        for year in range(START_YEAR, END_YEAR + 1):\n",
        "            if year in year_links:\n",
        "                continue  # Skip if already found\n",
        "                \n",
        "            for pattern in base_patterns:\n",
        "                test_url = pattern.format(year)\n",
        "                try:\n",
        "                    driver.get(test_url)\n",
        "                    time.sleep(1)\n",
        "                    if '404' not in driver.title.lower() and 'not found' not in driver.title.lower():\n",
        "                        year_links[year] = test_url\n",
        "                        print(f\"  ✓ Constructed year {year}: {test_url}\")\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "    \n",
        "    # Go back to browse page\n",
        "    driver.get(START_URL)\n",
        "    time.sleep(2)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Total year links found: {len(year_links)}\")\n",
        "    print(f\"Years: {sorted(year_links.keys())}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Scrape each year\n",
        "    total_articles_scraped = 0\n",
        "    for year in sorted(year_links.keys()):\n",
        "        if START_YEAR <= year <= END_YEAR:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Scraping year {year}\")\n",
        "            print(f\"{'='*60}\")\n",
        "            count = scrape_year_page(year_links[year], year)\n",
        "            total_articles_scraped += count\n",
        "            print(f\"Year {year}: {count} articles scraped\")\n",
        "            time.sleep(2)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Scraping complete!\")\n",
        "    print(f\"Total articles scraped: {total_articles_scraped}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Exception: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a1ad51d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total URLs: 1273\n",
            "Processing: 0 -> 50\n",
            "Output: /Users/keerthisagi/Documents/Journals/MIS_Quarterly/MISQ_article_data.csv\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv, time, os, random, re, json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# -----------------------\n",
        "# CONFIG\n",
        "# -----------------------\n",
        "START_INDEX = 0\n",
        "END_INDEX = 50  # set to len(df) for all\n",
        "WAIT_SEC = 25\n",
        "\n",
        "SLEEP_MIN = 2.0\n",
        "SLEEP_MAX = 4.5\n",
        "\n",
        "BOT_MAX_WAIT_SEC = 12 * 60  # 12 minutes\n",
        "BOT_POLL_SEC = 5\n",
        "\n",
        "CSV_PATH = os.path.join(os.getcwd(), \"MISQ_Issues.csv\")\n",
        "OUT_FILE = os.path.join(os.getcwd(), \"MISQ_article_data.csv\")\n",
        "JOURNAL_TITLE = \"MIS Quarterly\"\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# LOAD INPUT\n",
        "# -----------------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "n_total = len(df)\n",
        "END_INDEX = min(END_INDEX, n_total)\n",
        "\n",
        "print(f\"Total URLs: {n_total}\")\n",
        "print(f\"Processing: {START_INDEX} -> {END_INDEX}\")\n",
        "print(f\"Output: {OUT_FILE}\\n\")\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# OUTPUT HEADER\n",
        "# -----------------------\n",
        "if not os.path.exists(OUT_FILE) or os.path.getsize(OUT_FILE) == 0:\n",
        "    with open(OUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\n",
        "            \"URL\",\"Journal_Title\",\"Article_Title\",\"Volume_Issue\",\"Month_Year\",\n",
        "            \"Abstract\",\"Keywords\",\"Author_name\",\"Author_email\",\"Author_Address\"\n",
        "        ])\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# CHROME SETUP (single driver + persistent profile)\n",
        "# -----------------------\n",
        "opts = Options()\n",
        "opts.add_argument(\"--user-data-dir=/tmp/misq_profile\")\n",
        "opts.add_argument(\"--profile-directory=Default\")\n",
        "\n",
        "# lighter loads\n",
        "prefs = {\n",
        "    \"profile.managed_default_content_settings.images\": 2,\n",
        "    \"profile.managed_default_content_settings.stylesheets\": 2,\n",
        "    \"profile.managed_default_content_settings.fonts\": 2,\n",
        "}\n",
        "opts.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "driver = webdriver.Chrome(options=opts)\n",
        "wait = WebDriverWait(driver, WAIT_SEC)\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# HELPERS\n",
        "# -----------------------\n",
        "def is_block_page(driver) -> bool:\n",
        "    title = (driver.title or \"\").lower()\n",
        "    url = (driver.current_url or \"\").lower()\n",
        "    src = (driver.page_source or \"\").lower()\n",
        "\n",
        "    strong = [\n",
        "        \"captcha\",\n",
        "        \"checking your browser\",\n",
        "        \"access denied\",\n",
        "        \"attention required\",\n",
        "        \"cloudflare\",\n",
        "        \"unusual traffic\",\n",
        "        \"verify you are human\",\n",
        "        \"robot check\",\n",
        "        \"ddos\"\n",
        "    ]\n",
        "    url_signals = [\"crawlprevention\", \"challenge\", \"captcha\"]\n",
        "\n",
        "    return any(s in title for s in strong) or any(s in src for s in strong) or any(s in url for s in url_signals)\n",
        "\n",
        "def wait_until_unblocked(driver, max_wait_sec=BOT_MAX_WAIT_SEC, poll_sec=BOT_POLL_SEC) -> bool:\n",
        "    start = time.time()\n",
        "    warned = False\n",
        "    while is_block_page(driver):\n",
        "        if not warned:\n",
        "            print(\"⚠ Verification/block detected. If Chrome shows a challenge, solve it there.\")\n",
        "            print(\"   Waiting automatically...\")\n",
        "            warned = True\n",
        "        if time.time() - start > max_wait_sec:\n",
        "            return False\n",
        "        time.sleep(poll_sec)\n",
        "    return True\n",
        "\n",
        "def clean_text(s):\n",
        "    s = (s or \"\").strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def safe_join_keywords(items):\n",
        "    out, seen = [], set()\n",
        "    for x in items:\n",
        "        x = clean_text(x)\n",
        "        if not x:\n",
        "            continue\n",
        "        k = x.lower()\n",
        "        if k not in seen:\n",
        "            seen.add(k)\n",
        "            out.append(x)\n",
        "    return \"; \".join(out) if out else \"N/A\"\n",
        "\n",
        "def meta_all(soup, name=None, prop=None):\n",
        "    vals = []\n",
        "    if name:\n",
        "        for m in soup.find_all(\"meta\", attrs={\"name\": name}):\n",
        "            c = m.get(\"content\")\n",
        "            if c: vals.append(c)\n",
        "    if prop:\n",
        "        for m in soup.find_all(\"meta\", attrs={\"property\": prop}):\n",
        "            c = m.get(\"content\")\n",
        "            if c: vals.append(c)\n",
        "    return [clean_text(v) for v in vals if clean_text(v)]\n",
        "\n",
        "def meta_first(soup, name=None, prop=None):\n",
        "    vals = meta_all(soup, name=name, prop=prop)\n",
        "    return vals[0] if vals else None\n",
        "\n",
        "def parse_jsonld(soup):\n",
        "    \"\"\"Return a list of JSON-LD objects (dicts).\"\"\"\n",
        "    objs = []\n",
        "    for s in soup.find_all(\"script\", attrs={\"type\": \"application/ld+json\"}):\n",
        "        raw = s.get_text(strip=True)\n",
        "        if not raw:\n",
        "            continue\n",
        "        try:\n",
        "            data = json.loads(raw)\n",
        "            if isinstance(data, list):\n",
        "                objs.extend([d for d in data if isinstance(d, dict)])\n",
        "            elif isinstance(data, dict):\n",
        "                objs.append(data)\n",
        "        except:\n",
        "            continue\n",
        "    return objs\n",
        "\n",
        "def pick_article_jsonld(jsonlds):\n",
        "    \"\"\"Pick the most likely Article/ScholarlyArticle object.\"\"\"\n",
        "    for obj in jsonlds:\n",
        "        t = obj.get(\"@type\")\n",
        "        if isinstance(t, list):\n",
        "            t = [x.lower() for x in t if isinstance(x, str)]\n",
        "        elif isinstance(t, str):\n",
        "            t = [t.lower()]\n",
        "        else:\n",
        "            t = []\n",
        "        if any(x in t for x in [\"scholarlyarticle\", \"article\", \"newsarticle\"]):\n",
        "            return obj\n",
        "    return None\n",
        "\n",
        "def extract_authors_from_jsonld(article_obj):\n",
        "    authors = []\n",
        "    if not article_obj:\n",
        "        return authors\n",
        "    a = article_obj.get(\"author\")\n",
        "    if isinstance(a, dict):\n",
        "        name = a.get(\"name\")\n",
        "        if name: authors.append(clean_text(name))\n",
        "    elif isinstance(a, list):\n",
        "        for item in a:\n",
        "            if isinstance(item, dict):\n",
        "                name = item.get(\"name\")\n",
        "                if name: authors.append(clean_text(name))\n",
        "            elif isinstance(item, str):\n",
        "                authors.append(clean_text(item))\n",
        "    elif isinstance(a, str):\n",
        "        authors.append(clean_text(a))\n",
        "    # de-dupe\n",
        "    seen, out = set(), []\n",
        "    for x in authors:\n",
        "        k = x.lower()\n",
        "        if x and k not in seen:\n",
        "            seen.add(k)\n",
        "            out.append(x)\n",
        "    return out\n",
        "\n",
        "def looks_like_name(s):\n",
        "    if not s: return False\n",
        "    s = clean_text(s)\n",
        "    if len(s) < 3 or len(s) > 80: return False\n",
        "    bad = {\"author\",\"authors\",\"abstract\",\"keywords\",\"pdf\",\"n/a\"}\n",
        "    if s.lower() in bad: return False\n",
        "    if len(s.split()) > 8: return False\n",
        "    return True\n",
        "\n",
        "def extract_authors_fallback(soup):\n",
        "    \"\"\"\n",
        "    Fallback author extraction:\n",
        "    - citation_author meta tags (best)\n",
        "    - visible author blocks\n",
        "    \"\"\"\n",
        "    # 1) citation_author\n",
        "    cits = []\n",
        "    for m in soup.find_all(\"meta\"):\n",
        "        nm = (m.get(\"name\") or \"\").lower()\n",
        "        if nm == \"citation_author\":\n",
        "            c = m.get(\"content\")\n",
        "            if c: cits.append(clean_text(c))\n",
        "    cits = [c for c in cits if looks_like_name(c)]\n",
        "    if cits:\n",
        "        return cits\n",
        "\n",
        "    # 2) generic visible author selectors\n",
        "    selectors = [\n",
        "        \".author-name\", \".authors a\", \".byline a\", \".author a\",\n",
        "        \".author\", \".byline span\"\n",
        "    ]\n",
        "    names = []\n",
        "    for sel in selectors:\n",
        "        for el in soup.select(sel):\n",
        "            t = clean_text(el.get_text(\" \", strip=True))\n",
        "            # split combined blocks\n",
        "            for p in re.split(r\"[\\n,;]+\", t):\n",
        "                p = clean_text(p)\n",
        "                if looks_like_name(p):\n",
        "                    names.append(p)\n",
        "        if names:\n",
        "            break\n",
        "\n",
        "    # de-dupe\n",
        "    seen, out = set(), []\n",
        "    for x in names:\n",
        "        k = x.lower()\n",
        "        if k not in seen:\n",
        "            seen.add(k)\n",
        "            out.append(x)\n",
        "    return out\n",
        "\n",
        "def extract_title(soup, article_obj=None):\n",
        "    # 1) JSON-LD\n",
        "    if article_obj:\n",
        "        for k in [\"headline\", \"name\"]:\n",
        "            v = article_obj.get(k)\n",
        "            if isinstance(v, str) and clean_text(v):\n",
        "                return clean_text(v)\n",
        "\n",
        "    # 2) citation_title meta\n",
        "    ct = None\n",
        "    for m in soup.find_all(\"meta\"):\n",
        "        nm = (m.get(\"name\") or \"\").lower()\n",
        "        if nm == \"citation_title\":\n",
        "            ct = clean_text(m.get(\"content\"))\n",
        "            if ct: return ct\n",
        "\n",
        "    # 3) og:title / twitter:title / h1\n",
        "    for candidate in [\n",
        "        meta_first(soup, prop=\"og:title\"),\n",
        "        meta_first(soup, name=\"twitter:title\"),\n",
        "    ]:\n",
        "        if candidate:\n",
        "            return candidate\n",
        "\n",
        "    h1 = soup.select_one(\"h1\")\n",
        "    if h1:\n",
        "        t = clean_text(h1.get_text(\" \", strip=True))\n",
        "        if t: return t\n",
        "\n",
        "    return \"N/A\"\n",
        "\n",
        "def extract_abstract(soup, article_obj=None):\n",
        "    # 1) JSON-LD\n",
        "    if article_obj:\n",
        "        for k in [\"description\", \"abstract\"]:\n",
        "            v = article_obj.get(k)\n",
        "            if isinstance(v, str) and len(clean_text(v)) >= 20:\n",
        "                return clean_text(v)\n",
        "\n",
        "    # 2) citation_abstract meta\n",
        "    for m in soup.find_all(\"meta\"):\n",
        "        nm = (m.get(\"name\") or \"\").lower()\n",
        "        if nm in (\"citation_abstract\", \"dc.description\", \"description\"):\n",
        "            v = clean_text(m.get(\"content\"))\n",
        "            if v and len(v) >= 20:\n",
        "                return v\n",
        "\n",
        "    # 3) visible blocks\n",
        "    for sel in [\"#abstract\", \".abstract\", \"section.abstract\", \"[class*='abstract']\"]:\n",
        "        el = soup.select_one(sel)\n",
        "        if el:\n",
        "            t = clean_text(el.get_text(\" \", strip=True))\n",
        "            if t and len(t) >= 20:\n",
        "                return t\n",
        "\n",
        "    return \"N/A\"\n",
        "\n",
        "def extract_keywords(soup, article_obj=None):\n",
        "    # 1) JSON-LD\n",
        "    if article_obj:\n",
        "        kw = article_obj.get(\"keywords\")\n",
        "        if isinstance(kw, str):\n",
        "            # sometimes comma-separated\n",
        "            parts = [clean_text(x) for x in re.split(r\"[;,]+\", kw) if clean_text(x)]\n",
        "            return safe_join_keywords(parts)\n",
        "        if isinstance(kw, list):\n",
        "            parts = []\n",
        "            for x in kw:\n",
        "                if isinstance(x, str):\n",
        "                    parts.append(clean_text(x))\n",
        "            return safe_join_keywords(parts)\n",
        "\n",
        "    # 2) citation_keywords meta\n",
        "    for m in soup.find_all(\"meta\"):\n",
        "        nm = (m.get(\"name\") or \"\").lower()\n",
        "        if nm == \"citation_keywords\":\n",
        "            v = clean_text(m.get(\"content\"))\n",
        "            if v:\n",
        "                parts = [clean_text(x) for x in re.split(r\"[;,]+\", v) if clean_text(x)]\n",
        "                return safe_join_keywords(parts)\n",
        "\n",
        "    # 3) visible selectors\n",
        "    kws = []\n",
        "    for sel in [\".keyword\", \".keywords a\", \".keywords span\", \".tag\", \"[class*='keyword'] a\", \"[class*='keyword'] span\"]:\n",
        "        for el in soup.select(sel):\n",
        "            t = clean_text(el.get_text(\" \", strip=True))\n",
        "            if t: kws.append(t)\n",
        "    return safe_join_keywords(kws)\n",
        "\n",
        "# -----------------------\n",
        "# MAIN LOOP\n",
        "# -----------------------\n",
        "try:\n",
        "    for pos, (idx, row) in enumerate(df.iloc[START_INDEX:END_INDEX].iterrows(), start=1):\n",
        "\n",
        "        url = clean_text(str(row.get(\"URL\", \"\")))\n",
        "        month_year = row.get(\"Vol Issue Year\", \"N/A\")\n",
        "        volume_issue = row.get(\"Volume Issue\", \"N/A\")\n",
        "\n",
        "        if not url.startswith(\"http\"):\n",
        "            continue\n",
        "\n",
        "        # optional: skip clearly non-article links if Step 1 captured extras\n",
        "        # (uncomment if needed)\n",
        "        # if \"/article\" not in url and \"/misq/vol\" not in url:\n",
        "        #     continue\n",
        "\n",
        "        time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))\n",
        "\n",
        "        try:\n",
        "            driver.get(url)\n",
        "            try:\n",
        "                wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            if is_block_page(driver):\n",
        "                ok = wait_until_unblocked(driver)\n",
        "                if not ok:\n",
        "                    print(f\"[{pos}] Timed out blocked. Skipping: {url}\")\n",
        "                    continue\n",
        "                # reload after unblock\n",
        "                driver.get(url)\n",
        "                time.sleep(2)\n",
        "\n",
        "            html = driver.page_source\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "            jsonlds = parse_jsonld(soup)\n",
        "            article_obj = pick_article_jsonld(jsonlds)\n",
        "\n",
        "            title = extract_title(soup, article_obj)\n",
        "            abstract = extract_abstract(soup, article_obj)\n",
        "            keywords = extract_keywords(soup, article_obj)\n",
        "\n",
        "            authors = extract_authors_from_jsonld(article_obj)\n",
        "            if not authors:\n",
        "                authors = extract_authors_fallback(soup)\n",
        "\n",
        "            if not authors:\n",
        "                authors = [\"N/A\"]\n",
        "\n",
        "            final_data = [url, JOURNAL_TITLE, title, volume_issue, month_year, abstract, keywords]\n",
        "\n",
        "            with open(OUT_FILE, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                w = csv.writer(f)\n",
        "                for a in authors:\n",
        "                    w.writerow(final_data + [a, \"N/A\", \"N/A\"])\n",
        "                f.flush()\n",
        "\n",
        "            print(f\"[{pos}/{END_INDEX-START_INDEX}] ✓ {title[:60]} | authors: {len(authors)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[{pos}] ✗ Error on {url}: {e}\")\n",
        "            with open(OUT_FILE, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                csv.writer(f).writerow([url, JOURNAL_TITLE, \"N/A\", volume_issue, month_year, \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\"])\n",
        "                f.flush()\n",
        "\n",
        "finally:\n",
        "    driver.quit()\n",
        "    print(f\"\\nDONE. Saved to: {OUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77b091bc",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0decfb17",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
