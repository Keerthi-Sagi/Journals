URL,Journal_Title,Article_Title,Volume_Issue,Month_Year,Abstract,Keywords,Author_name,Author_email,Author_Address
https://misq.umn.edu/misq/article/34/1/1/488/Information-Systems-Innovation-for-Environmental,MIS Quarterly,Information Systems Innovation for Environmental Sustainability1,"Volume 34, Issue 1",March 2010,"Human life is dependent upon the natural environment, which, most would agree, is rapidly degrading. Business enterprises are a dominant form of social organization and contribute to the worsening, and enhancement, of the natural environment. Scholars in the administrative sciences examine questions spanning organizations and the natural environment but have largely omitted the information systems perspective. We develop a research agenda on information systems innovation for environmental sustainability that demonstrates the critical role that IS can play in shaping beliefs about the environment, in enabling and transforming sustainable processes and practices in organizations, and in improving environmental and economic performance. The belief–action–outcome (BAO) framework and associated research agenda provide the basis for a new discourse on IS for environmental sustainability.",[],"Melville, Nigel P.",N/A,"Stephen M. Ross School of Business, University of Michigan, 701 Tappan Street, Ann Arbor, MI 48109, U.S.A."
https://misq.umn.edu/misq/article/34/1/23/490/Information-Systems-and-Environmentally,MIS Quarterly,Information Systems and Environmentally Sustainable Development: Energy Informatics and New Directions for the IS Community1,"Volume 34, Issue 1",March 2010,"While many corporations and Information Systems units recognize that environmental sustainability is an urgent problem to address, the IS academic community has been slow to acknowledge the problem and take action. We propose ways for the IS community to engage in the development of environmentally sustainable business practices. Specifially, as IS researchers, educators, journal editors, and association leaders, we need to demonstrate how the transformative power of IS can be leveraged to create an ecologically sustainable society. In this Issues and Opinions piece, we advocate a research agenda to establish a new subfield of energy informatics, which applies information systems thinking and skills to increase energy efficiency. We also articulate how IS scholars can incorporate environmental sustainability as an underlying foundation in their teaching, and how IS leaders can embrace environmental sustainability in their core principles and foster changes that reduce the environmental impact of our community.",[],"Watson, Richard T.",N/A,"Department of MIS, University of Georgia, Atlanta, GA 30602-6273, U.S.A."
https://misq.umn.edu/misq/article/34/1/23/490/Information-Systems-and-Environmentally,MIS Quarterly,Information Systems and Environmentally Sustainable Development: Energy Informatics and New Directions for the IS Community1,"Volume 34, Issue 1",March 2010,"While many corporations and Information Systems units recognize that environmental sustainability is an urgent problem to address, the IS academic community has been slow to acknowledge the problem and take action. We propose ways for the IS community to engage in the development of environmentally sustainable business practices. Specifially, as IS researchers, educators, journal editors, and association leaders, we need to demonstrate how the transformative power of IS can be leveraged to create an ecologically sustainable society. In this Issues and Opinions piece, we advocate a research agenda to establish a new subfield of energy informatics, which applies information systems thinking and skills to increase energy efficiency. We also articulate how IS scholars can incorporate environmental sustainability as an underlying foundation in their teaching, and how IS leaders can embrace environmental sustainability in their core principles and foster changes that reduce the environmental impact of our community.",[],"Boudreau, Marie-Claude",N/A,"Department of MIS, University of Georgia, Atlanta, GA 30602-6273, U.S.A."
https://misq.umn.edu/misq/article/34/1/23/490/Information-Systems-and-Environmentally,MIS Quarterly,Information Systems and Environmentally Sustainable Development: Energy Informatics and New Directions for the IS Community1,"Volume 34, Issue 1",March 2010,"While many corporations and Information Systems units recognize that environmental sustainability is an urgent problem to address, the IS academic community has been slow to acknowledge the problem and take action. We propose ways for the IS community to engage in the development of environmentally sustainable business practices. Specifially, as IS researchers, educators, journal editors, and association leaders, we need to demonstrate how the transformative power of IS can be leveraged to create an ecologically sustainable society. In this Issues and Opinions piece, we advocate a research agenda to establish a new subfield of energy informatics, which applies information systems thinking and skills to increase energy efficiency. We also articulate how IS scholars can incorporate environmental sustainability as an underlying foundation in their teaching, and how IS leaders can embrace environmental sustainability in their core principles and foster changes that reduce the environmental impact of our community.",[],"Chen, Adela J.",N/A,"Department of MIS, University of Georgia, Atlanta, GA 30602-6273, U.S.A."
https://misq.umn.edu/misq/article/34/1/39/482/An-Empirical-Analysis-of-the-Impact-of-Information,MIS Quarterly,An Empirical Analysis of the Impact of Information Capabilities Design on Business Process Outsourcing Performance1,"Volume 34, Issue 1",March 2010,"Organizations today outsource diverse business processes to achieve a wide variety of business objectives ranging from reduction of costs to innovation and business transformation. We build on the information processing view of the firm to theorize that performance heterogeneity across business process outsourcing (BPO) exchanges is a function of the design of information capabilities (IC) that fit the unique information requirements (IR) of the exchange. Further, we compare performance effects of the fit between IR and IC across dominant categories of BPO relationships to provide insights into the relative benefits of enacting such fit between the constructs. Empirical tests of our hypotheses using survey data on 127 active BPO relationships find a significant increase (decrease) in satisfaction as a result of the fit (misfit) between IR and IC of the relationship. The results have implications for how BPO relationships must be designed and managed to realize significant performance gains. The study also extends the IPV to identify IC that provide the incentives and means to process information in an interfirm relationship.",[],"Mani, Deepa",N/A,"Indian School of Business, Hyderabad 500032, India"
https://misq.umn.edu/misq/article/34/1/39/482/An-Empirical-Analysis-of-the-Impact-of-Information,MIS Quarterly,An Empirical Analysis of the Impact of Information Capabilities Design on Business Process Outsourcing Performance1,"Volume 34, Issue 1",March 2010,"Organizations today outsource diverse business processes to achieve a wide variety of business objectives ranging from reduction of costs to innovation and business transformation. We build on the information processing view of the firm to theorize that performance heterogeneity across business process outsourcing (BPO) exchanges is a function of the design of information capabilities (IC) that fit the unique information requirements (IR) of the exchange. Further, we compare performance effects of the fit between IR and IC across dominant categories of BPO relationships to provide insights into the relative benefits of enacting such fit between the constructs. Empirical tests of our hypotheses using survey data on 127 active BPO relationships find a significant increase (decrease) in satisfaction as a result of the fit (misfit) between IR and IC of the relationship. The results have implications for how BPO relationships must be designed and managed to realize significant performance gains. The study also extends the IPV to identify IC that provide the incentives and means to process information in an interfirm relationship.",[],"Barua, Anitesh",N/A,"McCombs School of Business, University of Texas at Austin, 1 University Station, B6500, Austin, TX 78712, U.S.A."
https://misq.umn.edu/misq/article/34/1/39/482/An-Empirical-Analysis-of-the-Impact-of-Information,MIS Quarterly,An Empirical Analysis of the Impact of Information Capabilities Design on Business Process Outsourcing Performance1,"Volume 34, Issue 1",March 2010,"Organizations today outsource diverse business processes to achieve a wide variety of business objectives ranging from reduction of costs to innovation and business transformation. We build on the information processing view of the firm to theorize that performance heterogeneity across business process outsourcing (BPO) exchanges is a function of the design of information capabilities (IC) that fit the unique information requirements (IR) of the exchange. Further, we compare performance effects of the fit between IR and IC across dominant categories of BPO relationships to provide insights into the relative benefits of enacting such fit between the constructs. Empirical tests of our hypotheses using survey data on 127 active BPO relationships find a significant increase (decrease) in satisfaction as a result of the fit (misfit) between IR and IC of the relationship. The results have implications for how BPO relationships must be designed and managed to realize significant performance gains. The study also extends the IPV to identify IC that provide the incentives and means to process information in an interfirm relationship.",[],"Whinston, Andrew",N/A,"McCombs School of Business, University of Texas at Austin, 1 University Station, B6500, Austin, TX 78712, U.S.A."
https://misq.umn.edu/misq/article/34/1/63/484/Chasing-the-Hottest-IT-Effects-of-Information,MIS Quarterly,Chasing the Hottest IT: Effects of Information Technology Fashion on Organizations1,"Volume 34, Issue 1",March 2010,"What happens to organizations that chase the hottest information technologies? This study examines some of the important organizational impacts of the fashion phenomenon in IT. An IT fashion is a transitory collective belief that an information technology is new, efficient, and at the forefront of practice. Using data collected from published discourse and annual IT budgets of 109 large companies for a decade, I have found that firms whose names were associated with IT fashions in the press did not have higher performance, but they had better reputation and higher executive compensation in the near term. Companies investing in IT in fashion also had higher reputation and executive pay, but they had lower performance in the short term and then improved performance in the long term. These results support a fashion explanation for the middle phase diffusion of IT innovations, illustrating that following fashion can legitimize organizations and their leaders regardless of performance improvement. The findings also extend institutional theory from its usual focus on taken-for-granted practices to fashion as a novel source of social approval. This study suggests that practitioners balance between performance pressure and social approval when they confront whatever is hottest in IT.",[],"Wang, Ping",N/A,"College of Information Studies, University of Maryland at College Park, 4105 Hornbake Building, South Wing, College Park, MD 20742, U.S.A."
https://misq.umn.edu/misq/article/34/1/87/494/Toward-Agile-An-Integrated-Analysis-of,MIS Quarterly,Toward Agile: An Integrated Analysis of Quantitative and Qualitative Field Data on Software Development Agility1,"Volume 34, Issue 1",March 2010,"As business and technology environments change at an unprecedented rate, software development agility to respond to changing user requirements has become increasingly critical for software development performance. Agile software development approaches, which emphasize sense-and-respond, self-organization, cross-functional teams, and continuous adaptation, have been adopted by an increasing number of organizations to improve their software development agility. However, the agile development literature is largely anecdotal and prescriptive, lacking empirical evidence and theoretical foundation to support the principles and practices of agile development. Little research has empirically examined the software development agility construct in terms of its dimensions, determinants, and effects on software development performance. As a result, there is a lack of understanding about how organizations can effectively implement an agile development approach.Using an integrated research approach that combines quantitative and qualitative data analyses, this research opens the black box of agile development by empirically examining the relationships among two dimensions of software development agility (software team response extensiveness and software team response efficiency), two antecedents that can be controlled (team autonomy and team diversity), and three aspects of software development performance (on-time completion, on-budget completion, and software functionality). Our PLS results of survey responses of 399 software project managers suggest that the relationships among these variables are more complex than what has been perceived by the literature. The results suggest a tradeoff relationship between response extensiveness and response efficiency. These two agility dimensions impact software development performance differently: response efficiency positively affects all of on-time completion, on-budget completion, and software functionality, whereas response extensiveness positively affects only software functionality. The results also suggest that team autonomy has a positive effect on response efficiency and a negative effect on response extensiveness, and that team diversity has a positive effect on response extensiveness. We conducted 10 post hoc case studies to qualitatively cross-validate our PLS results and provide rich, additional insights regarding the complex, dynamic interplays between autonomy, diversity, agility, and performance. The qualitative analysis also provides explanations for both supported and unsupported hypotheses. We discuss these qualitative analysis results and conclude with the theoretical and practical implications of our research findings for agile development approaches.",[],"Lee, Gwanhoo",N/A,"Department of Information Technology, Kogod School of Business, American University, 4400 Massachusetts Avenue NW, Washington, DC 20016-8044, U.S.A."
https://misq.umn.edu/misq/article/34/1/87/494/Toward-Agile-An-Integrated-Analysis-of,MIS Quarterly,Toward Agile: An Integrated Analysis of Quantitative and Qualitative Field Data on Software Development Agility1,"Volume 34, Issue 1",March 2010,"As business and technology environments change at an unprecedented rate, software development agility to respond to changing user requirements has become increasingly critical for software development performance. Agile software development approaches, which emphasize sense-and-respond, self-organization, cross-functional teams, and continuous adaptation, have been adopted by an increasing number of organizations to improve their software development agility. However, the agile development literature is largely anecdotal and prescriptive, lacking empirical evidence and theoretical foundation to support the principles and practices of agile development. Little research has empirically examined the software development agility construct in terms of its dimensions, determinants, and effects on software development performance. As a result, there is a lack of understanding about how organizations can effectively implement an agile development approach.Using an integrated research approach that combines quantitative and qualitative data analyses, this research opens the black box of agile development by empirically examining the relationships among two dimensions of software development agility (software team response extensiveness and software team response efficiency), two antecedents that can be controlled (team autonomy and team diversity), and three aspects of software development performance (on-time completion, on-budget completion, and software functionality). Our PLS results of survey responses of 399 software project managers suggest that the relationships among these variables are more complex than what has been perceived by the literature. The results suggest a tradeoff relationship between response extensiveness and response efficiency. These two agility dimensions impact software development performance differently: response efficiency positively affects all of on-time completion, on-budget completion, and software functionality, whereas response extensiveness positively affects only software functionality. The results also suggest that team autonomy has a positive effect on response efficiency and a negative effect on response extensiveness, and that team diversity has a positive effect on response extensiveness. We conducted 10 post hoc case studies to qualitatively cross-validate our PLS results and provide rich, additional insights regarding the complex, dynamic interplays between autonomy, diversity, agility, and performance. The qualitative analysis also provides explanations for both supported and unsupported hypotheses. We discuss these qualitative analysis results and conclude with the theoretical and practical implications of our research findings for agile development approaches.",[],"Xia, Weidong",N/A,"Department of Decision Sciences and, Information Systems, College of Business Administration, Florida International University, 11200 SW 8 Street, Miami, FL 33199"
https://misq.umn.edu/misq/article/34/1/115/485/Vital-Signs-for-Virtual-Teams-An-Empirically,MIS Quarterly,Vital Signs for Virtual Teams: An Empirically Developed Trigger Model for Technology Adaptation Interventions1,"Volume 34, Issue 1",March 2010,"This study explores how team leaders sense the need for technology adaptation intervention in distributed, computer-mediated (“virtual”) teams. Analysis and coding of critical incident data collected in interviews of practicing leaders produce a five-trigger model including (1) external constraint, (2) internal constraint, (3) information and communication technology (ICT) inadequacy, (4) ICT knowledge, skills, and abilities inadequacy, and (5) trust and relationship inadequacies. The resulting five-trigger model provides several key contributions including (1) a diagnostic tool for examining real, multi-trigger team technology adaptation contexts, enabling better leader training and evaluation as well as improved research on team technology adaptation and interventions and (2) a better understanding of the relationship between the technology structure strength indicators in adaptive structuration theory and the need for team technology adaptation intervention.",[],"Thomas, Dominic M.",N/A,"Decision and Information Analysis Area, Goizueta Business School, Emory University, 1300 Clinton Road, Atlanta, GA 30322, U.S.A."
https://misq.umn.edu/misq/article/34/1/115/485/Vital-Signs-for-Virtual-Teams-An-Empirically,MIS Quarterly,Vital Signs for Virtual Teams: An Empirically Developed Trigger Model for Technology Adaptation Interventions1,"Volume 34, Issue 1",March 2010,"This study explores how team leaders sense the need for technology adaptation intervention in distributed, computer-mediated (“virtual”) teams. Analysis and coding of critical incident data collected in interviews of practicing leaders produce a five-trigger model including (1) external constraint, (2) internal constraint, (3) information and communication technology (ICT) inadequacy, (4) ICT knowledge, skills, and abilities inadequacy, and (5) trust and relationship inadequacies. The resulting five-trigger model provides several key contributions including (1) a diagnostic tool for examining real, multi-trigger team technology adaptation contexts, enabling better leader training and evaluation as well as improved research on team technology adaptation and interventions and (2) a better understanding of the relationship between the technology structure strength indicators in adaptive structuration theory and the need for team technology adaptation intervention.",[],"Bostrom, Robert P.",N/A,"MIS Department, Terry College of Business, University of Georgia, 312 Brooks Hall, Athens, GA 30602, U.S.A."
https://misq.umn.edu/misq/article/34/1/143/496/Job-Characteristics-and-Job-Satisfaction,MIS Quarterly,Job Characteristics and Job Satisfaction: Understanding the Role of Enterprise Resource Planning System Implementation1,"Volume 34, Issue 1",March 2010,"Little research has examined the impacts of enterprise resource planning (ERP) systems implementation on job satisfaction. Based on a 12-month study of 2,794 employees in a telecommunications firm, we found that ERP system implementation moderated the relationships between three job characteristics (skill variety, autonomy, and feedback) and job satisfaction. Our findings highlight the key role that ERP system implementation can have in altering well-established relationships in the context of technology-enabled organizational change situations. This work also extends research on technology diffusion by moving beyond a focus on technology-centric outcomes, such as system use, to understanding broader job outcomes.",[],"Morris, Michael G.",N/A,"McIntire School of Commerce, University of Virginia, Charlottesville, VA 22904, U.S.A."
https://misq.umn.edu/misq/article/34/1/143/496/Job-Characteristics-and-Job-Satisfaction,MIS Quarterly,Job Characteristics and Job Satisfaction: Understanding the Role of Enterprise Resource Planning System Implementation1,"Volume 34, Issue 1",March 2010,"Little research has examined the impacts of enterprise resource planning (ERP) systems implementation on job satisfaction. Based on a 12-month study of 2,794 employees in a telecommunications firm, we found that ERP system implementation moderated the relationships between three job characteristics (skill variety, autonomy, and feedback) and job satisfaction. Our findings highlight the key role that ERP system implementation can have in altering well-established relationships in the context of technology-enabled organizational change situations. This work also extends research on technology diffusion by moving beyond a focus on technology-centric outcomes, such as system use, to understanding broader job outcomes.",[],"Venkatesh, Viswanath",N/A,"Walton College of Business, University of Arkansas, Fayetteville, AR 72701, U.S.A."
https://misq.umn.edu/misq/article/34/1/163/492/The-Formation-and-Value-of-IT-Enabled-Resources,MIS Quarterly,The Formation and Value of IT-Enabled Resources: Antecedents and Consequences of Synergistic Relationships1,"Volume 34, Issue 1",March 2010,"This paper informs the literature on the business value of information technology by conceptualizing a path from IT assets—that is, commodity-like or off-the-shelf information technologies—to sustainable competitive advantage. This path suggests that IT assets can play a strategic role when they are combined with organizational resources to create IT-enabled resources. To the extent that relationships between IT assets and organizational resources are synergistic, the ensuing IT-enabled resources are capable of positively affecting firms’ sustainable competitive advantage via their improved strategic potential. This is an important contribution since IT-related organizational benefits have been hard to demonstrate despite attempts to study them through a variety of methods and theoretical lenses. This paper synthesizes systems theory and the resource-based view of the firm to build a unified conceptual model linking IT assets with firm-level benefits. Several propositions are derived from the model and their implications for IS research and practice are discussed.",[],"Nevo, Saggi",N/A,"Information Technology Management, School of Business, University at Albany, 1400 Washington Avenue, Albany, NY 12222, U.S.A."
https://misq.umn.edu/misq/article/34/1/163/492/The-Formation-and-Value-of-IT-Enabled-Resources,MIS Quarterly,The Formation and Value of IT-Enabled Resources: Antecedents and Consequences of Synergistic Relationships1,"Volume 34, Issue 1",March 2010,"This paper informs the literature on the business value of information technology by conceptualizing a path from IT assets—that is, commodity-like or off-the-shelf information technologies—to sustainable competitive advantage. This path suggests that IT assets can play a strategic role when they are combined with organizational resources to create IT-enabled resources. To the extent that relationships between IT assets and organizational resources are synergistic, the ensuing IT-enabled resources are capable of positively affecting firms’ sustainable competitive advantage via their improved strategic potential. This is an important contribution since IT-related organizational benefits have been hard to demonstrate despite attempts to study them through a variety of methods and theoretical lenses. This paper synthesizes systems theory and the resource-based view of the firm to build a unified conceptual model linking IT assets with firm-level benefits. Several propositions are derived from the model and their implications for IS research and practice are discussed.",[],"Wade, Michael R.",N/A,"Schulich School of Business, York University, 4700 Keele Street, Toronto, ON M3J 1P3, Canada"
https://misq.umn.edu/misq/article/34/1/185/499/What-Makes-a-Helpful-Online-Review-A-Study-of,MIS Quarterly,What Makes a Helpful Online Review? A Study of Customer Reviews on Amazon.Com1,"Volume 34, Issue 1",March 2010,"Customer reviews are increasingly available online for a wide range of products and services. They supplement other information provided by electronic storefronts such as product descriptions, reviews from experts, and personalized advice generated by automated recommendation systems. While researchers have demonstrated the benefits of the presence of customer reviews to an online retailer, a largely uninvestigated issue is what makes customer reviews helpful to a consumer in the process of making a purchase decision. Drawing on the paradigm of search and experience goods from information economics, we develop and test a model of customer review helpfulness. An analysis of 1,587 reviews from Amazon.Com across six products indicated that review extremity, review depth, and product type affect the perceived helpfulness of the review. Product type moderates the effect of review extremity on the helpfulness of the review. For experience goods, reviews with extreme ratings are less helpful than reviews with moderate ratings. For both product types, review depth has a positive effect on the helpfulness of the review, but the product type moderates the effect of review depth on the helpfulness of the review. Review depth has a greater positive effect on the helpfulness of the review for search goods than for experience goods. We discuss the implications of our findings for both theory and practice.",[],"Mudambi, Susan M.",N/A,"Department of Marketing and Supply, Chain Management, Fox School of Business, Temple University, 524 Alter Hall, 1801 Liacouras Walk, Philadelphia, PA 19122, U.S.A."
https://misq.umn.edu/misq/article/34/1/185/499/What-Makes-a-Helpful-Online-Review-A-Study-of,MIS Quarterly,What Makes a Helpful Online Review? A Study of Customer Reviews on Amazon.Com1,"Volume 34, Issue 1",March 2010,"Customer reviews are increasingly available online for a wide range of products and services. They supplement other information provided by electronic storefronts such as product descriptions, reviews from experts, and personalized advice generated by automated recommendation systems. While researchers have demonstrated the benefits of the presence of customer reviews to an online retailer, a largely uninvestigated issue is what makes customer reviews helpful to a consumer in the process of making a purchase decision. Drawing on the paradigm of search and experience goods from information economics, we develop and test a model of customer review helpfulness. An analysis of 1,587 reviews from Amazon.Com across six products indicated that review extremity, review depth, and product type affect the perceived helpfulness of the review. Product type moderates the effect of review extremity on the helpfulness of the review. For experience goods, reviews with extreme ratings are less helpful than reviews with moderate ratings. For both product types, review depth has a positive effect on the helpfulness of the review, but the product type moderates the effect of review depth on the helpfulness of the review. Review depth has a greater positive effect on the helpfulness of the review for search goods than for experience goods. We discuss the implications of our findings for both theory and practice.",[],"Schuff, David",N/A,"Department of Management Information Systems, Fox School of Business, Temple University, 207G Speakman Hall, 1810 North 13th Street, Philadelphia, PA 19122, U.S.A."
https://misq.umn.edu/misq/article/34/1/iii/480/Editor-s-CommentsJournal-Quality-and-Citations,MIS Quarterly,Editor’s CommentsJournal Quality and Citations: Common Metrics and Considerations about Their Use,"Volume 34, Issue 1",March 2010,N/A,[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/34/1/iii/480/Editor-s-CommentsJournal-Quality-and-Citations,MIS Quarterly,Editor’s CommentsJournal Quality and Citations: Common Metrics and Considerations about Their Use,"Volume 34, Issue 1",March 2010,N/A,[],"Anderson, Chad",N/A,Georgia State University
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/34/2/213/515/Computing-in-Everyday-Life-A-Call-for-Research-on,MIS Quarterly,Computing in Everyday Life: A Call for Research on Experiential Computing1,"Volume 34, Issue 2",June 2010,"The information systems field emerged as a new discipline of artificial science as a result of intellectual efforts to understand the nature and consequences of computer and communication technology in modern organizations. As the rapid development of digital technology continues to make computers and computing a part of everyday experiences, we are once again in need of a new discipline of the artificial. In this essay, I argue that the IS community must expand its intellectual boundaries by embracing experiential computing as an emerging field of inquiry in order to fill this growing intellectual void. Experiential computing involves digitally mediated embodied experiences in everyday activities through everyday artifacts that have embedded computing capabilities. Experiential computing is enabled by the mediation of four dimensions of human experiences (time, space, actors, and artifacts) through digital technology. Drawing on a research framework that encompasses both behavioral and design sciences, six research opportunities that the IS research community can explore are suggested. Ultimately, I propose that the IS field return to its roots, the science of the artificial, by decisively expanding the scope of its inquiry and establishing a new domain of research on computing in everyday life experiences.",[],"Yoo, Youngjin",N/A,"Fox School of Business, Temple University, Philadelphia, PA 19122, U.S.A."
https://misq.umn.edu/misq/article/34/2/233/511/Information-Systems-Strategy-Reconceptualization,MIS Quarterly,"Information Systems Strategy: Reconceptualization, Measurement, and Implications1","Volume 34, Issue 2",June 2010,"Information systems strategy is of central importance to IS practice and research. Our extensive review of the literature suggests that the concept of IS strategy is a term that is used readily; however, it is also a term that is not fully understood. In this study, we follow a perspective paradigm based on the strategic management literature to define IS strategy as an organizational perspective on the investment in, deployment, use, and management of IS. Through a systematic literature search, we identify the following three conceptions of IS strategy employed implicitly in 48 articles published in leading IS journals that focus on the construct of IS strategy: (1) IS strategy as the use of IS to support business strategy; (2) IS strategy as the master plan of the IS function; and (3) IS strategy as the shared view of the IS role within the organization. We find the third conception best fits our definition of IS strategy. As such, we consequently propose to operationalize IS strategy as the degree to which the organization has a shared perspective to seek innovation through IS. Specifically, our proposed IS strategic typology suggests an organization’s IS strategy falls into one of the two defined categories (i.e., IS innovator or IS conservative) or is simply undefined. We also develop measures for this new typology. We argue that the proposed instrument, which was cross-validated across both chief information officers and senior business executives, has the potential to serve as a diagnostic tool through which the organization can directly assess its IS strategy. We contend that our reconceptualization and operationalization of IS strategy provides theoretical and practical implications that advance the current level of understanding of IS strategy from extant studies within three predominant literature streams: strategic IS planning, IS/business strategic alignment, and competitive use of IS.",[],"Chen, Daniel Q.",N/A,"Information Systems and Supply Chain Management, M. J. Neeley School of Business, Texas Christian University, Fort Worth, TX 76109, U.S.A."
https://misq.umn.edu/misq/article/34/2/233/511/Information-Systems-Strategy-Reconceptualization,MIS Quarterly,"Information Systems Strategy: Reconceptualization, Measurement, and Implications1","Volume 34, Issue 2",June 2010,"Information systems strategy is of central importance to IS practice and research. Our extensive review of the literature suggests that the concept of IS strategy is a term that is used readily; however, it is also a term that is not fully understood. In this study, we follow a perspective paradigm based on the strategic management literature to define IS strategy as an organizational perspective on the investment in, deployment, use, and management of IS. Through a systematic literature search, we identify the following three conceptions of IS strategy employed implicitly in 48 articles published in leading IS journals that focus on the construct of IS strategy: (1) IS strategy as the use of IS to support business strategy; (2) IS strategy as the master plan of the IS function; and (3) IS strategy as the shared view of the IS role within the organization. We find the third conception best fits our definition of IS strategy. As such, we consequently propose to operationalize IS strategy as the degree to which the organization has a shared perspective to seek innovation through IS. Specifically, our proposed IS strategic typology suggests an organization’s IS strategy falls into one of the two defined categories (i.e., IS innovator or IS conservative) or is simply undefined. We also develop measures for this new typology. We argue that the proposed instrument, which was cross-validated across both chief information officers and senior business executives, has the potential to serve as a diagnostic tool through which the organization can directly assess its IS strategy. We contend that our reconceptualization and operationalization of IS strategy provides theoretical and practical implications that advance the current level of understanding of IS strategy from extant studies within three predominant literature streams: strategic IS planning, IS/business strategic alignment, and competitive use of IS.",[],"Mocker, Martin",N/A,"ESB Business School, Reutlingen University, 72762 Reutlingen, GERMANY, Rotterdam School of Management, Erasmus University, 3062 PA Rotterdam, The Netherlands"
https://misq.umn.edu/misq/article/34/2/233/511/Information-Systems-Strategy-Reconceptualization,MIS Quarterly,"Information Systems Strategy: Reconceptualization, Measurement, and Implications1","Volume 34, Issue 2",June 2010,"Information systems strategy is of central importance to IS practice and research. Our extensive review of the literature suggests that the concept of IS strategy is a term that is used readily; however, it is also a term that is not fully understood. In this study, we follow a perspective paradigm based on the strategic management literature to define IS strategy as an organizational perspective on the investment in, deployment, use, and management of IS. Through a systematic literature search, we identify the following three conceptions of IS strategy employed implicitly in 48 articles published in leading IS journals that focus on the construct of IS strategy: (1) IS strategy as the use of IS to support business strategy; (2) IS strategy as the master plan of the IS function; and (3) IS strategy as the shared view of the IS role within the organization. We find the third conception best fits our definition of IS strategy. As such, we consequently propose to operationalize IS strategy as the degree to which the organization has a shared perspective to seek innovation through IS. Specifically, our proposed IS strategic typology suggests an organization’s IS strategy falls into one of the two defined categories (i.e., IS innovator or IS conservative) or is simply undefined. We also develop measures for this new typology. We argue that the proposed instrument, which was cross-validated across both chief information officers and senior business executives, has the potential to serve as a diagnostic tool through which the organization can directly assess its IS strategy. We contend that our reconceptualization and operationalization of IS strategy provides theoretical and practical implications that advance the current level of understanding of IS strategy from extant studies within three predominant literature streams: strategic IS planning, IS/business strategic alignment, and competitive use of IS.",[],"Preston, David S.",N/A,"Information Systems and Supply Chain Management, M. J. Neeley School of Business, Texas Christian University, Fort Worth, TX 76109, U.S.A."
https://misq.umn.edu/misq/article/34/2/233/511/Information-Systems-Strategy-Reconceptualization,MIS Quarterly,"Information Systems Strategy: Reconceptualization, Measurement, and Implications1","Volume 34, Issue 2",June 2010,"Information systems strategy is of central importance to IS practice and research. Our extensive review of the literature suggests that the concept of IS strategy is a term that is used readily; however, it is also a term that is not fully understood. In this study, we follow a perspective paradigm based on the strategic management literature to define IS strategy as an organizational perspective on the investment in, deployment, use, and management of IS. Through a systematic literature search, we identify the following three conceptions of IS strategy employed implicitly in 48 articles published in leading IS journals that focus on the construct of IS strategy: (1) IS strategy as the use of IS to support business strategy; (2) IS strategy as the master plan of the IS function; and (3) IS strategy as the shared view of the IS role within the organization. We find the third conception best fits our definition of IS strategy. As such, we consequently propose to operationalize IS strategy as the degree to which the organization has a shared perspective to seek innovation through IS. Specifically, our proposed IS strategic typology suggests an organization’s IS strategy falls into one of the two defined categories (i.e., IS innovator or IS conservative) or is simply undefined. We also develop measures for this new typology. We argue that the proposed instrument, which was cross-validated across both chief information officers and senior business executives, has the potential to serve as a diagnostic tool through which the organization can directly assess its IS strategy. We contend that our reconceptualization and operationalization of IS strategy provides theoretical and practical implications that advance the current level of understanding of IS strategy from extant studies within three predominant literature streams: strategic IS planning, IS/business strategic alignment, and competitive use of IS.",[],"Teubner, Alexander",N/A,"Department of Information Systems, University of Muenster, 48149 Münster, Germany"
https://misq.umn.edu/misq/article/34/2/261/504/Brand-Positioning-Strategy-Using-Search-Engine,MIS Quarterly,Brand Positioning Strategy Using Search Engine Marketing1,"Volume 34, Issue 2",June 2010,"Whether and how firms can employ relative rankings in search engine results pages (SERPs) to differentiate their brands from competitors in cyberspace remains a critical, puzzling issue in e-commerce research. By synthesizing relevant literature from cognitive psychology, marketing, and e-commerce, this study identifies key contextual factors that are conducive for creating brand positioning online via SERPs. In two experiments, the authors establish that when Internet users’ implicit beliefs (i.e., schema) about the meaning of the display order of search engine results are activated or heightened through feature priming, they will have better recall of an unknown brand that is displayed before the well-known brands in SERPs. Further, those with low Internet search skills tend to evaluate the unknown brand more favorably along the particular brand attribute that activates the search engine ranking schema. This research has both theoretical and practical implications for under- standing the effectiveness of search engine optimization techniques.",[],"Dou, Wenyu",N/A,"Department of Marketing City University of Hong Kong Tat Chee Avenue Kowloon, Hong Kong Sar"
https://misq.umn.edu/misq/article/34/2/261/504/Brand-Positioning-Strategy-Using-Search-Engine,MIS Quarterly,Brand Positioning Strategy Using Search Engine Marketing1,"Volume 34, Issue 2",June 2010,"Whether and how firms can employ relative rankings in search engine results pages (SERPs) to differentiate their brands from competitors in cyberspace remains a critical, puzzling issue in e-commerce research. By synthesizing relevant literature from cognitive psychology, marketing, and e-commerce, this study identifies key contextual factors that are conducive for creating brand positioning online via SERPs. In two experiments, the authors establish that when Internet users’ implicit beliefs (i.e., schema) about the meaning of the display order of search engine results are activated or heightened through feature priming, they will have better recall of an unknown brand that is displayed before the well-known brands in SERPs. Further, those with low Internet search skills tend to evaluate the unknown brand more favorably along the particular brand attribute that activates the search engine ranking schema. This research has both theoretical and practical implications for under- standing the effectiveness of search engine optimization techniques.",[],"Lim, Kai H.",N/A,"Department of Information Systems City University of Hong Kong, Tat Chee Avenue Kowloon, Hong Kong Sar"
https://misq.umn.edu/misq/article/34/2/261/504/Brand-Positioning-Strategy-Using-Search-Engine,MIS Quarterly,Brand Positioning Strategy Using Search Engine Marketing1,"Volume 34, Issue 2",June 2010,"Whether and how firms can employ relative rankings in search engine results pages (SERPs) to differentiate their brands from competitors in cyberspace remains a critical, puzzling issue in e-commerce research. By synthesizing relevant literature from cognitive psychology, marketing, and e-commerce, this study identifies key contextual factors that are conducive for creating brand positioning online via SERPs. In two experiments, the authors establish that when Internet users’ implicit beliefs (i.e., schema) about the meaning of the display order of search engine results are activated or heightened through feature priming, they will have better recall of an unknown brand that is displayed before the well-known brands in SERPs. Further, those with low Internet search skills tend to evaluate the unknown brand more favorably along the particular brand attribute that activates the search engine ranking schema. This research has both theoretical and practical implications for under- standing the effectiveness of search engine optimization techniques.",[],"Su, Chenting",N/A,"Department of Marketing City University of Hong Kong Tat Chee Avenue, Kowloon, Hong Kong Sar"
https://misq.umn.edu/misq/article/34/2/261/504/Brand-Positioning-Strategy-Using-Search-Engine,MIS Quarterly,Brand Positioning Strategy Using Search Engine Marketing1,"Volume 34, Issue 2",June 2010,"Whether and how firms can employ relative rankings in search engine results pages (SERPs) to differentiate their brands from competitors in cyberspace remains a critical, puzzling issue in e-commerce research. By synthesizing relevant literature from cognitive psychology, marketing, and e-commerce, this study identifies key contextual factors that are conducive for creating brand positioning online via SERPs. In two experiments, the authors establish that when Internet users’ implicit beliefs (i.e., schema) about the meaning of the display order of search engine results are activated or heightened through feature priming, they will have better recall of an unknown brand that is displayed before the well-known brands in SERPs. Further, those with low Internet search skills tend to evaluate the unknown brand more favorably along the particular brand attribute that activates the search engine ranking schema. This research has both theoretical and practical implications for under- standing the effectiveness of search engine optimization techniques.",[],"Zhou, Nan",N/A,"Department of Marketing City University of Hong Kong Tat Chee Avenue, Kowloon, Hong Kong Sar"
https://misq.umn.edu/misq/article/34/2/261/504/Brand-Positioning-Strategy-Using-Search-Engine,MIS Quarterly,Brand Positioning Strategy Using Search Engine Marketing1,"Volume 34, Issue 2",June 2010,"Whether and how firms can employ relative rankings in search engine results pages (SERPs) to differentiate their brands from competitors in cyberspace remains a critical, puzzling issue in e-commerce research. By synthesizing relevant literature from cognitive psychology, marketing, and e-commerce, this study identifies key contextual factors that are conducive for creating brand positioning online via SERPs. In two experiments, the authors establish that when Internet users’ implicit beliefs (i.e., schema) about the meaning of the display order of search engine results are activated or heightened through feature priming, they will have better recall of an unknown brand that is displayed before the well-known brands in SERPs. Further, those with low Internet search skills tend to evaluate the unknown brand more favorably along the particular brand attribute that activates the search engine ranking schema. This research has both theoretical and practical implications for under- standing the effectiveness of search engine optimization techniques.",[],"Cui, Nan",N/A,"Department of Marketing Wuhan University Wuhan, China"
https://misq.umn.edu/misq/article/34/2/281/501/Expectation-Disconfirmation-and-Technology,MIS Quarterly,Expectation Disconfirmation and Technology Adoption: Polynomial Modeling and Response Surface Analysis1,"Volume 34, Issue 2",June 2010,"Individual-level information systems adoption research has recently seen the introduction of expectation–disconfirmation theory (EDT) to explain how and why user reactions change over time. This prior research has produced valuable insights into the phenomenon of technology adoption beyond traditional models, such as the technology acceptance model. First, we identify gaps in EDT research that present potential opportunities for advances—specifically, we discuss methodological and analytical limitations in EDT research in information systems and present polynomial modeling and response surface methodology as solutions. Second, we draw from research on cognitive dissonance, realistic job preview, and prospect theory to present a polynomial model of expectation– disconfirmation in information systems. Finally, we test our model using data gathered over a period of 6 months among 1,143 employees being introduced to a new technology. The results confirmed our hypotheses that disconfirmation in general was bad, as evidenced by low behavioral intention to continue using a system for both positive and negative disconfirmation, thus supporting the need for a polynomial model to understand expectation disconfirmation in information systems.",[],"Venkatesh, Viswanath",N/A,"Walton College of Business, University of Arkansas, Fayetteville, AR 72701, U.S.A."
https://misq.umn.edu/misq/article/34/2/281/501/Expectation-Disconfirmation-and-Technology,MIS Quarterly,Expectation Disconfirmation and Technology Adoption: Polynomial Modeling and Response Surface Analysis1,"Volume 34, Issue 2",June 2010,"Individual-level information systems adoption research has recently seen the introduction of expectation–disconfirmation theory (EDT) to explain how and why user reactions change over time. This prior research has produced valuable insights into the phenomenon of technology adoption beyond traditional models, such as the technology acceptance model. First, we identify gaps in EDT research that present potential opportunities for advances—specifically, we discuss methodological and analytical limitations in EDT research in information systems and present polynomial modeling and response surface methodology as solutions. Second, we draw from research on cognitive dissonance, realistic job preview, and prospect theory to present a polynomial model of expectation– disconfirmation in information systems. Finally, we test our model using data gathered over a period of 6 months among 1,143 employees being introduced to a new technology. The results confirmed our hypotheses that disconfirmation in general was bad, as evidenced by low behavioral intention to continue using a system for both positive and negative disconfirmation, thus supporting the need for a polynomial model to understand expectation disconfirmation in information systems.",[],"Goyal, Sandeep",N/A,"College of Business, University of Southern, Indiana Evansville, IN 47712, U.S.A."
https://misq.umn.edu/misq/article/34/2/305/509/A-Multi-Project-Model-of-Key-Factors-Affecting,MIS Quarterly,A Multi-Project Model of Key Factors Affecting Organizational Benefits from Enterprise Systems1,"Volume 34, Issue 2",June 2010,"This paper develops a long-term, multi-project model of factors affecting organizational benefits from enterprise systems (ES), then reports a preliminary test of the model. In the shorter-term half of the model, it is hypothesized that once a system has gone live, two factors, namely functional fit and overcoming organizational inertia, drive organizational benefits flowing from each major ES improvement project. The importance of these factors may vary from project to project. In the long-term half of the model, it is hypothesized that four additional factors, namely integration, process optimization, improved access to information, and on-going major ES business improvement projects, drive organizational benefits from ES over the long term. Preliminary tests of the model were conducted using data from 126 customer presentations from SAP’s 2003 and 2005 Sapphire U.S. conferences. All six factors were found to be important in explaining variance in organizational benefits from enterprise systems from the perspective of senior management.",[],"Seddon, Peter B.",N/A,"Department of Information Systems, University of Melbourne, Victoria 3010, Australia"
https://misq.umn.edu/misq/article/34/2/305/509/A-Multi-Project-Model-of-Key-Factors-Affecting,MIS Quarterly,A Multi-Project Model of Key Factors Affecting Organizational Benefits from Enterprise Systems1,"Volume 34, Issue 2",June 2010,"This paper develops a long-term, multi-project model of factors affecting organizational benefits from enterprise systems (ES), then reports a preliminary test of the model. In the shorter-term half of the model, it is hypothesized that once a system has gone live, two factors, namely functional fit and overcoming organizational inertia, drive organizational benefits flowing from each major ES improvement project. The importance of these factors may vary from project to project. In the long-term half of the model, it is hypothesized that four additional factors, namely integration, process optimization, improved access to information, and on-going major ES business improvement projects, drive organizational benefits from ES over the long term. Preliminary tests of the model were conducted using data from 126 customer presentations from SAP’s 2003 and 2005 Sapphire U.S. conferences. All six factors were found to be important in explaining variance in organizational benefits from enterprise systems from the perspective of senior management.",[],"Calvert, Cheryl",N/A,"Corporate Business Systems, Monash University, Victoria 3800, Australia"
https://misq.umn.edu/misq/article/34/2/305/509/A-Multi-Project-Model-of-Key-Factors-Affecting,MIS Quarterly,A Multi-Project Model of Key Factors Affecting Organizational Benefits from Enterprise Systems1,"Volume 34, Issue 2",June 2010,"This paper develops a long-term, multi-project model of factors affecting organizational benefits from enterprise systems (ES), then reports a preliminary test of the model. In the shorter-term half of the model, it is hypothesized that once a system has gone live, two factors, namely functional fit and overcoming organizational inertia, drive organizational benefits flowing from each major ES improvement project. The importance of these factors may vary from project to project. In the long-term half of the model, it is hypothesized that four additional factors, namely integration, process optimization, improved access to information, and on-going major ES business improvement projects, drive organizational benefits from ES over the long term. Preliminary tests of the model were conducted using data from 126 customer presentations from SAP’s 2003 and 2005 Sapphire U.S. conferences. All six factors were found to be important in explaining variance in organizational benefits from enterprise systems from the perspective of senior management.",[],"Yang, Song",N/A,"Department of Information Systems, University of Melbourne, Victoria 3010, Australia"
https://misq.umn.edu/misq/article/34/2/329/507/Information-About-Information-A-Taxonomy-of-Views1,MIS Quarterly,Information About Information: A Taxonomy of Views1,"Volume 34, Issue 2",June 2010,"“Information” is poorly defined in the Information Systems research literature, and is almost always unspecified, a reflexive, all-purpose but indiscriminant solution to an unbounded variety of problems. We present a taxonomy of four views—token, syntax, representation, and adaptation—to enable scholars and practitioners to specify their concept of information. This taxonomy is normative, but we also provide a background review of the etymology and chronology of information, and we sample uses of the term in current IS research. IS research will improve as the term information, via the taxonomy we contribute, is employed more explicitly and consistently.",[],"McKinney, Earl H.",N/A,"College of Business, Bowling Green State University, BA 337 BGSU, Bowling Green, Ohio 43403, U.S.A."
https://misq.umn.edu/misq/article/34/2/329/507/Information-About-Information-A-Taxonomy-of-Views1,MIS Quarterly,Information About Information: A Taxonomy of Views1,"Volume 34, Issue 2",June 2010,"“Information” is poorly defined in the Information Systems research literature, and is almost always unspecified, a reflexive, all-purpose but indiscriminant solution to an unbounded variety of problems. We present a taxonomy of four views—token, syntax, representation, and adaptation—to enable scholars and practitioners to specify their concept of information. This taxonomy is normative, but we also provide a background review of the etymology and chronology of information, and we sample uses of the term in current IS research. IS research will improve as the term information, via the taxonomy we contribute, is employed more explicitly and consistently.",[],"Yoos, Charles J.",N/A,"School of Business Administration, Fort Lewis College, 1000 Rim Drive, Durango, CO 81301-3999, U.S.A."
https://misq.umn.edu/misq/article/34/2/345/503/Investigating-Two-Contradictory-Views-of-Formative,MIS Quarterly,Investigating Two Contradictory Views of Formative Measurement in Information Systems Research1,"Volume 34, Issue 2",June 2010,"The use of formative measurement in the field of Information Systems has increased, arguably due to statistical tools (e.g., PLS) that can test such models. However, in the literature, there exist two contradictory views on the potential deficiency of formative measurement. While opponents who are critical of formative measurement argue that there are native weaknesses of the formative approach in model estimation, proponents who are in favor of using formative measurement counter that opponents’ research methods in measurement model specification are flawed. The goal of this work is to empirically test these opposing views on whether the alleged estimation instability of formative measurement is due to measurement model misspecification or simply the shortcoming of formative measurement. To assess the integrity of arguments of both parties, we adopt a research design in which four different cases are tested in terms of interpretational confounding and external consistency. We find that regardless of whether there is a specification issue, formative measures can lead to misleading outcomes. Based on the results, we offer guidelines that researchers may adopt in planning and executing data analysis with structural equation modeling. Given that the use of formative measurement is at a critical juncture in the IS field, we believe that the guidelines in this research note are important to promote appropriate use of the approach rather than relegate it to a bandwagon effect.",[],"Kim, Gimun",N/A,"College of Business Administration, Konyang University, Nonsan, Korea"
https://misq.umn.edu/misq/article/34/2/345/503/Investigating-Two-Contradictory-Views-of-Formative,MIS Quarterly,Investigating Two Contradictory Views of Formative Measurement in Information Systems Research1,"Volume 34, Issue 2",June 2010,"The use of formative measurement in the field of Information Systems has increased, arguably due to statistical tools (e.g., PLS) that can test such models. However, in the literature, there exist two contradictory views on the potential deficiency of formative measurement. While opponents who are critical of formative measurement argue that there are native weaknesses of the formative approach in model estimation, proponents who are in favor of using formative measurement counter that opponents’ research methods in measurement model specification are flawed. The goal of this work is to empirically test these opposing views on whether the alleged estimation instability of formative measurement is due to measurement model misspecification or simply the shortcoming of formative measurement. To assess the integrity of arguments of both parties, we adopt a research design in which four different cases are tested in terms of interpretational confounding and external consistency. We find that regardless of whether there is a specification issue, formative measures can lead to misleading outcomes. Based on the results, we offer guidelines that researchers may adopt in planning and executing data analysis with structural equation modeling. Given that the use of formative measurement is at a critical juncture in the IS field, we believe that the guidelines in this research note are important to promote appropriate use of the approach rather than relegate it to a bandwagon effect.",[],"Shin, Bongsik",N/A,"Information and Decision Systems, San Diego State University, San Diego, CA 92182, U.S.A."
https://misq.umn.edu/misq/article/34/2/345/503/Investigating-Two-Contradictory-Views-of-Formative,MIS Quarterly,Investigating Two Contradictory Views of Formative Measurement in Information Systems Research1,"Volume 34, Issue 2",June 2010,"The use of formative measurement in the field of Information Systems has increased, arguably due to statistical tools (e.g., PLS) that can test such models. However, in the literature, there exist two contradictory views on the potential deficiency of formative measurement. While opponents who are critical of formative measurement argue that there are native weaknesses of the formative approach in model estimation, proponents who are in favor of using formative measurement counter that opponents’ research methods in measurement model specification are flawed. The goal of this work is to empirically test these opposing views on whether the alleged estimation instability of formative measurement is due to measurement model misspecification or simply the shortcoming of formative measurement. To assess the integrity of arguments of both parties, we adopt a research design in which four different cases are tested in terms of interpretational confounding and external consistency. We find that regardless of whether there is a specification issue, formative measures can lead to misleading outcomes. Based on the results, we offer guidelines that researchers may adopt in planning and executing data analysis with structural equation modeling. Given that the use of formative measurement is at a critical juncture in the IS field, we believe that the guidelines in this research note are important to promote appropriate use of the approach rather than relegate it to a bandwagon effect.",[],"Grover, Varun",N/A,"Department of Management, Clemson University, 101 Sirrine Hall, Clemson, SC 29634-1305"
https://misq.umn.edu/misq/article/34/2/367/500/Introduction-to-the-Special-Issue-on-Novel,MIS Quarterly,Introduction to the Special Issue on Novel Perspectives on Trust in Information Systems,"Volume 34, Issue 2",June 2010,N/A,[],"Benbasat, Izak",N/A,"Sauder School of Business, University of British Columbia, 2053 Main Maill, Vancouver, BC V6T 1Z2, Canada"
https://misq.umn.edu/misq/article/34/2/367/500/Introduction-to-the-Special-Issue-on-Novel,MIS Quarterly,Introduction to the Special Issue on Novel Perspectives on Trust in Information Systems,"Volume 34, Issue 2",June 2010,N/A,[],"Gefen, David",N/A,"LeBow College of Business, Drexel University, 101 North 33rd Street, Philadelphia, PA 19104, U.S.A."
https://misq.umn.edu/misq/article/34/2/367/500/Introduction-to-the-Special-Issue-on-Novel,MIS Quarterly,Introduction to the Special Issue on Novel Perspectives on Trust in Information Systems,"Volume 34, Issue 2",June 2010,N/A,[],"Pavlou, Paul A.",N/A,"Fox School of Business, Temple University, 1801 Liacouras Walk, Philadelphia, PA 19122, U.S.A."
https://misq.umn.edu/misq/article/34/2/iii/498/Editor-s-CommentsA-Midterm-MIS-Quarterly-Progress,MIS Quarterly,Editor’s CommentsA Midterm MIS Quarterly Progress Report,"Volume 34, Issue 2",June 2010,N/A,[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/34/2/373/497/What-Does-the-Brain-Tell-Us-About-Trust-and,MIS Quarterly,What Does the Brain Tell Us About Trust and Distrust? Evidence from a Functional Neuroimaging Study,"Volume 34, Issue 2",June 2010,"Determining whom to trust and whom to distrust is a major decision in impersonal IT-enabled exchanges. Despite the potential role of both trust and distrust in impersonal exchanges, the information systems literature has primarily focused on trust, alas paying relatively little attention to distrust. Given the importance of studying both trust and distrust, this study aims to shed light on the nature, dimensionality, distinction, and relationship, and relative effects of trust and distrust on economic outcomes in the context of impersonal IT-enabled exchanges between buyers and sellers in online marketplaces.This study uses functional neuroimaging (fMRI) tools to complement psychometric measures of trust and distrust by observing the location, timing, and level of brain activity that underlies trust and distrust and their underlying dimensions. The neural correlates of trust and distrust are identified when subjects interact with four experimentally manipulated seller profiles that differ on their level of trust and distrust. The results show that trust and distrust activate different brain areas and have different effects, helping explain why trust and distrust are distinct constructs associated with different neurological processes. Implications for the nature, distinction and relationship, dimensionality, and effects of trust and distrust are discussed.",[],"Dimoka, Angelika",N/A,"Fox School of Business, Temple University, 1801 Liacouras Walk, Philadelphia, PA 19122, U.S.A."
https://misq.umn.edu/misq/article/34/2/397/517/Are-There-Neural-Gender-Differences-in-Online,MIS Quarterly,Are There Neural Gender Differences in Online Trust? An fMRI Study on the Perceived Trustworthiness of eBay Offers1,"Volume 34, Issue 2",June 2010,"Research provides increasing evidence that women and men differ in their decisions to trust. However, information systems research does not satisfactorily explain why these gender differences exist. One possible reason is that, surprisingly, theoretical concepts often do not address the most obvious factor that influences human behavior: biology. Given the essential role of biological factors—and specifically those of the brain—in decisions to trust, the biological influences should naturally include those related to gender. As trust considerations in economic decision making have become increasingly complex with the expansion of Internet use, understanding the related biological/brain functions and the involvement of gender provides a range of valuable insights.To show empirically that online trust is associated with activity changes in certain brain areas, we used functional magnetic resonance imaging (fMRI). In a laboratory experiment, we captured the brain activity of 10 female and 10 male participants simultaneous to decisions on trustworthiness of eBay offers. We found that most of the brain areas that encode trustworthiness differ between women and men. Moreover, we found that women activated more brain areas than did men. These results confirm the empathizing– systemizing theory, which predicts gender differences in neural information processing modes.In demonstrating that perceived trustworthiness of Internet offers is affected by neurobiology, our study has major implications for both IS research and management. We confirm the value of a category of research heretofore neglected in IS research and practice, and argue that future IS research investigating human behavior should consider the role of biological factors. In practice, biological factors are a significant consideration for management, marketing, and engineering attempts to influence behavior.",[],"Riedl, René",N/A,"Department of Business Informatics – Information Engineering, University of Linz, Altenberger Strasse 69, 4040 Linz, Austria"
https://misq.umn.edu/misq/article/34/2/397/517/Are-There-Neural-Gender-Differences-in-Online,MIS Quarterly,Are There Neural Gender Differences in Online Trust? An fMRI Study on the Perceived Trustworthiness of eBay Offers1,"Volume 34, Issue 2",June 2010,"Research provides increasing evidence that women and men differ in their decisions to trust. However, information systems research does not satisfactorily explain why these gender differences exist. One possible reason is that, surprisingly, theoretical concepts often do not address the most obvious factor that influences human behavior: biology. Given the essential role of biological factors—and specifically those of the brain—in decisions to trust, the biological influences should naturally include those related to gender. As trust considerations in economic decision making have become increasingly complex with the expansion of Internet use, understanding the related biological/brain functions and the involvement of gender provides a range of valuable insights.To show empirically that online trust is associated with activity changes in certain brain areas, we used functional magnetic resonance imaging (fMRI). In a laboratory experiment, we captured the brain activity of 10 female and 10 male participants simultaneous to decisions on trustworthiness of eBay offers. We found that most of the brain areas that encode trustworthiness differ between women and men. Moreover, we found that women activated more brain areas than did men. These results confirm the empathizing– systemizing theory, which predicts gender differences in neural information processing modes.In demonstrating that perceived trustworthiness of Internet offers is affected by neurobiology, our study has major implications for both IS research and management. We confirm the value of a category of research heretofore neglected in IS research and practice, and argue that future IS research investigating human behavior should consider the role of biological factors. In practice, biological factors are a significant consideration for management, marketing, and engineering attempts to influence behavior.",[],"Hubert, Marco",N/A,"Department of Corporate Management, and Economics, Zeppelin University, Am Seemooser Horn 20, 88045 Friedrichshafen, Germany"
https://misq.umn.edu/misq/article/34/2/397/517/Are-There-Neural-Gender-Differences-in-Online,MIS Quarterly,Are There Neural Gender Differences in Online Trust? An fMRI Study on the Perceived Trustworthiness of eBay Offers1,"Volume 34, Issue 2",June 2010,"Research provides increasing evidence that women and men differ in their decisions to trust. However, information systems research does not satisfactorily explain why these gender differences exist. One possible reason is that, surprisingly, theoretical concepts often do not address the most obvious factor that influences human behavior: biology. Given the essential role of biological factors—and specifically those of the brain—in decisions to trust, the biological influences should naturally include those related to gender. As trust considerations in economic decision making have become increasingly complex with the expansion of Internet use, understanding the related biological/brain functions and the involvement of gender provides a range of valuable insights.To show empirically that online trust is associated with activity changes in certain brain areas, we used functional magnetic resonance imaging (fMRI). In a laboratory experiment, we captured the brain activity of 10 female and 10 male participants simultaneous to decisions on trustworthiness of eBay offers. We found that most of the brain areas that encode trustworthiness differ between women and men. Moreover, we found that women activated more brain areas than did men. These results confirm the empathizing– systemizing theory, which predicts gender differences in neural information processing modes.In demonstrating that perceived trustworthiness of Internet offers is affected by neurobiology, our study has major implications for both IS research and management. We confirm the value of a category of research heretofore neglected in IS research and practice, and argue that future IS research investigating human behavior should consider the role of biological factors. In practice, biological factors are a significant consideration for management, marketing, and engineering attempts to influence behavior.",[],"Kenning, Peter",N/A,"Department of Corporate Management, and Economics, Zeppelin University, Am Seemooser Horn 20, 88045 Friedrichshafen, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/34/3/431/1425/Moving-Toward-Black-Hat-Research-in-Information,MIS Quarterly,Moving Toward Black Hat Research in Information Systems Security: An Editorial Introduction to the Special Issue,"Volume 34, Issue 3",September 2010,N/A,[],"Mahmood, M. Adam",N/A,University of Texas at El Paso
https://misq.umn.edu/misq/article/34/3/431/1425/Moving-Toward-Black-Hat-Research-in-Information,MIS Quarterly,Moving Toward Black Hat Research in Information Systems Security: An Editorial Introduction to the Special Issue,"Volume 34, Issue 3",September 2010,N/A,[],"Siponen, Mikko",N/A,"University of Oulu, Finland"
https://misq.umn.edu/misq/article/34/3/431/1425/Moving-Toward-Black-Hat-Research-in-Information,MIS Quarterly,Moving Toward Black Hat Research in Information Systems Security: An Editorial Introduction to the Special Issue,"Volume 34, Issue 3",September 2010,N/A,[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/34/3/431/1425/Moving-Toward-Black-Hat-Research-in-Information,MIS Quarterly,Moving Toward Black Hat Research in Information Systems Security: An Editorial Introduction to the Special Issue,"Volume 34, Issue 3",September 2010,N/A,[],"Rao, H. Raghav",N/A,State University of New York at Buffalo
https://misq.umn.edu/misq/article/34/3/431/1425/Moving-Toward-Black-Hat-Research-in-Information,MIS Quarterly,Moving Toward Black Hat Research in Information Systems Security: An Editorial Introduction to the Special Issue,"Volume 34, Issue 3",September 2010,N/A,[],"Raghu, T. S.",N/A,Arizona State University
https://misq.umn.edu/misq/article/34/3/iii/1424/Editor-s-CommentsMISQ-Inc-or-an-Online-Collective,MIS Quarterly,"Editor’s CommentsMISQ, Inc. or an Online Collective? Is There a Journal Personality and What it Means for Authors","Volume 34, Issue 3",September 2010,N/A,[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/34/3/435/1426/Detecting-Fake-Websites-The-Contribution-of,MIS Quarterly,Detecting Fake Websites: The Contribution of Statistical Learning Theory1,"Volume 34, Issue 3",September 2010,"Fake websites have become increasingly pervasive, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Automated detection systems have emerged as a mechanism for combating fake websites, however most are fairly simplistic in terms of their fraud cues and detection methods employed. Consequently, existing systems are susceptible to the myriad of obfuscation tactics used by fraudsters, resulting in highly ineffective fake website detection performance. In light of these deficiencies, we propose the development of a new class of fake website detection systems that are based on statistical learning theory (SLT). Using a design science approach, a prototype system was developed to demonstrate the potential utility of this class of systems. We conducted a series of experiments, comparing the proposed system against several existing fake website detection systems on a test bed encompassing 900 websites. The results indicate that systems grounded in SLT can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. Given the hefty cost exacted by fake websites, the results have important implications for ecommerce and online security.",[],"Abbasi, Ahmed",N/A,"Sheldon B. Lubar School of Business, University of Wisconsin–Milwaukee, Milwaukee, WI 53201, U.S.A."
https://misq.umn.edu/misq/article/34/3/435/1426/Detecting-Fake-Websites-The-Contribution-of,MIS Quarterly,Detecting Fake Websites: The Contribution of Statistical Learning Theory1,"Volume 34, Issue 3",September 2010,"Fake websites have become increasingly pervasive, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Automated detection systems have emerged as a mechanism for combating fake websites, however most are fairly simplistic in terms of their fraud cues and detection methods employed. Consequently, existing systems are susceptible to the myriad of obfuscation tactics used by fraudsters, resulting in highly ineffective fake website detection performance. In light of these deficiencies, we propose the development of a new class of fake website detection systems that are based on statistical learning theory (SLT). Using a design science approach, a prototype system was developed to demonstrate the potential utility of this class of systems. We conducted a series of experiments, comparing the proposed system against several existing fake website detection systems on a test bed encompassing 900 websites. The results indicate that systems grounded in SLT can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. Given the hefty cost exacted by fake websites, the results have important implications for ecommerce and online security.",[],"Zhang, Zhu",N/A,"Department of Management Information Systems, Eller College of Management, University of Arizona, Tucson, AZ 85721, U.S.A."
https://misq.umn.edu/misq/article/34/3/435/1426/Detecting-Fake-Websites-The-Contribution-of,MIS Quarterly,Detecting Fake Websites: The Contribution of Statistical Learning Theory1,"Volume 34, Issue 3",September 2010,"Fake websites have become increasingly pervasive, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Automated detection systems have emerged as a mechanism for combating fake websites, however most are fairly simplistic in terms of their fraud cues and detection methods employed. Consequently, existing systems are susceptible to the myriad of obfuscation tactics used by fraudsters, resulting in highly ineffective fake website detection performance. In light of these deficiencies, we propose the development of a new class of fake website detection systems that are based on statistical learning theory (SLT). Using a design science approach, a prototype system was developed to demonstrate the potential utility of this class of systems. We conducted a series of experiments, comparing the proposed system against several existing fake website detection systems on a test bed encompassing 900 websites. The results indicate that systems grounded in SLT can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. Given the hefty cost exacted by fake websites, the results have important implications for ecommerce and online security.",[],"Zimbra, David",N/A,"Department of Management Information Systems, Eller College of Management, University of Arizona, Tucson, AZ 85721, U.S.A."
https://misq.umn.edu/misq/article/34/3/435/1426/Detecting-Fake-Websites-The-Contribution-of,MIS Quarterly,Detecting Fake Websites: The Contribution of Statistical Learning Theory1,"Volume 34, Issue 3",September 2010,"Fake websites have become increasingly pervasive, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Automated detection systems have emerged as a mechanism for combating fake websites, however most are fairly simplistic in terms of their fraud cues and detection methods employed. Consequently, existing systems are susceptible to the myriad of obfuscation tactics used by fraudsters, resulting in highly ineffective fake website detection performance. In light of these deficiencies, we propose the development of a new class of fake website detection systems that are based on statistical learning theory (SLT). Using a design science approach, a prototype system was developed to demonstrate the potential utility of this class of systems. We conducted a series of experiments, comparing the proposed system against several existing fake website detection systems on a test bed encompassing 900 websites. The results indicate that systems grounded in SLT can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. Given the hefty cost exacted by fake websites, the results have important implications for ecommerce and online security.",[],"Chen, Hsinchun",N/A,"Department of Management Information Systems, Eller College of Management, University of Arizona, Tucson, AZ 85721, U.S.A."
https://misq.umn.edu/misq/article/34/3/435/1426/Detecting-Fake-Websites-The-Contribution-of,MIS Quarterly,Detecting Fake Websites: The Contribution of Statistical Learning Theory1,"Volume 34, Issue 3",September 2010,"Fake websites have become increasingly pervasive, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Automated detection systems have emerged as a mechanism for combating fake websites, however most are fairly simplistic in terms of their fraud cues and detection methods employed. Consequently, existing systems are susceptible to the myriad of obfuscation tactics used by fraudsters, resulting in highly ineffective fake website detection performance. In light of these deficiencies, we propose the development of a new class of fake website detection systems that are based on statistical learning theory (SLT). Using a design science approach, a prototype system was developed to demonstrate the potential utility of this class of systems. We conducted a series of experiments, comparing the proposed system against several existing fake website detection systems on a test bed encompassing 900 websites. The results indicate that systems grounded in SLT can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. Given the hefty cost exacted by fake websites, the results have important implications for ecommerce and online security.",[],"Nunamaker, Jay F.",N/A,"Department of Management Information Systems, Eller College of Management, University of Arizona, Tucson, AZ 85721, U.S.A."
https://misq.umn.edu/misq/article/34/3/463/1439/Circuits-of-Power-A-Study-of-Mandated-Compliance,MIS Quarterly,Circuits of Power: A Study of Mandated Compliance to an Information Systems Security De Jure Standard in a Government Organization1,"Volume 34, Issue 3",September 2010,"Organizations need to protect information assets against cyber crime, denial-of-service attacks, web hackers, data breaches, identity and credit card theft, and fraud. Criminals often try to achieve financial, political, or personal gain through these attacks, so the threats that their actions prompt are insidious motivators for organizations to adopt information systems security (ISS) approaches. Extant ISS research has traditionally examined ISS in e-commerce business organizations. The present study investigates ISS within government, analyzing power relationships during an ISS standards adoption and accreditation process, where a head of state mandates that all government agencies are to comply with a national de jure ISS standard. Using a canonical action research method, designated managers of ISS services across small, medium, and large agencies were monitored and assessed for progress to accreditation through surveys, interviews, participant observation at round table forums, and focus groups. By 2008, accreditation status across the 89 agencies participating in this study was approximately 33 percent fully accredited, with 67 percent partially compliant. The research uses Clegg’s (1989) circuits of power framework to interpret power, resistance, norms, and cultural relationships in the process of compliance. The paper highlights that a strategy based on organization subunit size is helpful in motivating and assisting organizations to move toward accreditation. Mandated standard accreditation was inhibited by insufficient resource allocation, lack of senior management input, and commitment. Factors contributing to this resistance were group norms and cultural biases.",[],"Smith, Stephen",N/A,"Government Chief Information Office, Department of Commerce, New South Wales Government, Level 21, McKell Building, 2-24 Rawson Place, Sydney, NSW 2000, Australia"
https://misq.umn.edu/misq/article/34/3/463/1439/Circuits-of-Power-A-Study-of-Mandated-Compliance,MIS Quarterly,Circuits of Power: A Study of Mandated Compliance to an Information Systems Security De Jure Standard in a Government Organization1,"Volume 34, Issue 3",September 2010,"Organizations need to protect information assets against cyber crime, denial-of-service attacks, web hackers, data breaches, identity and credit card theft, and fraud. Criminals often try to achieve financial, political, or personal gain through these attacks, so the threats that their actions prompt are insidious motivators for organizations to adopt information systems security (ISS) approaches. Extant ISS research has traditionally examined ISS in e-commerce business organizations. The present study investigates ISS within government, analyzing power relationships during an ISS standards adoption and accreditation process, where a head of state mandates that all government agencies are to comply with a national de jure ISS standard. Using a canonical action research method, designated managers of ISS services across small, medium, and large agencies were monitored and assessed for progress to accreditation through surveys, interviews, participant observation at round table forums, and focus groups. By 2008, accreditation status across the 89 agencies participating in this study was approximately 33 percent fully accredited, with 67 percent partially compliant. The research uses Clegg’s (1989) circuits of power framework to interpret power, resistance, norms, and cultural relationships in the process of compliance. The paper highlights that a strategy based on organization subunit size is helpful in motivating and assisting organizations to move toward accreditation. Mandated standard accreditation was inhibited by insufficient resource allocation, lack of senior management input, and commitment. Factors contributing to this resistance were group norms and cultural biases.",[],"Winchester, Donald",N/A,"School of Information Systems, Technology and Management, The Australian School of Business, University of New South Wales, ANZAC Parade, Kensington, Sydney, NSW 2052, Australia"
https://misq.umn.edu/misq/article/34/3/463/1439/Circuits-of-Power-A-Study-of-Mandated-Compliance,MIS Quarterly,Circuits of Power: A Study of Mandated Compliance to an Information Systems Security De Jure Standard in a Government Organization1,"Volume 34, Issue 3",September 2010,"Organizations need to protect information assets against cyber crime, denial-of-service attacks, web hackers, data breaches, identity and credit card theft, and fraud. Criminals often try to achieve financial, political, or personal gain through these attacks, so the threats that their actions prompt are insidious motivators for organizations to adopt information systems security (ISS) approaches. Extant ISS research has traditionally examined ISS in e-commerce business organizations. The present study investigates ISS within government, analyzing power relationships during an ISS standards adoption and accreditation process, where a head of state mandates that all government agencies are to comply with a national de jure ISS standard. Using a canonical action research method, designated managers of ISS services across small, medium, and large agencies were monitored and assessed for progress to accreditation through surveys, interviews, participant observation at round table forums, and focus groups. By 2008, accreditation status across the 89 agencies participating in this study was approximately 33 percent fully accredited, with 67 percent partially compliant. The research uses Clegg’s (1989) circuits of power framework to interpret power, resistance, norms, and cultural relationships in the process of compliance. The paper highlights that a strategy based on organization subunit size is helpful in motivating and assisting organizations to move toward accreditation. Mandated standard accreditation was inhibited by insufficient resource allocation, lack of senior management input, and commitment. Factors contributing to this resistance were group norms and cultural biases.",[],"Bunker, Deborah",N/A,"Discipline of Business Information Systems, Faculty of Economics and Business, University of Sydney, Economics & Business Building H69, Codrington & Rose Streets, Sydney, NSW 2006, Australia"
https://misq.umn.edu/misq/article/34/3/463/1439/Circuits-of-Power-A-Study-of-Mandated-Compliance,MIS Quarterly,Circuits of Power: A Study of Mandated Compliance to an Information Systems Security De Jure Standard in a Government Organization1,"Volume 34, Issue 3",September 2010,"Organizations need to protect information assets against cyber crime, denial-of-service attacks, web hackers, data breaches, identity and credit card theft, and fraud. Criminals often try to achieve financial, political, or personal gain through these attacks, so the threats that their actions prompt are insidious motivators for organizations to adopt information systems security (ISS) approaches. Extant ISS research has traditionally examined ISS in e-commerce business organizations. The present study investigates ISS within government, analyzing power relationships during an ISS standards adoption and accreditation process, where a head of state mandates that all government agencies are to comply with a national de jure ISS standard. Using a canonical action research method, designated managers of ISS services across small, medium, and large agencies were monitored and assessed for progress to accreditation through surveys, interviews, participant observation at round table forums, and focus groups. By 2008, accreditation status across the 89 agencies participating in this study was approximately 33 percent fully accredited, with 67 percent partially compliant. The research uses Clegg’s (1989) circuits of power framework to interpret power, resistance, norms, and cultural relationships in the process of compliance. The paper highlights that a strategy based on organization subunit size is helpful in motivating and assisting organizations to move toward accreditation. Mandated standard accreditation was inhibited by insufficient resource allocation, lack of senior management input, and commitment. Factors contributing to this resistance were group norms and cultural biases.",[],"Jamieson, Rodger",N/A,"School of Information Systems, Technology and Management, The Australian School of Business, University of New South Wales, ANZAC Parade, Kensington, Sydney, NSW 2052, Australia"
https://misq.umn.edu/misq/article/34/3/487/1443/Neutralization-New-Insights-into-the-Problem-of,MIS Quarterly,Neutralization: New Insights into the Problem of Employee Information Systems Security Policy Violations1,"Volume 34, Issue 3",September 2010,"Employees’ failure to comply with information systems security policies is a major concern for information technology security managers. In efforts to understand this problem, IS security researchers have traditionally viewed violations of IS security policies through the lens of deterrence theory. In this article, we show that neutralization theory, a theory prominent in Criminology but not yet applied in the context of IS, provides a compelling explanation for IS security policy violations and offers new insight into how employees rationalize this behavior. In doing so, we propose a theoretical model in which the effects of neutralization techniques are tested alongside those of sanctions described by deterrence theory. Our empirical results highlight neutralization as an important factor to take into account with regard to developing and implementing organizational security policies and practices.",[],"Siponen, Mikko",N/A,"Information Systems Security Research Centre, Department of Information Processing Science, University of Oulu, Linnanmaa, Oulu 3000, Finland"
https://misq.umn.edu/misq/article/34/3/487/1443/Neutralization-New-Insights-into-the-Problem-of,MIS Quarterly,Neutralization: New Insights into the Problem of Employee Information Systems Security Policy Violations1,"Volume 34, Issue 3",September 2010,"Employees’ failure to comply with information systems security policies is a major concern for information technology security managers. In efforts to understand this problem, IS security researchers have traditionally viewed violations of IS security policies through the lens of deterrence theory. In this article, we show that neutralization theory, a theory prominent in Criminology but not yet applied in the context of IS, provides a compelling explanation for IS security policy violations and offers new insight into how employees rationalize this behavior. In doing so, we propose a theoretical model in which the effects of neutralization techniques are tested alongside those of sanctions described by deterrence theory. Our empirical results highlight neutralization as an important factor to take into account with regard to developing and implementing organizational security policies and practices.",[],"Vance, Anthony",N/A,"Information Systems Department, Marriott School of Management, Brigham Young University, Provo, Utah 84602, U.S.A."
https://misq.umn.edu/misq/article/34/3/503/1428/User-Participation-in-Information-Systems-Security,MIS Quarterly,User Participation in Information Systems Security Risk Management1,"Volume 34, Issue 3",September 2010,"This paper examines user participation in information systems security risk management and its influence in the context of regulatory compliance via a multi-method study at the organizational level. First, eleven informants across five organizations were interviewed to gain an understanding of the types of activities and security controls in which users participated as part of Sarbanes-Oxley compliance, along with associated outcomes. A research model was developed based on the findings of the qualitative study and extant user participation theories in the systems development literature. Analysis of the data collected in a questionnaire survey of 228 members of ISACA, a professional association specialized in information technology governance, audit, and security, supported the research model. The findings of the two studies converged and indicated that user participation contributed to improved security control performance through greater awareness, greater alignment between IS security risk management and the business environment, and improved control development. While the IS security literature often portrays users as the weak link in security, the current study suggests that users may be an important resource to IS security by providing needed business knowledge that contributes to more effective security measures. User participation is also a means to engage users in protecting sensitive information in their business processes.",[],"Spears, Janine L.",N/A,"DePaul University, 243 South Wabash Avenue, Chicago, IL 60604, U.S.A."
https://misq.umn.edu/misq/article/34/3/503/1428/User-Participation-in-Information-Systems-Security,MIS Quarterly,User Participation in Information Systems Security Risk Management1,"Volume 34, Issue 3",September 2010,"This paper examines user participation in information systems security risk management and its influence in the context of regulatory compliance via a multi-method study at the organizational level. First, eleven informants across five organizations were interviewed to gain an understanding of the types of activities and security controls in which users participated as part of Sarbanes-Oxley compliance, along with associated outcomes. A research model was developed based on the findings of the qualitative study and extant user participation theories in the systems development literature. Analysis of the data collected in a questionnaire survey of 228 members of ISACA, a professional association specialized in information technology governance, audit, and security, supported the research model. The findings of the two studies converged and indicated that user participation contributed to improved security control performance through greater awareness, greater alignment between IS security risk management and the business environment, and improved control development. While the IS security literature often portrays users as the weak link in security, the current study suggests that users may be an important resource to IS security by providing needed business knowledge that contributes to more effective security measures. User participation is also a means to engage users in protecting sensitive information in their business processes.",[],"Barki, Henri",N/A,"HEC Montréal, 3000, chemin de la Côte-Ste-Catherine, Montréal, QC H3T2A7, Canada"
https://misq.umn.edu/misq/article/34/3/523/1436/Information-Security-Policy-Compliance-An,MIS Quarterly,Information Security Policy Compliance: An Empirical Study of Rationality-Based Beliefs and Information Security Awareness1,"Volume 34, Issue 3",September 2010,"Many organizations recognize that their employees, who are often considered the weakest link in information security, can also be great assets in the effort to reduce risk related to information security. Since employees who comply with the information security rules and regulations of the organization are the key to strengthening information security, understanding compliance behavior is crucial for organizations that want to leverage their human capital.This research identifies the antecedents of employee compliance with the information security policy (ISP) of an organization. Specifically, we investigate the rationality-based factors that drive an employee to comply with requirements of the ISP with regard to protecting the organization’s information and technology resources. Drawing on the theory of planned behavior, we posit that, along with normative belief and self-efficacy, an employee’s attitude toward compliance determines intention to comply with the ISP. As a key contribution, we posit that an employee’s attitude is influenced by benefit of compliance, cost of compliance, and cost of noncompliance, which are beliefs about the overall assessment of consequences of compliance or noncompliance. We then postulate that these beliefs are shaped by the employee’s outcome beliefs concerning the events that follow compliance or noncompliance: benefit of compliance is shaped by intrinsic benefit, safety of resources, and rewards, while cost of compliance is shaped by work impediment; and cost of noncompliance is shaped by intrinsic cost, vulnerability of resources, and sanctions. We also investigate the impact of information security awareness (ISA) on outcome beliefs and an employee’s attitude toward compliance with the ISP.Our results show that an employee’s intention to comply with the ISP is significantly influenced by attitude, normative beliefs, and self-efficacy to comply. Outcome beliefs significantly affect beliefs about overall assessment of consequences, and they, in turn, significantly affect an employee’s attitude. Furthermore, ISA positively affects both attitude and outcome beliefs. As the importance of employees’ following their organizations’ information security rules and regulations increases, our study sheds light on the role of ISA and compliance-related beliefs in an organization’s efforts to encourage compliance.",[],"Bulgurcu, Burcu",N/A,"Sauder School of Business, University of British Columbia, Vancouver, BC V6T 1Z2, Canada"
https://misq.umn.edu/misq/article/34/3/523/1436/Information-Security-Policy-Compliance-An,MIS Quarterly,Information Security Policy Compliance: An Empirical Study of Rationality-Based Beliefs and Information Security Awareness1,"Volume 34, Issue 3",September 2010,"Many organizations recognize that their employees, who are often considered the weakest link in information security, can also be great assets in the effort to reduce risk related to information security. Since employees who comply with the information security rules and regulations of the organization are the key to strengthening information security, understanding compliance behavior is crucial for organizations that want to leverage their human capital.This research identifies the antecedents of employee compliance with the information security policy (ISP) of an organization. Specifically, we investigate the rationality-based factors that drive an employee to comply with requirements of the ISP with regard to protecting the organization’s information and technology resources. Drawing on the theory of planned behavior, we posit that, along with normative belief and self-efficacy, an employee’s attitude toward compliance determines intention to comply with the ISP. As a key contribution, we posit that an employee’s attitude is influenced by benefit of compliance, cost of compliance, and cost of noncompliance, which are beliefs about the overall assessment of consequences of compliance or noncompliance. We then postulate that these beliefs are shaped by the employee’s outcome beliefs concerning the events that follow compliance or noncompliance: benefit of compliance is shaped by intrinsic benefit, safety of resources, and rewards, while cost of compliance is shaped by work impediment; and cost of noncompliance is shaped by intrinsic cost, vulnerability of resources, and sanctions. We also investigate the impact of information security awareness (ISA) on outcome beliefs and an employee’s attitude toward compliance with the ISP.Our results show that an employee’s intention to comply with the ISP is significantly influenced by attitude, normative beliefs, and self-efficacy to comply. Outcome beliefs significantly affect beliefs about overall assessment of consequences, and they, in turn, significantly affect an employee’s attitude. Furthermore, ISA positively affects both attitude and outcome beliefs. As the importance of employees’ following their organizations’ information security rules and regulations increases, our study sheds light on the role of ISA and compliance-related beliefs in an organization’s efforts to encourage compliance.",[],"Cavusoglu, Hasan",N/A,"Sauder School of Business, University of British Columbia, Vancouver, BC V6T 1Z2, Canada"
https://misq.umn.edu/misq/article/34/3/523/1436/Information-Security-Policy-Compliance-An,MIS Quarterly,Information Security Policy Compliance: An Empirical Study of Rationality-Based Beliefs and Information Security Awareness1,"Volume 34, Issue 3",September 2010,"Many organizations recognize that their employees, who are often considered the weakest link in information security, can also be great assets in the effort to reduce risk related to information security. Since employees who comply with the information security rules and regulations of the organization are the key to strengthening information security, understanding compliance behavior is crucial for organizations that want to leverage their human capital.This research identifies the antecedents of employee compliance with the information security policy (ISP) of an organization. Specifically, we investigate the rationality-based factors that drive an employee to comply with requirements of the ISP with regard to protecting the organization’s information and technology resources. Drawing on the theory of planned behavior, we posit that, along with normative belief and self-efficacy, an employee’s attitude toward compliance determines intention to comply with the ISP. As a key contribution, we posit that an employee’s attitude is influenced by benefit of compliance, cost of compliance, and cost of noncompliance, which are beliefs about the overall assessment of consequences of compliance or noncompliance. We then postulate that these beliefs are shaped by the employee’s outcome beliefs concerning the events that follow compliance or noncompliance: benefit of compliance is shaped by intrinsic benefit, safety of resources, and rewards, while cost of compliance is shaped by work impediment; and cost of noncompliance is shaped by intrinsic cost, vulnerability of resources, and sanctions. We also investigate the impact of information security awareness (ISA) on outcome beliefs and an employee’s attitude toward compliance with the ISP.Our results show that an employee’s intention to comply with the ISP is significantly influenced by attitude, normative beliefs, and self-efficacy to comply. Outcome beliefs significantly affect beliefs about overall assessment of consequences, and they, in turn, significantly affect an employee’s attitude. Furthermore, ISA positively affects both attitude and outcome beliefs. As the importance of employees’ following their organizations’ information security rules and regulations increases, our study sheds light on the role of ISA and compliance-related beliefs in an organization’s efforts to encourage compliance.",[],"Benbasat, Izak",N/A,"Sauder School of Business, University of British Columbia, Vancouver, BC V6T 1Z2, Canada"
https://misq.umn.edu/misq/article/34/3/549/1429/Fear-Appeals-and-Information-Security-Behaviors-An,MIS Quarterly,Fear Appeals and Information Security Behaviors: An Empirical Study1,"Volume 34, Issue 3",September 2010,"Information technology executives strive to align the actions of end users with the desired security posture of management and of the firm through persuasive communication. In many cases, some element of fear is incorporated within these communications. However, within the context of computer security and information assurance, it is not yet clear how these fear-inducing arguments, known as fear appeals, will ultimately impact the actions of end users. The purpose of this study is to investigate the influence of fear appeals on the compliance of end users with recommendations to enact specific individual computer security actions toward the mitigation of threats. An examination was performed that culminated in the development and testing of a conceptual model representing an infusion of technology adoption and fear appeal theories.Results of the study suggest that fear appeals do impact end user behavioral intentions to comply with recommended individual acts of security, but the impact is not uniform across all end users. It is determined in part by perceptions of self-efficacy, response efficacy, threat severity, and social influence. The findings of this research contribute to information systems security research, human–computer interaction, and organizational communication by revealing a new paradigm in which IT users form perceptions of the technology, not on the basis of performance gains, but on the basis of utility for threat mitigation.",[],"Johnston, Allen C.",N/A,"Department of Management, Information Systems, and Quantitative Methods, School of Business, University of Alabama at Birmingham, 1530 Third Avenue South, Birmingham, AL 35294-4460, U.S.A."
https://misq.umn.edu/misq/article/34/3/549/1429/Fear-Appeals-and-Information-Security-Behaviors-An,MIS Quarterly,Fear Appeals and Information Security Behaviors: An Empirical Study1,"Volume 34, Issue 3",September 2010,"Information technology executives strive to align the actions of end users with the desired security posture of management and of the firm through persuasive communication. In many cases, some element of fear is incorporated within these communications. However, within the context of computer security and information assurance, it is not yet clear how these fear-inducing arguments, known as fear appeals, will ultimately impact the actions of end users. The purpose of this study is to investigate the influence of fear appeals on the compliance of end users with recommendations to enact specific individual computer security actions toward the mitigation of threats. An examination was performed that culminated in the development and testing of a conceptual model representing an infusion of technology adoption and fear appeal theories.Results of the study suggest that fear appeals do impact end user behavioral intentions to comply with recommended individual acts of security, but the impact is not uniform across all end users. It is determined in part by perceptions of self-efficacy, response efficacy, threat severity, and social influence. The findings of this research contribute to information systems security research, human–computer interaction, and organizational communication by revealing a new paradigm in which IT users form perceptions of the technology, not on the basis of performance gains, but on the basis of utility for threat mitigation.",[],"Warkentin, Merrill",N/A,"Department of Management and Information Systems, College of Business, Mississippi State University, P.O. Box 9581, Mississippi State, MS 39762-9581, U.S.A."
https://misq.umn.edu/misq/article/34/3/567/1441/Market-Value-of-Voluntary-Disclosures-Concerning,MIS Quarterly,Market Value of Voluntary Disclosures Concerning Information Security1,"Volume 34, Issue 3",September 2010,"Information security is a fundamental concern for corporations operating in today’s digital economy. The number of firms disclosing items concerning their information security on reports filed with the U.S. Securities and Exchange Commission (SEC) has increased in recent years. A question then arises as to whether or not there is value to the voluntary disclosures concerning information security. Thus, the primary objective of this paper is to assess empirically the market value of voluntary disclosures of items pertaining to information security. Based on a sample of 1,641 disclosing and 19,266 non-disclosing firm-years in a cross-sectional pooled model, our primary findings provide strong evidence that voluntarily disclosing items concerning information security is associated positively with the market value of a firm. These findings are based on the use of a market-value relevance model, as well as a bid-ask spread analysis. The study’s findings are robust to alternative statistical analyses. The findings also provide support for the signaling argument, which states that managers disclose information in a manner consistent with increased firm value. Finally, the study findings provide some insight into the strategic choice that firms make regarding voluntary disclosures about information security.",[],"Gordon, Lawrence A.",N/A,"Robert H. Smith School of Business, University of Maryland, 4332F Van Munching Hall, College Park, MD 20742-1815, U.S.A."
https://misq.umn.edu/misq/article/34/3/567/1441/Market-Value-of-Voluntary-Disclosures-Concerning,MIS Quarterly,Market Value of Voluntary Disclosures Concerning Information Security1,"Volume 34, Issue 3",September 2010,"Information security is a fundamental concern for corporations operating in today’s digital economy. The number of firms disclosing items concerning their information security on reports filed with the U.S. Securities and Exchange Commission (SEC) has increased in recent years. A question then arises as to whether or not there is value to the voluntary disclosures concerning information security. Thus, the primary objective of this paper is to assess empirically the market value of voluntary disclosures of items pertaining to information security. Based on a sample of 1,641 disclosing and 19,266 non-disclosing firm-years in a cross-sectional pooled model, our primary findings provide strong evidence that voluntarily disclosing items concerning information security is associated positively with the market value of a firm. These findings are based on the use of a market-value relevance model, as well as a bid-ask spread analysis. The study’s findings are robust to alternative statistical analyses. The findings also provide support for the signaling argument, which states that managers disclose information in a manner consistent with increased firm value. Finally, the study findings provide some insight into the strategic choice that firms make regarding voluntary disclosures about information security.",[],"Loeb, Martin P.",N/A,"Robert H. Smith School of Business, University of Maryland, 4333L Van Munching Hall, College Park, MD 20742-1815, U.S.A."
https://misq.umn.edu/misq/article/34/3/567/1441/Market-Value-of-Voluntary-Disclosures-Concerning,MIS Quarterly,Market Value of Voluntary Disclosures Concerning Information Security1,"Volume 34, Issue 3",September 2010,"Information security is a fundamental concern for corporations operating in today’s digital economy. The number of firms disclosing items concerning their information security on reports filed with the U.S. Securities and Exchange Commission (SEC) has increased in recent years. A question then arises as to whether or not there is value to the voluntary disclosures concerning information security. Thus, the primary objective of this paper is to assess empirically the market value of voluntary disclosures of items pertaining to information security. Based on a sample of 1,641 disclosing and 19,266 non-disclosing firm-years in a cross-sectional pooled model, our primary findings provide strong evidence that voluntarily disclosing items concerning information security is associated positively with the market value of a firm. These findings are based on the use of a market-value relevance model, as well as a bid-ask spread analysis. The study’s findings are robust to alternative statistical analyses. The findings also provide support for the signaling argument, which states that managers disclose information in a manner consistent with increased firm value. Finally, the study findings provide some insight into the strategic choice that firms make regarding voluntary disclosures about information security.",[],"Sohail, Tashfeen",N/A,"IE Business School, Madrid, Calle Pinar 15 - 1B, 28006 Madrid, Spain"
https://misq.umn.edu/misq/article/34/3/595/1423/The-Impact-of-Malicious-Agents-on-the-Enterprise,MIS Quarterly,The Impact of Malicious Agents on the Enterprise Software Industry1,"Volume 34, Issue 3",September 2010,"In this paper, a competitive software market that includes horizontal and quality differentiation, as well as a negative network effect driven by the presence of malicious agents, is modeled. Software products with larger installed bases, and therefore more potential computers to attack, present more appealing targets for malicious agents. One finding is that software firms may profit from increased malicious activity. Software products in a more competitive market are less likely to invest in security, while monopolistic or niche products are likely to be more secure from malicious attack. The results provide insights for IS managers considering enterprise software adoption.",[],"Galbreth, Michael R.",N/A,"Department of Management Science, Moore School of Business, University of South Carolina, Columbia, SC 29208, U.S.A."
https://misq.umn.edu/misq/article/34/3/595/1423/The-Impact-of-Malicious-Agents-on-the-Enterprise,MIS Quarterly,The Impact of Malicious Agents on the Enterprise Software Industry1,"Volume 34, Issue 3",September 2010,"In this paper, a competitive software market that includes horizontal and quality differentiation, as well as a negative network effect driven by the presence of malicious agents, is modeled. Software products with larger installed bases, and therefore more potential computers to attack, present more appealing targets for malicious agents. One finding is that software firms may profit from increased malicious activity. Software products in a more competitive market are less likely to invest in security, while monopolistic or niche products are likely to be more secure from malicious attack. The results provide insights for IS managers considering enterprise software adoption.",[],"Shor, Mikhael",N/A,"Department of Economics, Owen Graduate School of Management, Vanderbilt University, Nashville, TN 37240, U.S.A."
https://misq.umn.edu/misq/article/34/3/613/1433/Practicing-Safe-Computing-A-Multimethod-Empirical,MIS Quarterly,Practicing Safe Computing: A Multimethod Empirical Examination of Home Computer User Security Behavioral Intentions1,"Volume 34, Issue 3",September 2010,"Although firms are expending substantial resources to develop technology and processes that can help safeguard the security of their computing assets, increased attention is being focused on the role people play in maintaining a safe computing environment. Unlike employees in a work setting, home users are not subject to training, nor are they protected by a technical staff dedicated to keeping security software and hardware current. Thus, with over one billion people with access to the Internet, individual home computer users represent a significant point of weakness in achieving the security of the cyber infrastructure. We study the phenomenon of conscientious cybercitizens, defined as individuals who are motivated to take the necessary precautions under their direct control to secure their own computer and the Internet in a home setting. Using a multidisciplinary, phased approach, we develop a conceptual model of the conscientious cybercitizen. We present results from two studies—a survey and an experiment—conducted to understand the drivers of intentions to perform security-related behavior, and the interventions that can positively influence these drivers. In the first study, we use protection motivation theory as the underlying conceptual foundation and extend the theory by drawing upon the public goods literature and the concept of psychological ownership. Results from a survey of 594 home computer users from a wide range of demographic and socioeconomic backgrounds suggest that a home computer user’s intention to perform security-related behavior is influenced by a combination of cognitive, social, and psychological components. In the second study, we draw upon the concepts of goal framing and self-view to examine how the proximal drivers of intentions to perform security-related behavior identified in the first study can be influenced by appropriate messaging. An experiment with 101 subjects is used to test the research hypotheses. Overall, the two studies shed important new light on creating more conscientious cybercitizens. Theoretical and practical implications of the findings are discussed.2",[],"Anderson, Catherine L.",N/A,"Decision, Operations, and Information Technologies Department, Robert H. Smith School of Business, University of Maryland, Van Munching Hall, College Park, MD 20742-1815, U.S.A."
https://misq.umn.edu/misq/article/34/3/613/1433/Practicing-Safe-Computing-A-Multimethod-Empirical,MIS Quarterly,Practicing Safe Computing: A Multimethod Empirical Examination of Home Computer User Security Behavioral Intentions1,"Volume 34, Issue 3",September 2010,"Although firms are expending substantial resources to develop technology and processes that can help safeguard the security of their computing assets, increased attention is being focused on the role people play in maintaining a safe computing environment. Unlike employees in a work setting, home users are not subject to training, nor are they protected by a technical staff dedicated to keeping security software and hardware current. Thus, with over one billion people with access to the Internet, individual home computer users represent a significant point of weakness in achieving the security of the cyber infrastructure. We study the phenomenon of conscientious cybercitizens, defined as individuals who are motivated to take the necessary precautions under their direct control to secure their own computer and the Internet in a home setting. Using a multidisciplinary, phased approach, we develop a conceptual model of the conscientious cybercitizen. We present results from two studies—a survey and an experiment—conducted to understand the drivers of intentions to perform security-related behavior, and the interventions that can positively influence these drivers. In the first study, we use protection motivation theory as the underlying conceptual foundation and extend the theory by drawing upon the public goods literature and the concept of psychological ownership. Results from a survey of 594 home computer users from a wide range of demographic and socioeconomic backgrounds suggest that a home computer user’s intention to perform security-related behavior is influenced by a combination of cognitive, social, and psychological components. In the second study, we draw upon the concepts of goal framing and self-view to examine how the proximal drivers of intentions to perform security-related behavior identified in the first study can be influenced by appropriate messaging. An experiment with 101 subjects is used to test the research hypotheses. Overall, the two studies shed important new light on creating more conscientious cybercitizens. Theoretical and practical implications of the findings are discussed.2",[],"Agarwal, Ritu",N/A,"Center for Health Information and Decision Systems, Robert H. Smith School of Business, University of Maryland, 4327 Van Munching Hall, College Park, MD 20742-1815, U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/34/4/647/506/Focus-and-Diversity-in-Information-Systems,MIS Quarterly,Focus and Diversity in Information Systems Research: Meeting the Dual Demands of a Healthy Applied Discipline1,"Volume 34, Issue 4",December 2010,"Drawing on sociology of science foundations, we argue that, in order to survive and prosper, healthy applied disciplines must meet the dual demands of academic and practitioner audiences by demonstrating both focus and diversity in their research. First, we use this concomitant modality to explain why previous studies into the structure of the Information Systems discipline have reported contradictory results, with some finding a focused field while others conclude that the field is diverse. In support of our argument, we then present the results of a longitudinal, author co-citation analysis, looking across the full range of journals in which IS research is published. Our results suggest that the IS field has sustained a focus on research within three subfields over a 20year period from 1986 to 2005: (1) a thematic miscellany of research on development, implementation, and use of systems in various application domains; (2) IS strategy and business outcomes; and (3) group work and decision support. At the same time, the field has demonstrated considerable diversity within and around these core subfields, with researchers responding flexibly to the rapidly changing field by investigating these areas with new paradigms and in new contexts, and by exploring new topics including inter-business and Internet applications, computer-supported collaborative work, virtual teams, and knowledge management. Finally, we demonstrate that, over the 20-year period from 1986 to 2005, the discipline has shifted from fragmented adhocracy to a polycentric state, which is particularly appropriate to an applied discipline such as IS that must address the dual demands of focus and diversity in a rapidly changing technological context.",[],"Taylor, Hazel",N/A,"Information School, University of Washington, Box 352840, Seattle, WA 98195-2840, U.S.A."
https://misq.umn.edu/misq/article/34/4/647/506/Focus-and-Diversity-in-Information-Systems,MIS Quarterly,Focus and Diversity in Information Systems Research: Meeting the Dual Demands of a Healthy Applied Discipline1,"Volume 34, Issue 4",December 2010,"Drawing on sociology of science foundations, we argue that, in order to survive and prosper, healthy applied disciplines must meet the dual demands of academic and practitioner audiences by demonstrating both focus and diversity in their research. First, we use this concomitant modality to explain why previous studies into the structure of the Information Systems discipline have reported contradictory results, with some finding a focused field while others conclude that the field is diverse. In support of our argument, we then present the results of a longitudinal, author co-citation analysis, looking across the full range of journals in which IS research is published. Our results suggest that the IS field has sustained a focus on research within three subfields over a 20year period from 1986 to 2005: (1) a thematic miscellany of research on development, implementation, and use of systems in various application domains; (2) IS strategy and business outcomes; and (3) group work and decision support. At the same time, the field has demonstrated considerable diversity within and around these core subfields, with researchers responding flexibly to the rapidly changing field by investigating these areas with new paradigms and in new contexts, and by exploring new topics including inter-business and Internet applications, computer-supported collaborative work, virtual teams, and knowledge management. Finally, we demonstrate that, over the 20-year period from 1986 to 2005, the discipline has shifted from fragmented adhocracy to a polycentric state, which is particularly appropriate to an applied discipline such as IS that must address the dual demands of focus and diversity in a rapidly changing technological context.",[],"Dillon, Stuart",N/A,"University of Waikato, Private Bag 3105, Hamilton, 3240, New Zealand"
https://misq.umn.edu/misq/article/34/4/647/506/Focus-and-Diversity-in-Information-Systems,MIS Quarterly,Focus and Diversity in Information Systems Research: Meeting the Dual Demands of a Healthy Applied Discipline1,"Volume 34, Issue 4",December 2010,"Drawing on sociology of science foundations, we argue that, in order to survive and prosper, healthy applied disciplines must meet the dual demands of academic and practitioner audiences by demonstrating both focus and diversity in their research. First, we use this concomitant modality to explain why previous studies into the structure of the Information Systems discipline have reported contradictory results, with some finding a focused field while others conclude that the field is diverse. In support of our argument, we then present the results of a longitudinal, author co-citation analysis, looking across the full range of journals in which IS research is published. Our results suggest that the IS field has sustained a focus on research within three subfields over a 20year period from 1986 to 2005: (1) a thematic miscellany of research on development, implementation, and use of systems in various application domains; (2) IS strategy and business outcomes; and (3) group work and decision support. At the same time, the field has demonstrated considerable diversity within and around these core subfields, with researchers responding flexibly to the rapidly changing field by investigating these areas with new paradigms and in new contexts, and by exploring new topics including inter-business and Internet applications, computer-supported collaborative work, virtual teams, and knowledge management. Finally, we demonstrate that, over the 20-year period from 1986 to 2005, the discipline has shifted from fragmented adhocracy to a polycentric state, which is particularly appropriate to an applied discipline such as IS that must address the dual demands of focus and diversity in a rapidly changing technological context.",[],"Van Wingen, Melinda",N/A,"Everett Public Library, 2702 Hoyt Avenue, Everett, WA 98201, U.S.A."
https://misq.umn.edu/misq/article/34/4/669/508/Web-2-0-and-Politics-The-2008-U-S-Presidential,MIS Quarterly,Web 2.0 and Politics: The 2008 U.S. Presidential Election and an E-Politics Research Agenda1,"Volume 34, Issue 4",December 2010,"The Internet was a major factor in the 2008 U.S. presidential campaign and has become an important tool for political communication and persuasion. Yet, information systems research is generally silent on the role of the Internet in politics. In this paper, we argue that IS is positioned to enhance understanding of the influence of the Internet on politics, and, more specifically, the process of election campaigning using Internet-based technologies such as Web 2.0. In this paper, we discuss how these technologies can change the nature of competition in politics and replace or complement traditional media. Our empirical study on how Web 2.0 technologies were used by the candidates leading up to the 2008 U.S. presidential primaries sheds light on how these technologies influenced candidate performance. Finally, we outline a research agenda highlighting where IS can contribute to the academic discourse on e-politics.",[],"Wattal, Sunil",N/A,"Department of Management Information Systems, Fox School of Business, Temple University, Philadelphia, PA 19122, U.S.A."
https://misq.umn.edu/misq/article/34/4/669/508/Web-2-0-and-Politics-The-2008-U-S-Presidential,MIS Quarterly,Web 2.0 and Politics: The 2008 U.S. Presidential Election and an E-Politics Research Agenda1,"Volume 34, Issue 4",December 2010,"The Internet was a major factor in the 2008 U.S. presidential campaign and has become an important tool for political communication and persuasion. Yet, information systems research is generally silent on the role of the Internet in politics. In this paper, we argue that IS is positioned to enhance understanding of the influence of the Internet on politics, and, more specifically, the process of election campaigning using Internet-based technologies such as Web 2.0. In this paper, we discuss how these technologies can change the nature of competition in politics and replace or complement traditional media. Our empirical study on how Web 2.0 technologies were used by the candidates leading up to the 2008 U.S. presidential primaries sheds light on how these technologies influenced candidate performance. Finally, we outline a research agenda highlighting where IS can contribute to the academic discourse on e-politics.",[],"Schuff, David",N/A,"Department of Management Information Systems, Fox School of Business, Temple University, Philadelphia, PA 19122, U.S.A."
https://misq.umn.edu/misq/article/34/4/669/508/Web-2-0-and-Politics-The-2008-U-S-Presidential,MIS Quarterly,Web 2.0 and Politics: The 2008 U.S. Presidential Election and an E-Politics Research Agenda1,"Volume 34, Issue 4",December 2010,"The Internet was a major factor in the 2008 U.S. presidential campaign and has become an important tool for political communication and persuasion. Yet, information systems research is generally silent on the role of the Internet in politics. In this paper, we argue that IS is positioned to enhance understanding of the influence of the Internet on politics, and, more specifically, the process of election campaigning using Internet-based technologies such as Web 2.0. In this paper, we discuss how these technologies can change the nature of competition in politics and replace or complement traditional media. Our empirical study on how Web 2.0 technologies were used by the candidates leading up to the 2008 U.S. presidential primaries sheds light on how these technologies influenced candidate performance. Finally, we outline a research agenda highlighting where IS can contribute to the academic discourse on e-politics.",[],"Mandviwalla, Munir",N/A,"Department of Management Information Systems, Fox School of Business, Temple University, Philadelphia, PA 19122, U.S.A."
https://misq.umn.edu/misq/article/34/4/669/508/Web-2-0-and-Politics-The-2008-U-S-Presidential,MIS Quarterly,Web 2.0 and Politics: The 2008 U.S. Presidential Election and an E-Politics Research Agenda1,"Volume 34, Issue 4",December 2010,"The Internet was a major factor in the 2008 U.S. presidential campaign and has become an important tool for political communication and persuasion. Yet, information systems research is generally silent on the role of the Internet in politics. In this paper, we argue that IS is positioned to enhance understanding of the influence of the Internet on politics, and, more specifically, the process of election campaigning using Internet-based technologies such as Web 2.0. In this paper, we discuss how these technologies can change the nature of competition in politics and replace or complement traditional media. Our empirical study on how Web 2.0 technologies were used by the candidates leading up to the 2008 U.S. presidential primaries sheds light on how these technologies influenced candidate performance. Finally, we outline a research agenda highlighting where IS can contribute to the academic discourse on e-politics.",[],"Williams, Christine B.",N/A,"Department of Global Studies, Bentley University, Waltham, MA 02452-4705, U.S.A."
https://misq.umn.edu/misq/article/34/4/689/516/The-Other-Side-of-Acceptance-Studying-the-Direct,MIS Quarterly,The Other Side of Acceptance: Studying the Direct and Indirect Effects of Emotions on Information Technology Use1,"Volume 34, Issue 4",December 2010,"Much ado has been made regarding user acceptance of new information technologies. However, research has been primarily based on cognitive models and little attention has been given to emotions. This paper argues that emotions are important drivers of behaviors and examines how emotions experienced early in the implementation of new IT applications relate to IT use. We develop a framework that classifies emotions into four distinct types: challenge, achievement, loss, and deterrence emotions. The direct and indirect relationships between four emotions (excitement, happiness, anger, and anxiety) and IT use were studied through a survey of 249 bank account managers. Our results indicate that excitement was positively related to IT use through task adaptation. Happiness was directly positively related to IT use and, surprisingly, was negatively associated with task adaptation, which is a facilitator of IT use. Anger was not related to IT use directly, but it was positively related to seeking social support, which in turn was positively related to IT use. Finally, anxiety was negatively related to IT use, both directly and indirectly through psychological distancing. Anxiety was also indirectly positively related to IT use through seeking social support, which countered the original negative effect of anxiety. Post hoc ANOVAs were conducted to compare IT usage of different groups of users experiencing similar emotions but relying on different adaptation behaviors. The paper shows that emotions felt by users early in the implementation of a new IT have important effects on IT use. As such, the paper provides a complementary perspective to understanding acceptance and antecedents of IT use. By showing the importance and complexity of the relationships between emotions and IT use, the paper calls for more research on the topic.",[],"Beaudry, Anne",N/A,"John Molson School of Business, Concordia University, 1455 de Maisonneuve Boulevard West, Montréal, Québec, Canada"
https://misq.umn.edu/misq/article/34/4/689/516/The-Other-Side-of-Acceptance-Studying-the-Direct,MIS Quarterly,The Other Side of Acceptance: Studying the Direct and Indirect Effects of Emotions on Information Technology Use1,"Volume 34, Issue 4",December 2010,"Much ado has been made regarding user acceptance of new information technologies. However, research has been primarily based on cognitive models and little attention has been given to emotions. This paper argues that emotions are important drivers of behaviors and examines how emotions experienced early in the implementation of new IT applications relate to IT use. We develop a framework that classifies emotions into four distinct types: challenge, achievement, loss, and deterrence emotions. The direct and indirect relationships between four emotions (excitement, happiness, anger, and anxiety) and IT use were studied through a survey of 249 bank account managers. Our results indicate that excitement was positively related to IT use through task adaptation. Happiness was directly positively related to IT use and, surprisingly, was negatively associated with task adaptation, which is a facilitator of IT use. Anger was not related to IT use directly, but it was positively related to seeking social support, which in turn was positively related to IT use. Finally, anxiety was negatively related to IT use, both directly and indirectly through psychological distancing. Anxiety was also indirectly positively related to IT use through seeking social support, which countered the original negative effect of anxiety. Post hoc ANOVAs were conducted to compare IT usage of different groups of users experiencing similar emotions but relying on different adaptation behaviors. The paper shows that emotions felt by users early in the implementation of a new IT have important effects on IT use. As such, the paper provides a complementary perspective to understanding acceptance and antecedents of IT use. By showing the importance and complexity of the relationships between emotions and IT use, the paper calls for more research on the topic.",[],"Pinsonneault, Alain",N/A,"Desautels Faculty of Management, McGill University, 1001 Sherbrooke Street West, Montréal, Québec, Canada"
https://misq.umn.edu/misq/article/34/4/711/512/Affect-in-Web-Interfaces-A-Study-of-the-Impacts-of,MIS Quarterly,Affect in Web Interfaces: A Study of the Impacts of Web Page Visual Complexity and Order1,"Volume 34, Issue 4",December 2010,"This research concentrates on visual complexity and order as central factors in the design of webpages that enhance users’ positive emotional reactions and facilitate desirable psychological states and behaviors. Drawing on existing theories and empirical findings in the environmental psychology, human–computer interaction, and marketing research literatures, a research model is developed to explain the relationships among visual complexity and order design features of a webpage, induced emotional responses in users, and users’ approach behaviors toward the website as moderated by users’ metamotivational states. A laboratory experiment was conducted to test the model and its associated hypotheses. The results of the study suggested that a web user’s initial emotional responses (i.e., pleasantness and arousal), evoked by the visual complexity and order design features of a webpage when first encountered, will have carry-over effects on subsequent approach behavior toward the website. The results also revealed how webpage visual complexity and order influence users’ emotions and behaviors differently when users are in different metamotivational states. The salience and importance of webpage visual complexity and order for users’ feelings of pleasantness were largely dependent on users’ metamotivational states.",[],"Deng, Liqiong",N/A,"Richards College of Business, University of West Georgia, 1601 Maple Street, Carrollton, GA 30118, U.S.A."
https://misq.umn.edu/misq/article/34/4/711/512/Affect-in-Web-Interfaces-A-Study-of-the-Impacts-of,MIS Quarterly,Affect in Web Interfaces: A Study of the Impacts of Web Page Visual Complexity and Order1,"Volume 34, Issue 4",December 2010,"This research concentrates on visual complexity and order as central factors in the design of webpages that enhance users’ positive emotional reactions and facilitate desirable psychological states and behaviors. Drawing on existing theories and empirical findings in the environmental psychology, human–computer interaction, and marketing research literatures, a research model is developed to explain the relationships among visual complexity and order design features of a webpage, induced emotional responses in users, and users’ approach behaviors toward the website as moderated by users’ metamotivational states. A laboratory experiment was conducted to test the model and its associated hypotheses. The results of the study suggested that a web user’s initial emotional responses (i.e., pleasantness and arousal), evoked by the visual complexity and order design features of a webpage when first encountered, will have carry-over effects on subsequent approach behavior toward the website. The results also revealed how webpage visual complexity and order influence users’ emotions and behaviors differently when users are in different metamotivational states. The salience and importance of webpage visual complexity and order for users’ feelings of pleasantness were largely dependent on users’ metamotivational states.",[],"Poole, Marshall Scott",N/A,"Department of Communication, University of Illinois at Urbana-Champaign, 1207 W. Oregon Street, Urbana, IL 61801, U.S.A."
https://misq.umn.edu/misq/article/34/4/731/513/Understanding-Organization-Enterprise-System-Fit-A,MIS Quarterly,Understanding Organization—Enterprise System Fit: A Path to Theorizing the Information Technology Artifact1,"Volume 34, Issue 4",December 2010,"Packaged software applications such as enterprise systems are designed to support generic rather than specific requirements, and hence are likely to be an imperfect fit in any particular instance. Using critical realism as our philosophical perspective, we conducted a three-year qualitative study of misfits that arose from an enterprise system (ES) implementation. A detailed analysis of the observed misfits resulted in a richer understanding of the concept of fit and of the ES artifact itself. Specifically, we found six misfit domains (functionality, data, usability, role, control and organizational culture) and within each, two types of misfit (deficiencies and impositions). These misfit types correspond to two newly defined types of fit: fit as coverage and fit as enablement. Our analysis of fit also revealed a new conceptualization of the ES artifact, with implications for IT artifacts in general.",[],"Strong, Diane M.",N/A,"School of Business, Worcester Polytechnic Institute, 100 Institute Road, Worcester, MA 01609, U.S.A."
https://misq.umn.edu/misq/article/34/4/731/513/Understanding-Organization-Enterprise-System-Fit-A,MIS Quarterly,Understanding Organization—Enterprise System Fit: A Path to Theorizing the Information Technology Artifact1,"Volume 34, Issue 4",December 2010,"Packaged software applications such as enterprise systems are designed to support generic rather than specific requirements, and hence are likely to be an imperfect fit in any particular instance. Using critical realism as our philosophical perspective, we conducted a three-year qualitative study of misfits that arose from an enterprise system (ES) implementation. A detailed analysis of the observed misfits resulted in a richer understanding of the concept of fit and of the ES artifact itself. Specifically, we found six misfit domains (functionality, data, usability, role, control and organizational culture) and within each, two types of misfit (deficiencies and impositions). These misfit types correspond to two newly defined types of fit: fit as coverage and fit as enablement. Our analysis of fit also revealed a new conceptualization of the ES artifact, with implications for IT artifacts in general.",[],"Volkoff, Olga",N/A,"Faculty of Business Administration, Simon Fraser University, 8888 University Drive, Burnaby, British Columbia V5A 1S6, Canada"
https://misq.umn.edu/misq/article/34/4/757/510/Improving-Employees-Compliance-Through-Information,MIS Quarterly,Improving Employees' Compliance Through Information Systems Security Training: An Action Research Study1,"Volume 34, Issue 4",December 2010,"Employee noncompliance with information systems security policies is a key concern for organizations. If users do not comply with IS security policies, security solutions lose their efficacy. Of the different IS security policy compliance approaches, training is the most commonly suggested in the literature. Yet, few of the existing studies about training to promote IS policy compliance utilize theory to explain what learning principles affect user compliance with IS security policies, or offer empirical evidence of their practical effectiveness. Consequently, there is a need for IS security training approaches that are theory-based and empirically evaluated. Accordingly, we propose a training program based on two theories: the universal constructive instructional theory and the elaboration likelihood model. We then validate the training program for IS security policy compliance training through an action research project. The action research intervention suggests that the theory-based training achieved positive results and was practical to deploy. Moreover, the intervention suggests that information security training should utilize contents and methods that activate and motivate the learners to systematic cognitive processing of information they receive during the training. In addition, the action research study made clear that a continuous communication process was also required to improve user IS security policy compliance. The findings of this study offer new insights for scholars and practitioners involved in IS security policy compliance.",[],"Puhakainen, Petri",N/A,"IS Security Research Center, Department of Information Processing Science, University of Oulu, Oulu, Finland"
https://misq.umn.edu/misq/article/34/4/757/510/Improving-Employees-Compliance-Through-Information,MIS Quarterly,Improving Employees' Compliance Through Information Systems Security Training: An Action Research Study1,"Volume 34, Issue 4",December 2010,"Employee noncompliance with information systems security policies is a key concern for organizations. If users do not comply with IS security policies, security solutions lose their efficacy. Of the different IS security policy compliance approaches, training is the most commonly suggested in the literature. Yet, few of the existing studies about training to promote IS policy compliance utilize theory to explain what learning principles affect user compliance with IS security policies, or offer empirical evidence of their practical effectiveness. Consequently, there is a need for IS security training approaches that are theory-based and empirically evaluated. Accordingly, we propose a training program based on two theories: the universal constructive instructional theory and the elaboration likelihood model. We then validate the training program for IS security policy compliance training through an action research project. The action research intervention suggests that the theory-based training achieved positive results and was practical to deploy. Moreover, the intervention suggests that information security training should utilize contents and methods that activate and motivate the learners to systematic cognitive processing of information they receive during the training. In addition, the action research study made clear that a continuous communication process was also required to improve user IS security policy compliance. The findings of this study offer new insights for scholars and practitioners involved in IS security policy compliance.",[],"Siponen, Mikko",N/A,"IS Security Research Center, Department of Information Processing Science, University of Oulu, Oulu, Finland"
https://misq.umn.edu/misq/article/34/4/779/518/An-Alternative-to-Methodological-Individualism-A,MIS Quarterly,An Alternative to Methodological Individualism: A Non-Reductionist Approach to Studying Technology Adoption by Groups1,"Volume 34, Issue 4",December 2010,"Studies on groups within the MIS discipline have largely been based on the paradigm of methodological individualism. Commentaries on methodological individualism within the reference disciplines suggest that studies embracing this paradigm can lead to potentially misleading or incorrect conclusions. This study illustrates the appropriateness of the alternate non-reductionist approach to investigating group-related phenomenon, specifically in the context of technology adoption. Drawing on theories of group influence, prior research on conflict, technology characteristics, task– technology fit, group communication media, and recent theoretical work surrounding group technology adoption, the paper proposes and empirically tests a new non-reductionist model for conceptualizing technology adoption by groups. Further, the study also empirically compares this non-reductionist model with a (hypothetical) methodological individualist model of technology adoption by groups. Results strongly support most of the assertions of the non-reductionist model and highlight that this model provides a more robust explanation of technology adoption by groups than a methodological individualist view. Further, the study also highlights some conditions wherein the methodological individualist view fails to provide correct explanations. The implications of the study’s findings for future research are discussed.",[],"Sarker, Saonee",N/A,"Department of Entrepreneurship and Information Systems, College of Business, Washington State University, Pullman, WA 99163, U.S.A."
https://misq.umn.edu/misq/article/34/4/779/518/An-Alternative-to-Methodological-Individualism-A,MIS Quarterly,An Alternative to Methodological Individualism: A Non-Reductionist Approach to Studying Technology Adoption by Groups1,"Volume 34, Issue 4",December 2010,"Studies on groups within the MIS discipline have largely been based on the paradigm of methodological individualism. Commentaries on methodological individualism within the reference disciplines suggest that studies embracing this paradigm can lead to potentially misleading or incorrect conclusions. This study illustrates the appropriateness of the alternate non-reductionist approach to investigating group-related phenomenon, specifically in the context of technology adoption. Drawing on theories of group influence, prior research on conflict, technology characteristics, task– technology fit, group communication media, and recent theoretical work surrounding group technology adoption, the paper proposes and empirically tests a new non-reductionist model for conceptualizing technology adoption by groups. Further, the study also empirically compares this non-reductionist model with a (hypothetical) methodological individualist model of technology adoption by groups. Results strongly support most of the assertions of the non-reductionist model and highlight that this model provides a more robust explanation of technology adoption by groups than a methodological individualist view. Further, the study also highlights some conditions wherein the methodological individualist view fails to provide correct explanations. The implications of the study’s findings for future research are discussed.",[],"Valacich, Joseph S.",N/A,"Department of Entrepreneurship and Information Systems, College of Business, Washington State University, Pullman, WA 99163, U.S.A."
https://misq.umn.edu/misq/article/34/4/809/514/Price-Effects-in-Online-Product-Reviews-An,MIS Quarterly,Price Effects in Online Product Reviews: An Analytical Model and Empirical Analysis1,"Volume 34, Issue 4",December 2010,"Consumer reviews may reflect not only perceived quality but also the difference between quality and price (perceived value). In markets where product prices change frequently, these price-influenced reviews may be biased as a signal of product quality when used by consumers possessing no knowledge of historical prices. In this paper, we develop an analytical model that examines the impact of price-influenced reviews on firm optimal pricing and consumer welfare. We quantify the price effects in consumer reviews for different formats of review systems using actual market prices and online consumer ratings data collected for the digital camera market. Our empirical results suggest that unidimensional ratings, commonly used in most review systems, can be substantially biased by price effects. In fact, unidimensional ratings are more closely correlated with ratings of product value than ratings of product quality. Our findings suggest the importance for firms to account for these price effects in their overall marketing strategy and suggest that review systems could better serve consumers by explicitly expanding review dimensions to separate perceived value and perceived quality.",[],"Li, Xinxin",N/A,"School of Business, University of Connecticut, 2100 Hillside Road U1041, Storrs, CT 06279, U.S.A."
https://misq.umn.edu/misq/article/34/4/809/514/Price-Effects-in-Online-Product-Reviews-An,MIS Quarterly,Price Effects in Online Product Reviews: An Analytical Model and Empirical Analysis1,"Volume 34, Issue 4",December 2010,"Consumer reviews may reflect not only perceived quality but also the difference between quality and price (perceived value). In markets where product prices change frequently, these price-influenced reviews may be biased as a signal of product quality when used by consumers possessing no knowledge of historical prices. In this paper, we develop an analytical model that examines the impact of price-influenced reviews on firm optimal pricing and consumer welfare. We quantify the price effects in consumer reviews for different formats of review systems using actual market prices and online consumer ratings data collected for the digital camera market. Our empirical results suggest that unidimensional ratings, commonly used in most review systems, can be substantially biased by price effects. In fact, unidimensional ratings are more closely correlated with ratings of product value than ratings of product quality. Our findings suggest the importance for firms to account for these price effects in their overall marketing strategy and suggest that review systems could better serve consumers by explicitly expanding review dimensions to separate perceived value and perceived quality.",[],"Hitt, Lorin M.",N/A,"The Wharton School, University of Pennsylvania, 3730 Walnut Street, 500 JMHH, Philadelphia, PA 19104-6381, U.S.A."
https://misq.umn.edu/misq/article/34/4/833/502/Toward-Ethical-Information-Systems-The,MIS Quarterly,Toward Ethical Information Systems: The Contribution of Discourse Ethics1,"Volume 34, Issue 4",December 2010,"Ethics is important in the Information Systems field as illustrated by the direct effect of the Sarbanes-Oxley Act on the work of IS professionals. There is a substantial literature on ethical issues surrounding computing and information technology in the contemporary world, but much of this work is not published nor widely cited in the mainstream IS literature. The purpose of this paper is to offer one contribution to an increased emphasis on ethics in the IS field. The distinctive contribution is a focus on Habermas’s discourse ethics. After outlining some traditional theories of ethics and morality, the literature on IS and ethics is reviewed, and then the paper details the development of discourse ethics. Discourse ethics is different from other approaches to ethics as it is grounded in actual debates between those affected by decisions and proposals. Recognizing that the theory could be considered rather abstract, the paper discusses the need to pragmatize discourse ethics for the IS field through, for example, the use of existing techniques such as soft systems methodology. In addition, the practical potential of the theory is illustrated through a discussion of its application to specific IS topic areas including Web 2.0, open source software, the digital divide, and the UK biometric identity card scheme. The final section summarizes ways in which the paper could be used in IS research, teaching, and practice.",[],"Mingers, John",N/A,"Kent Business School, University of Kent, Canterbury CT7 2PE, United Kingdom"
https://misq.umn.edu/misq/article/34/4/833/502/Toward-Ethical-Information-Systems-The,MIS Quarterly,Toward Ethical Information Systems: The Contribution of Discourse Ethics1,"Volume 34, Issue 4",December 2010,"Ethics is important in the Information Systems field as illustrated by the direct effect of the Sarbanes-Oxley Act on the work of IS professionals. There is a substantial literature on ethical issues surrounding computing and information technology in the contemporary world, but much of this work is not published nor widely cited in the mainstream IS literature. The purpose of this paper is to offer one contribution to an increased emphasis on ethics in the IS field. The distinctive contribution is a focus on Habermas’s discourse ethics. After outlining some traditional theories of ethics and morality, the literature on IS and ethics is reviewed, and then the paper details the development of discourse ethics. Discourse ethics is different from other approaches to ethics as it is grounded in actual debates between those affected by decisions and proposals. Recognizing that the theory could be considered rather abstract, the paper discusses the need to pragmatize discourse ethics for the IS field through, for example, the use of existing techniques such as soft systems methodology. In addition, the practical potential of the theory is illustrated through a discussion of its application to specific IS topic areas including Web 2.0, open source software, the digital divide, and the UK biometric identity card scheme. The final section summarizes ways in which the paper could be used in IS research, teaching, and practice.",[],"Walsham, Geoff",N/A,"Judge Business School, University of Cambridge, Trumpington Street, Cambridge CB2 1AG, United Kingdom"
https://misq.umn.edu/misq/article/34/4/855/519/The-Impact-of-Information-Technology-and,MIS Quarterly,"The Impact of Information Technology and Transactive Memory Systems on Knowledge Sharing, Application, and Team Performance: A Field Study1","Volume 34, Issue 4",December 2010,"In contemporary knowledge-based organizations, teams often play an essential role in leveraging knowledge resources. Organizations make significant investments in information technology to support knowledge management practices in teams. At the same time, recent studies show that the transactive memory system (TMS)—the specialized division of cognitive labor among team members that relates to the encoding, storage, and retrieval of knowledge—is an important factor that affects a team’s performance. Yet little is known of how IT support for knowledge management practices in organizations affects the development of TMS. Furthermore, the precise role of TMS on knowledge sharing and knowledge application, which in turn influences team performance, has not been fully explored. In order to close this gap in the literature, we conducted a field study that involved 139 on-going teams of 743 individuals from two major firms in South Korea. Our results show that IT support in organizations has a positive impact on the development of TMS in teams, and that both TMS and IT support have a positive impact on knowledge sharing and knowledge application. Furthermore, we found that knowledge sharing has a positive impact on knowledge application, which in turn has a direct impact on team performance. However, contrary to our expectation, knowledge sharing does not have a direct impact on team performance and its impact on team performance was fully mediated by knowledge application. Our research shows that organizations can improve team members’ meta-knowledge of who knows what through the careful investment in information technology. Finally, our results show that sharing knowledge alone is not enough. Organizations must ensure that shared knowledge is in fact applied in order to improve team performance.",[],"Choi, Sue Young",N/A,"Human Resources Team, Samsung Life Insurance, Seoul, Korea"
https://misq.umn.edu/misq/article/34/4/855/519/The-Impact-of-Information-Technology-and,MIS Quarterly,"The Impact of Information Technology and Transactive Memory Systems on Knowledge Sharing, Application, and Team Performance: A Field Study1","Volume 34, Issue 4",December 2010,"In contemporary knowledge-based organizations, teams often play an essential role in leveraging knowledge resources. Organizations make significant investments in information technology to support knowledge management practices in teams. At the same time, recent studies show that the transactive memory system (TMS)—the specialized division of cognitive labor among team members that relates to the encoding, storage, and retrieval of knowledge—is an important factor that affects a team’s performance. Yet little is known of how IT support for knowledge management practices in organizations affects the development of TMS. Furthermore, the precise role of TMS on knowledge sharing and knowledge application, which in turn influences team performance, has not been fully explored. In order to close this gap in the literature, we conducted a field study that involved 139 on-going teams of 743 individuals from two major firms in South Korea. Our results show that IT support in organizations has a positive impact on the development of TMS in teams, and that both TMS and IT support have a positive impact on knowledge sharing and knowledge application. Furthermore, we found that knowledge sharing has a positive impact on knowledge application, which in turn has a direct impact on team performance. However, contrary to our expectation, knowledge sharing does not have a direct impact on team performance and its impact on team performance was fully mediated by knowledge application. Our research shows that organizations can improve team members’ meta-knowledge of who knows what through the careful investment in information technology. Finally, our results show that sharing knowledge alone is not enough. Organizations must ensure that shared knowledge is in fact applied in order to improve team performance.",[],"Lee, Heeseok",N/A,"College of Business, Korea Advanced Institute of Science, and Technology, Seoul 130-722, Korea"
https://misq.umn.edu/misq/article/34/4/855/519/The-Impact-of-Information-Technology-and,MIS Quarterly,"The Impact of Information Technology and Transactive Memory Systems on Knowledge Sharing, Application, and Team Performance: A Field Study1","Volume 34, Issue 4",December 2010,"In contemporary knowledge-based organizations, teams often play an essential role in leveraging knowledge resources. Organizations make significant investments in information technology to support knowledge management practices in teams. At the same time, recent studies show that the transactive memory system (TMS)—the specialized division of cognitive labor among team members that relates to the encoding, storage, and retrieval of knowledge—is an important factor that affects a team’s performance. Yet little is known of how IT support for knowledge management practices in organizations affects the development of TMS. Furthermore, the precise role of TMS on knowledge sharing and knowledge application, which in turn influences team performance, has not been fully explored. In order to close this gap in the literature, we conducted a field study that involved 139 on-going teams of 743 individuals from two major firms in South Korea. Our results show that IT support in organizations has a positive impact on the development of TMS in teams, and that both TMS and IT support have a positive impact on knowledge sharing and knowledge application. Furthermore, we found that knowledge sharing has a positive impact on knowledge application, which in turn has a direct impact on team performance. However, contrary to our expectation, knowledge sharing does not have a direct impact on team performance and its impact on team performance was fully mediated by knowledge application. Our research shows that organizations can improve team members’ meta-knowledge of who knows what through the careful investment in information technology. Finally, our results show that sharing knowledge alone is not enough. Organizations must ensure that shared knowledge is in fact applied in order to improve team performance.",[],"Yoo, Youngjin",N/A,"Fox School of Business and Management, Temple University, Philadelphia, PA 19122, U.S.A."
https://misq.umn.edu/misq/article/34/4/iii/505/Editor-s-CommentsAuthorship-Trends-from-2000,MIS Quarterly,Editor’s CommentsAuthorship Trends from 2000 Through 2009,"Volume 34, Issue 4",December 2010,N/A,[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/34/4/iii/505/Editor-s-CommentsAuthorship-Trends-from-2000,MIS Quarterly,Editor’s CommentsAuthorship Trends from 2000 Through 2009,"Volume 34, Issue 4",December 2010,N/A,[],"Anderson, Chad",N/A,Georgia State University
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/35/1/1/1437/Profiling-the-Research-Productivity-of-Tenured,MIS Quarterly,Profiling the Research Productivity of Tenured Information Systems Faculty at U.S. Institutions1,"Volume 35, Issue 1",March 2011,"How many articles in highly rated journals do Information Systems research faculty publish to earn tenure? Which journals are highly rated outlets? Tenure candidates, promotion and tenure committees, and those who are asked to write external letters are frequently called upon to answer such questions. When Dennis et al. (2006) examined all IS Ph.D. graduates entering academic careers, few faculty had published enough articles in 20 “elite” journals in six years to meet tenure research expectations at research-intensive schools. Our study builds on the dialog started by Dennis et al. In our study, we counted the number of journal articles at the point of tenure for faculty who earned tenure within five to seven years after their Ph.D. graduation date. We also examined the effect of acknowledging different sets of journals as highly rated on the publication rates of faculty who earned tenure. Specifically, we examined the effects of expanding on Dennis et al. by including MIS Quarterly, Information Systems Research, Journal of Management Information Systems, Journal of the AIS, Information Systems Journal, European Journal of Information Systems, Journal of Information Technology, and Journal of Strategic Information Systems in the journal basket. We also looked at the effect of acknowledging highly rated non-IS business journals and highly rated computer science and engineering journals. Finally, we present journal publication benchmarks based on these findings for different types of research institutions.",[],"Dean, Douglas L.",N/A,"Information Systems Department, Marriott School of Management, Brigham Young University, Provo, UT 84602 U.S.A."
https://misq.umn.edu/misq/article/35/1/1/1437/Profiling-the-Research-Productivity-of-Tenured,MIS Quarterly,Profiling the Research Productivity of Tenured Information Systems Faculty at U.S. Institutions1,"Volume 35, Issue 1",March 2011,"How many articles in highly rated journals do Information Systems research faculty publish to earn tenure? Which journals are highly rated outlets? Tenure candidates, promotion and tenure committees, and those who are asked to write external letters are frequently called upon to answer such questions. When Dennis et al. (2006) examined all IS Ph.D. graduates entering academic careers, few faculty had published enough articles in 20 “elite” journals in six years to meet tenure research expectations at research-intensive schools. Our study builds on the dialog started by Dennis et al. In our study, we counted the number of journal articles at the point of tenure for faculty who earned tenure within five to seven years after their Ph.D. graduation date. We also examined the effect of acknowledging different sets of journals as highly rated on the publication rates of faculty who earned tenure. Specifically, we examined the effects of expanding on Dennis et al. by including MIS Quarterly, Information Systems Research, Journal of Management Information Systems, Journal of the AIS, Information Systems Journal, European Journal of Information Systems, Journal of Information Technology, and Journal of Strategic Information Systems in the journal basket. We also looked at the effect of acknowledging highly rated non-IS business journals and highly rated computer science and engineering journals. Finally, we present journal publication benchmarks based on these findings for different types of research institutions.",[],"Lowry, Paul Benjamin",N/A,"Information Systems Department, Marriott School of Management, Brigham Young University, Provo, UT 84602 U.S.A."
https://misq.umn.edu/misq/article/35/1/1/1437/Profiling-the-Research-Productivity-of-Tenured,MIS Quarterly,Profiling the Research Productivity of Tenured Information Systems Faculty at U.S. Institutions1,"Volume 35, Issue 1",March 2011,"How many articles in highly rated journals do Information Systems research faculty publish to earn tenure? Which journals are highly rated outlets? Tenure candidates, promotion and tenure committees, and those who are asked to write external letters are frequently called upon to answer such questions. When Dennis et al. (2006) examined all IS Ph.D. graduates entering academic careers, few faculty had published enough articles in 20 “elite” journals in six years to meet tenure research expectations at research-intensive schools. Our study builds on the dialog started by Dennis et al. In our study, we counted the number of journal articles at the point of tenure for faculty who earned tenure within five to seven years after their Ph.D. graduation date. We also examined the effect of acknowledging different sets of journals as highly rated on the publication rates of faculty who earned tenure. Specifically, we examined the effects of expanding on Dennis et al. by including MIS Quarterly, Information Systems Research, Journal of Management Information Systems, Journal of the AIS, Information Systems Journal, European Journal of Information Systems, Journal of Information Technology, and Journal of Strategic Information Systems in the journal basket. We also looked at the effect of acknowledging highly rated non-IS business journals and highly rated computer science and engineering journals. Finally, we present journal publication benchmarks based on these findings for different types of research institutions.",[],"Humpherys, Sean",N/A,"Management Information Systems Department, Eller College of Business, University of Arizona, Tucson, AZ 85721 U.S.A."
https://misq.umn.edu/misq/article/35/1/17/1435/A-Set-of-Principles-for-Conducting-Critical,MIS Quarterly,A Set of Principles for Conducting Critical Research in Information Systems1,"Volume 35, Issue 1",March 2011,"While criteria or principles for conducting positivist and interpretive research have been widely discussed in the IS research literature, criteria or principles for critical research are lacking. Therefore, the purpose of this paper is to propose a set of principles for the conduct of critical research in information systems. We examine the nature of the critical research perspective, clarify its significance, and review its major discourses, recognizing that its mission and methods cannot be captured by a fixed set of criteria once and for all, particularly as multiple approaches are still in the process of defining their identity. However, we suggest it is possible to formulate a set of principles capturing some of the commonalities of those approaches that have so far become most visible in the IS research literature. The usefulness of the principles is illustrated by analyzing three critical field studies in information systems. We hope that this paper will further reflection and debate on the important subject of grounding critical research methodology.",[],"Myers, Michael D.",N/A,"Department of Information Systems and Operations Management, University of Auckland Business School, Auckland, New Zealand 1142"
https://misq.umn.edu/misq/article/35/1/17/1435/A-Set-of-Principles-for-Conducting-Critical,MIS Quarterly,A Set of Principles for Conducting Critical Research in Information Systems1,"Volume 35, Issue 1",March 2011,"While criteria or principles for conducting positivist and interpretive research have been widely discussed in the IS research literature, criteria or principles for critical research are lacking. Therefore, the purpose of this paper is to propose a set of principles for the conduct of critical research in information systems. We examine the nature of the critical research perspective, clarify its significance, and review its major discourses, recognizing that its mission and methods cannot be captured by a fixed set of criteria once and for all, particularly as multiple approaches are still in the process of defining their identity. However, we suggest it is possible to formulate a set of principles capturing some of the commonalities of those approaches that have so far become most visible in the IS research literature. The usefulness of the principles is illustrated by analyzing three critical field studies in information systems. We hope that this paper will further reflection and debate on the important subject of grounding critical research methodology.",[],"Klein, Heinz K.",N/A,"School of Management, State University of New York at Binghamton, Binghamton, NY 13902-6000 U.S.A."
https://misq.umn.edu/misq/article/35/1/37/1434/Action-Design-Research1,MIS Quarterly,Action Design Research1,"Volume 35, Issue 1",March 2011,"Design research (DR) positions information technology artifacts at the core of the Information Systems discipline. However, dominant DR thinking takes a technological view of the IT artifact, paying scant attention to its shaping by the organizational context. Consequently, existing DR methods focus on building the artifact and relegate evaluation to a subsequent and separate phase. They value technological rigor at the cost of organizational relevance, and fail to recognize that the artifact emerges from interaction with the organizational context even when its initial design is guided by the researchers’ intent.We propose action design research (ADR) as a new DR method to address this problem. ADR reflects the premise that IT artifacts are ensembles shaped by the organizational context during development and use. The method conceptualizes the research process as containing the inseparable and inherently interwoven activities of building the IT artifact, intervening in the organization, and evaluating it concurrently. The essay describes the stages of ADR and associated principles that encapsulate its underlying beliefs and values. We illustrate ADR through a case of competence management at Volvo IT.",[],"Sein, Maung K.",N/A,"University of Agder, Post Box 422, 4604 Kristiansand S, Norway"
https://misq.umn.edu/misq/article/35/1/37/1434/Action-Design-Research1,MIS Quarterly,Action Design Research1,"Volume 35, Issue 1",March 2011,"Design research (DR) positions information technology artifacts at the core of the Information Systems discipline. However, dominant DR thinking takes a technological view of the IT artifact, paying scant attention to its shaping by the organizational context. Consequently, existing DR methods focus on building the artifact and relegate evaluation to a subsequent and separate phase. They value technological rigor at the cost of organizational relevance, and fail to recognize that the artifact emerges from interaction with the organizational context even when its initial design is guided by the researchers’ intent.We propose action design research (ADR) as a new DR method to address this problem. ADR reflects the premise that IT artifacts are ensembles shaped by the organizational context during development and use. The method conceptualizes the research process as containing the inseparable and inherently interwoven activities of building the IT artifact, intervening in the organization, and evaluating it concurrently. The essay describes the stages of ADR and associated principles that encapsulate its underlying beliefs and values. We illustrate ADR through a case of competence management at Volvo IT.",[],"Henfridsson, Ola",N/A,"Viktoria Institute, Hörselgången 4, 417 45 Göteborg, Sweden;University of Oslo, Boks 1072 Blindern, 0316 Oslo, Norway"
https://misq.umn.edu/misq/article/35/1/37/1434/Action-Design-Research1,MIS Quarterly,Action Design Research1,"Volume 35, Issue 1",March 2011,"Design research (DR) positions information technology artifacts at the core of the Information Systems discipline. However, dominant DR thinking takes a technological view of the IT artifact, paying scant attention to its shaping by the organizational context. Consequently, existing DR methods focus on building the artifact and relegate evaluation to a subsequent and separate phase. They value technological rigor at the cost of organizational relevance, and fail to recognize that the artifact emerges from interaction with the organizational context even when its initial design is guided by the researchers’ intent.We propose action design research (ADR) as a new DR method to address this problem. ADR reflects the premise that IT artifacts are ensembles shaped by the organizational context during development and use. The method conceptualizes the research process as containing the inseparable and inherently interwoven activities of building the IT artifact, intervening in the organization, and evaluating it concurrently. The essay describes the stages of ADR and associated principles that encapsulate its underlying beliefs and values. We illustrate ADR through a case of competence management at Volvo IT.",[],"Purao, Sandeep",N/A,"Penn State University, University Park, PA 16802, U.S.A."
https://misq.umn.edu/misq/article/35/1/37/1434/Action-Design-Research1,MIS Quarterly,Action Design Research1,"Volume 35, Issue 1",March 2011,"Design research (DR) positions information technology artifacts at the core of the Information Systems discipline. However, dominant DR thinking takes a technological view of the IT artifact, paying scant attention to its shaping by the organizational context. Consequently, existing DR methods focus on building the artifact and relegate evaluation to a subsequent and separate phase. They value technological rigor at the cost of organizational relevance, and fail to recognize that the artifact emerges from interaction with the organizational context even when its initial design is guided by the researchers’ intent.We propose action design research (ADR) as a new DR method to address this problem. ADR reflects the premise that IT artifacts are ensembles shaped by the organizational context during development and use. The method conceptualizes the research process as containing the inseparable and inherently interwoven activities of building the IT artifact, intervening in the organization, and evaluating it concurrently. The essay describes the stages of ADR and associated principles that encapsulate its underlying beliefs and values. We illustrate ADR through a case of competence management at Volvo IT.",[],"Rossi, Matti",N/A,"Aalto University School of Economics, P.O. Box 21220, 00076 Aalto, Finland"
https://misq.umn.edu/misq/article/35/1/37/1434/Action-Design-Research1,MIS Quarterly,Action Design Research1,"Volume 35, Issue 1",March 2011,"Design research (DR) positions information technology artifacts at the core of the Information Systems discipline. However, dominant DR thinking takes a technological view of the IT artifact, paying scant attention to its shaping by the organizational context. Consequently, existing DR methods focus on building the artifact and relegate evaluation to a subsequent and separate phase. They value technological rigor at the cost of organizational relevance, and fail to recognize that the artifact emerges from interaction with the organizational context even when its initial design is guided by the researchers’ intent.We propose action design research (ADR) as a new DR method to address this problem. ADR reflects the premise that IT artifacts are ensembles shaped by the organizational context during development and use. The method conceptualizes the research process as containing the inseparable and inherently interwoven activities of building the IT artifact, intervening in the organization, and evaluating it concurrently. The essay describes the stages of ADR and associated principles that encapsulate its underlying beliefs and values. We illustrate ADR through a case of competence management at Volvo IT.",[],"Lindgren, Rikard",N/A,"IT University of Gothenburg, 412 96 Göteborg, Sweden; University of Borås, 501 90 Borås, Sweden"
https://misq.umn.edu/misq/article/35/1/57/1440/Do-Ontological-Deficiencies-in-Modeling-Grammars,MIS Quarterly,Do Ontological Deficiencies in Modeling Grammars Matter?1,"Volume 35, Issue 1",March 2011,"Conceptual modeling grammars are a fundamental means for specifying information systems requirements. However, the actual usage of these grammars is only poorly understood. In particular, little is known about how properties of these grammars inform usage beliefs such as usefulness and ease of use. In this paper, we use an ontological theory to describe conceptual modeling grammars in terms of their ontological deficiencies, and formulate two propositions in regard to how these ontological deficiencies influence primary usage beliefs. Using BPMN as an example modeling grammar, we surveyed 528 modeling practitioners to test the theorized relationships. Our results show that users of conceptual modeling grammars perceive ontological deficiencies to exist, and that these deficiency perceptions are negatively associated with usefulness and ease of use of these grammars. With our research, we provide empirical evidence in support of the predictions of the ontological theory of modeling grammar expressiveness, and we identify previously unexplored links between conceptual modeling grammars and grammar usage beliefs. This work implies for practice a much closer coupling of the act of (re-)designing modeling grammars with usage-related success metrics.",[],"Recker, Jan",j.recker@qut.edu.au,"Faculty of Science and Technology, Queensland University of Technology, 126 Margaret Street, Brisbane QLD 4000 Australia ,"
https://misq.umn.edu/misq/article/35/1/57/1440/Do-Ontological-Deficiencies-in-Modeling-Grammars,MIS Quarterly,Do Ontological Deficiencies in Modeling Grammars Matter?1,"Volume 35, Issue 1",March 2011,"Conceptual modeling grammars are a fundamental means for specifying information systems requirements. However, the actual usage of these grammars is only poorly understood. In particular, little is known about how properties of these grammars inform usage beliefs such as usefulness and ease of use. In this paper, we use an ontological theory to describe conceptual modeling grammars in terms of their ontological deficiencies, and formulate two propositions in regard to how these ontological deficiencies influence primary usage beliefs. Using BPMN as an example modeling grammar, we surveyed 528 modeling practitioners to test the theorized relationships. Our results show that users of conceptual modeling grammars perceive ontological deficiencies to exist, and that these deficiency perceptions are negatively associated with usefulness and ease of use of these grammars. With our research, we provide empirical evidence in support of the predictions of the ontological theory of modeling grammar expressiveness, and we identify previously unexplored links between conceptual modeling grammars and grammar usage beliefs. This work implies for practice a much closer coupling of the act of (re-)designing modeling grammars with usage-related success metrics.",[],"Rosemann, Michael",j.recker@qut.edu.au,"Faculty of Science and Technology, Queensland University of Technology, 126 Margaret Street, Brisbane QLD 4000 Australia ,"
https://misq.umn.edu/misq/article/35/1/57/1440/Do-Ontological-Deficiencies-in-Modeling-Grammars,MIS Quarterly,Do Ontological Deficiencies in Modeling Grammars Matter?1,"Volume 35, Issue 1",March 2011,"Conceptual modeling grammars are a fundamental means for specifying information systems requirements. However, the actual usage of these grammars is only poorly understood. In particular, little is known about how properties of these grammars inform usage beliefs such as usefulness and ease of use. In this paper, we use an ontological theory to describe conceptual modeling grammars in terms of their ontological deficiencies, and formulate two propositions in regard to how these ontological deficiencies influence primary usage beliefs. Using BPMN as an example modeling grammar, we surveyed 528 modeling practitioners to test the theorized relationships. Our results show that users of conceptual modeling grammars perceive ontological deficiencies to exist, and that these deficiency perceptions are negatively associated with usefulness and ease of use of these grammars. With our research, we provide empirical evidence in support of the predictions of the ontological theory of modeling grammar expressiveness, and we identify previously unexplored links between conceptual modeling grammars and grammar usage beliefs. This work implies for practice a much closer coupling of the act of (re-)designing modeling grammars with usage-related success metrics.",[],"Green, Peter",p.green@business.uq.edu.au,"UQ Business School, The University of Queensland, 11 Salisbury Road, Ipswich QLD 4305 Australia ,"
https://misq.umn.edu/misq/article/35/1/57/1440/Do-Ontological-Deficiencies-in-Modeling-Grammars,MIS Quarterly,Do Ontological Deficiencies in Modeling Grammars Matter?1,"Volume 35, Issue 1",March 2011,"Conceptual modeling grammars are a fundamental means for specifying information systems requirements. However, the actual usage of these grammars is only poorly understood. In particular, little is known about how properties of these grammars inform usage beliefs such as usefulness and ease of use. In this paper, we use an ontological theory to describe conceptual modeling grammars in terms of their ontological deficiencies, and formulate two propositions in regard to how these ontological deficiencies influence primary usage beliefs. Using BPMN as an example modeling grammar, we surveyed 528 modeling practitioners to test the theorized relationships. Our results show that users of conceptual modeling grammars perceive ontological deficiencies to exist, and that these deficiency perceptions are negatively associated with usefulness and ease of use of these grammars. With our research, we provide empirical evidence in support of the predictions of the ontological theory of modeling grammar expressiveness, and we identify previously unexplored links between conceptual modeling grammars and grammar usage beliefs. This work implies for practice a much closer coupling of the act of (re-)designing modeling grammars with usage-related success metrics.",[],"Indulska, Marta",p.green@business.uq.edu.au,"UQ Business School, The University of Queensland, 11 Salisbury Road, Ipswich QLD 4305 Australia ,"
https://misq.umn.edu/misq/article/35/1/81/1442/Price-Discrimination-in-E-Commerce-An-Examination,MIS Quarterly,Price Discrimination in E-Commerce? An Examination of Dynamic Pricing in Name-Your-Own Price Markets1,"Volume 35, Issue 1",March 2011,"The enhanced abilities of online retailers to learn about their customers’ shopping behaviors have increased fears of dynamic pricing, a practice in which a seller sets prices based on the estimated buyer’s willingness-to-pay. However, among online retailers, a deviation from a one-price-for-all policy is the exception. When price discrimination is observed, it is often in the context of customer outrage about unfair pricing.One setting where pricing varies is the name-your-own-price (NYOP) mechanism. In contrast to a typical retail setting, in NYOP markets, it is the buyer who places an initial offer. This offer is accepted if it is above some threshold price set by the seller. If the initial offer is rejected, the buyer can update her offer in subsequent rounds. By design, the final purchase price is opaque to the public; the price paid depends on the individual buyer’s willingness-to-pay and offer strategy. Further, most forms of NYOP employ a fixed threshold price policy.In this paper, we compare a fixed threshold price setting with an adaptive threshold price setting. A seller who considers an adaptive threshold price has to weigh potentially greater profits against customer objections about the perceived fairness of such a policy. We first derive the optimal strategy for the seller. We analyze the effectiveness of an adaptive threshold price vis-à-vis a fixed threshold price on seller profit and customer satisfaction. Further, we evaluate the moderating effect of revealing the threshold price policy (adaptive versus fixed) to buyers. We test our model in a series of laboratory experiments and in a large field experiment at a prominent NYOP seller involving real purchases. Our results show that revealing the usage of an adaptive mechanism yields higher profits and more transactions than not revealing this information. In the field experiment, we find that applying a revealed adaptive threshold price can increase profits by over 20 percent without lowering customer satisfaction.",[],"Hinz, Oliver",N/A,"Faculty of Economics and Business Administration, Goethe-University of Frankfurt, Grüneburgplatz 1, 60323 Frankfurt am Main Germany"
https://misq.umn.edu/misq/article/35/1/81/1442/Price-Discrimination-in-E-Commerce-An-Examination,MIS Quarterly,Price Discrimination in E-Commerce? An Examination of Dynamic Pricing in Name-Your-Own Price Markets1,"Volume 35, Issue 1",March 2011,"The enhanced abilities of online retailers to learn about their customers’ shopping behaviors have increased fears of dynamic pricing, a practice in which a seller sets prices based on the estimated buyer’s willingness-to-pay. However, among online retailers, a deviation from a one-price-for-all policy is the exception. When price discrimination is observed, it is often in the context of customer outrage about unfair pricing.One setting where pricing varies is the name-your-own-price (NYOP) mechanism. In contrast to a typical retail setting, in NYOP markets, it is the buyer who places an initial offer. This offer is accepted if it is above some threshold price set by the seller. If the initial offer is rejected, the buyer can update her offer in subsequent rounds. By design, the final purchase price is opaque to the public; the price paid depends on the individual buyer’s willingness-to-pay and offer strategy. Further, most forms of NYOP employ a fixed threshold price policy.In this paper, we compare a fixed threshold price setting with an adaptive threshold price setting. A seller who considers an adaptive threshold price has to weigh potentially greater profits against customer objections about the perceived fairness of such a policy. We first derive the optimal strategy for the seller. We analyze the effectiveness of an adaptive threshold price vis-à-vis a fixed threshold price on seller profit and customer satisfaction. Further, we evaluate the moderating effect of revealing the threshold price policy (adaptive versus fixed) to buyers. We test our model in a series of laboratory experiments and in a large field experiment at a prominent NYOP seller involving real purchases. Our results show that revealing the usage of an adaptive mechanism yields higher profits and more transactions than not revealing this information. In the field experiment, we find that applying a revealed adaptive threshold price can increase profits by over 20 percent without lowering customer satisfaction.",[],"Hann, Il-Horn",N/A,"Robert H. Smith School of Business, University of Maryland, 4332R Van Munching Hall, College Park, MD 20742 U.S.A."
https://misq.umn.edu/misq/article/35/1/81/1442/Price-Discrimination-in-E-Commerce-An-Examination,MIS Quarterly,Price Discrimination in E-Commerce? An Examination of Dynamic Pricing in Name-Your-Own Price Markets1,"Volume 35, Issue 1",March 2011,"The enhanced abilities of online retailers to learn about their customers’ shopping behaviors have increased fears of dynamic pricing, a practice in which a seller sets prices based on the estimated buyer’s willingness-to-pay. However, among online retailers, a deviation from a one-price-for-all policy is the exception. When price discrimination is observed, it is often in the context of customer outrage about unfair pricing.One setting where pricing varies is the name-your-own-price (NYOP) mechanism. In contrast to a typical retail setting, in NYOP markets, it is the buyer who places an initial offer. This offer is accepted if it is above some threshold price set by the seller. If the initial offer is rejected, the buyer can update her offer in subsequent rounds. By design, the final purchase price is opaque to the public; the price paid depends on the individual buyer’s willingness-to-pay and offer strategy. Further, most forms of NYOP employ a fixed threshold price policy.In this paper, we compare a fixed threshold price setting with an adaptive threshold price setting. A seller who considers an adaptive threshold price has to weigh potentially greater profits against customer objections about the perceived fairness of such a policy. We first derive the optimal strategy for the seller. We analyze the effectiveness of an adaptive threshold price vis-à-vis a fixed threshold price on seller profit and customer satisfaction. Further, we evaluate the moderating effect of revealing the threshold price policy (adaptive versus fixed) to buyers. We test our model in a series of laboratory experiments and in a large field experiment at a prominent NYOP seller involving real purchases. Our results show that revealing the usage of an adaptive mechanism yields higher profits and more transactions than not revealing this information. In the field experiment, we find that applying a revealed adaptive threshold price can increase profits by over 20 percent without lowering customer satisfaction.",[],"Spann, Martin",N/A,"Munich School of Management, Ludwig-Maximilians-University Munich, Geschwister-Scholl-Platz 1, 80539 Munich Germany"
https://misq.umn.edu/misq/article/35/1/99/1444/The-Effects-of-Tree-View-Based-Presentation,MIS Quarterly,The Effects of Tree-View Based Presentation Adaptation on Mobile Web Browsing1,"Volume 35, Issue 1",March 2011,"Accessing the Web from mobile handheld devices has become increasingly common. However, accomplishing that task remains challenging mainly due to the physical constraints of handheld devices and the static presentation of Web pages. Adapting the presentation of Web pages is, therefore, critical to enabling effective mobile Web browsing and information searching. Based on cognitive fit theory and information foraging theory, we propose a novel hybrid approach to adapting Web page presentation that integrates three types of adaptation techniques, namely tree-view, hierarchical text summarization, and colored keyword highlighting. By following the design science research framework, we implemented the proposed approach on handheld devices and empirically evaluated the effects of presentation adaptation on mobile Web browsing. The results show that presentation adaptation significantly improves user performance and perception of mobile Web browsing. We also discover that the positive impact of presentation adaptation is moderated by the complexity of an information search task. The findings have significant theoretical and practical implications for the design and implementation of mobile Web applications.",[],"Adipat, Boonlit",N/A,"Graduate School of Management and Innovation, King Mongkut’s University of Technology Thonburi, 126 Pracha-Utit Road, Bangmod, Thung-Khru, Bangkok, Thailand 10140"
https://misq.umn.edu/misq/article/35/1/99/1444/The-Effects-of-Tree-View-Based-Presentation,MIS Quarterly,The Effects of Tree-View Based Presentation Adaptation on Mobile Web Browsing1,"Volume 35, Issue 1",March 2011,"Accessing the Web from mobile handheld devices has become increasingly common. However, accomplishing that task remains challenging mainly due to the physical constraints of handheld devices and the static presentation of Web pages. Adapting the presentation of Web pages is, therefore, critical to enabling effective mobile Web browsing and information searching. Based on cognitive fit theory and information foraging theory, we propose a novel hybrid approach to adapting Web page presentation that integrates three types of adaptation techniques, namely tree-view, hierarchical text summarization, and colored keyword highlighting. By following the design science research framework, we implemented the proposed approach on handheld devices and empirically evaluated the effects of presentation adaptation on mobile Web browsing. The results show that presentation adaptation significantly improves user performance and perception of mobile Web browsing. We also discover that the positive impact of presentation adaptation is moderated by the complexity of an information search task. The findings have significant theoretical and practical implications for the design and implementation of mobile Web applications.",[],"Zhang, Dongsong",N/A,"Department of Information Systems, University of Maryland, Baltimore County, 1000 Hilltop Circle, Baltimore, MD 21250 U.S.A."
https://misq.umn.edu/misq/article/35/1/99/1444/The-Effects-of-Tree-View-Based-Presentation,MIS Quarterly,The Effects of Tree-View Based Presentation Adaptation on Mobile Web Browsing1,"Volume 35, Issue 1",March 2011,"Accessing the Web from mobile handheld devices has become increasingly common. However, accomplishing that task remains challenging mainly due to the physical constraints of handheld devices and the static presentation of Web pages. Adapting the presentation of Web pages is, therefore, critical to enabling effective mobile Web browsing and information searching. Based on cognitive fit theory and information foraging theory, we propose a novel hybrid approach to adapting Web page presentation that integrates three types of adaptation techniques, namely tree-view, hierarchical text summarization, and colored keyword highlighting. By following the design science research framework, we implemented the proposed approach on handheld devices and empirically evaluated the effects of presentation adaptation on mobile Web browsing. The results show that presentation adaptation significantly improves user performance and perception of mobile Web browsing. We also discover that the positive impact of presentation adaptation is moderated by the complexity of an information search task. The findings have significant theoretical and practical implications for the design and implementation of mobile Web applications.",[],"Zhou, Lina",N/A,"Department of Information Systems, University of Maryland, Baltimore County, 1000 Hilltop Circle, Baltimore, MD 21250 U.S.A."
https://misq.umn.edu/misq/article/35/1/123/1438/Stuck-in-the-Conflicted-Middle-A-Role-Theoretic,MIS Quarterly,Stuck in the Conflicted Middle: A Role-Theoretic Perspective on B2B E-Marketplaces1,"Volume 35, Issue 1",March 2011,"Over the years, research on the implications of information technology on network governance structures has explored the “move to the market” and the “move to the middle” hypotheses. The middle is a space in which the logic and modalities of markets and hierarchies are intermingled. There is increasing evidence that most network relations reflect mixed-mode or hybrid logic. Despite the apparent advantages that make the middle so populous or “swollen” (Hennart 1993, p. 472), Kambil et al. (1999) highlight that it is riddled with uncertainty and high transaction costs. They label it “the conflicted middle” and propose that online marketplaces, specifically all-in-one markets, are capable of resolving this conflict. Unfortunately, however, Kambil et al. provide limited insight into both the nature of the conflict that plagues the middle and the ability of all-in-one markets to resolve it.To address these questions, this paper applies a role-theoretic perspective to the study of an e-marketplace that served the energy industry and evolved into an all-in-one market. Relying on an interpretive case study, this paper addresses the following research questions: (1) What is the nature of the conflict that characterizes the conflicted middle? (2) How do e-marketplaces, specifically all-in-one markets, help resolve this conflict? Our research highlights that brokers, trading partners, and agents who operate in the middle (where the contradictory logic of markets and hierarchies are mixed) experience goal, behavior, and identity conflict. All-in-one markets can help resolve these conflicts by supporting role integration at the group level and role segmentation at the individual level.",[],"Koch, Hope",N/A,"Hankamer School of Business, Baylor University, Waco, TX 76798 U.S.A."
https://misq.umn.edu/misq/article/35/1/123/1438/Stuck-in-the-Conflicted-Middle-A-Role-Theoretic,MIS Quarterly,Stuck in the Conflicted Middle: A Role-Theoretic Perspective on B2B E-Marketplaces1,"Volume 35, Issue 1",March 2011,"Over the years, research on the implications of information technology on network governance structures has explored the “move to the market” and the “move to the middle” hypotheses. The middle is a space in which the logic and modalities of markets and hierarchies are intermingled. There is increasing evidence that most network relations reflect mixed-mode or hybrid logic. Despite the apparent advantages that make the middle so populous or “swollen” (Hennart 1993, p. 472), Kambil et al. (1999) highlight that it is riddled with uncertainty and high transaction costs. They label it “the conflicted middle” and propose that online marketplaces, specifically all-in-one markets, are capable of resolving this conflict. Unfortunately, however, Kambil et al. provide limited insight into both the nature of the conflict that plagues the middle and the ability of all-in-one markets to resolve it.To address these questions, this paper applies a role-theoretic perspective to the study of an e-marketplace that served the energy industry and evolved into an all-in-one market. Relying on an interpretive case study, this paper addresses the following research questions: (1) What is the nature of the conflict that characterizes the conflicted middle? (2) How do e-marketplaces, specifically all-in-one markets, help resolve this conflict? Our research highlights that brokers, trading partners, and agents who operate in the middle (where the contradictory logic of markets and hierarchies are mixed) experience goal, behavior, and identity conflict. All-in-one markets can help resolve these conflicts by supporting role integration at the group level and role segmentation at the individual level.",[],"Schultze, Ulrike",N/A,"Southern Methodist University, PO Box 750333, Dallas, TX 75275-0333 U.S.A."
https://misq.umn.edu/misq/article/35/1/147/1430/When-Flexible-Routines-Meet-Flexible-Technologies,MIS Quarterly,"When Flexible Routines Meet Flexible Technologies: Affordance, Constraint, and the Imbrication of Human and Material Agencies1","Volume 35, Issue 1",March 2011,"Employees in many contemporary organizations work with flexible routines and flexible technologies. When those employees find that they are unable to achieve their goals in the current environment, how do they decide whether they should change the composition of their routines or the materiality of the technologies with which they work? The perspective advanced in this paper suggests that the answer to this question depends on how human and material agencies—the basic building blocks common to both routines and technologies—are imbricated. Imbrication of human and material agencies creates infrastructure in the form of routines and technologies that people use to carry out their work. Routine or technological infrastructure used at any given moment is the result of previous imbrications of human and material agencies. People draw on this infrastructure to construct a perception that a technology either constrains their ability to achieve their goals, or that the technology affords the possibility of achieving new goals. The case of a computer simulation technology for automotive design used to illustrate this framework suggests that perceptions of constraint lead people to change their technologies while perceptions of affordance lead people to change their routines. This imbrication metaphor is used to suggest how a human agency approach to technology can usefully incorporate notions of material agency into its explanations of organizational change.",[],"Leonardi, Paul M.",N/A,"Department of Communication Studies and Department of Industrial Engineering and Management Sciences, Northwestern University, 2240 Campus Drive, Evanston, IL 60208 U.S.A."
https://misq.umn.edu/misq/article/35/1/169/1432/Product-Related-Deception-in-E-Commerce-A,MIS Quarterly,Product-Related Deception in E-Commerce: A Theoretical Perspective1,"Volume 35, Issue 1",March 2011,"With the advent of e-commerce, the potential of new Internet technologies to mislead or deceive consumers has increased considerably. This paper extends prior classifications of deception and presents a typology of product-related deceptive information practices that illustrates the various ways in which online merchants can deceive consumers via e-commerce product websites. The typology can be readily used as educational material to promote consumer awareness of deception in e-commerce and as input to establish benchmarks for good business practices for online companies. In addition, the paper develops an integrative model and a set of theory-based propositions addressing why consumers are deceived by the various types of deceptive information practices and what factors contribute to consumer success (or failure) in detecting such deceptions. The model not only enhances our conceptual understanding of the phenomenon of product-based deception and its outcomes in e-commerce but also serves as a foundation for further theoretical and empirical investigations. Moreover, a better understanding of the factors contributing to or inhibiting deception detection can also help government agencies and consumer organizations design more effective solutions to fight online deception.",[],"Xiao, Bo",N/A,"Department of Computer Science (Computing and Information Systems), Faculty of Science, Hong Kong Baptist University, Kowloon, Hong Kong, China"
https://misq.umn.edu/misq/article/35/1/169/1432/Product-Related-Deception-in-E-Commerce-A,MIS Quarterly,Product-Related Deception in E-Commerce: A Theoretical Perspective1,"Volume 35, Issue 1",March 2011,"With the advent of e-commerce, the potential of new Internet technologies to mislead or deceive consumers has increased considerably. This paper extends prior classifications of deception and presents a typology of product-related deceptive information practices that illustrates the various ways in which online merchants can deceive consumers via e-commerce product websites. The typology can be readily used as educational material to promote consumer awareness of deception in e-commerce and as input to establish benchmarks for good business practices for online companies. In addition, the paper develops an integrative model and a set of theory-based propositions addressing why consumers are deceived by the various types of deceptive information practices and what factors contribute to consumer success (or failure) in detecting such deceptions. The model not only enhances our conceptual understanding of the phenomenon of product-based deception and its outcomes in e-commerce but also serves as a foundation for further theoretical and empirical investigations. Moreover, a better understanding of the factors contributing to or inhibiting deception detection can also help government agencies and consumer organizations design more effective solutions to fight online deception.",[],"Benbasat, Izak",N/A,"Sauder School of Business, University of British Columbia, 2053 Main Mall, Vancouver, BC V6T 1Z2, Canada"
https://misq.umn.edu/misq/article/35/1/197/1450/Transdisciplinary-Perspectives-on-Environmental,MIS Quarterly,Transdisciplinary Perspectives on Environmental Sustainability: A Resource Base and Framework for IT-Enabled Business Transformation1,"Volume 35, Issue 1",March 2011,"The quality and future of human existence are directly related to the condition of our natural environment, but we are damaging the environment. Scientific evidence has mounted a compelling case that human behavior is responsible for deterioration in the Earth’s natural environment, with the rate of deterioration predicted to increase in the future. Acknowledging this evidence, the governments of 192 countries have formally agreed to take action to resolve problems with the climate system, one of the most highly stressed parts of the natural environment. While the intention is clear, the question of how best to proceed is not.The research reported here undertook a three-phase approach of selecting, analyzing, and synthesizing relevant literature to develop a holistic, transdisciplinary, integrative framework for IT-enabled business transformation. The focus on business transformation is because business is recognized as being a critical contributor in realizing the challenges of environmental sustainability due to its potential capacity for innovation and change—locally, nationally, and globally. This article also serves as a resource base for researchers to begin to undertake significant information systems and multidisciplinary work toward the goal of environmental sustainability. Through selection and analysis of illustrative examples of current work from 12 academic disciplines across 6 core categories, the framework addresses the key issues of uncertainty: What is meant by environmental sustainability?What are its major challenges?What is being done about these challenges?What needs to be done?",[],"Elliot, Steve",N/A,"Discipline of Business Information Systems, The University of Sydney Business School, Sydney, New South Wales 2006, Australia"
https://misq.umn.edu/misq/article/35/1/237/1446/How-Information-Management-Capability-Influences,MIS Quarterly,How Information Management Capability Influences Firm Performance1,"Volume 35, Issue 1",March 2011,"How do information technology capabilities contribute to firm performance? This study develops a conceptual model linking IT-enabled information management capability with three important organizational capabilities (customer management capability, process management capability, and performance management capability). We argue that these three capabilities mediate the relationship between information management capability and firm performance. We use a rare archival data set from a conglomerate business group that had adopted a model of performance excellence for organizational transformation based on the Baldrige criteria. This data set contains actual scores from high quality assessments of firms and intraorganizational units of the conglomerate, and hence provides unobtrusive measures of the key constructs to validate our conceptual model.We find that information management capability plays an important role in developing other firm capabilities for customer management, process management, and performance management. In turn, these capabilities favorably influence customer, financial, human resources, and organizational effectiveness measures of firm performance. Among key managerial implications, senior leaders must focus on creating necessary conditions for developing IT infrastructure and information management capability because they play a foundational role in building other capabilities for improved firm performance. The Baldrige model also needs some changes to more explicitly acknowledge the role and importance of information management capability so that senior leaders know where to begin in their journey toward business excellence.",[],"Mithas, Sunil",N/A,"Robert H. Smith School of Business, University of Maryland, 4357 Van Munching Hall, College Park, MD 20742 U.S.A."
https://misq.umn.edu/misq/article/35/1/237/1446/How-Information-Management-Capability-Influences,MIS Quarterly,How Information Management Capability Influences Firm Performance1,"Volume 35, Issue 1",March 2011,"How do information technology capabilities contribute to firm performance? This study develops a conceptual model linking IT-enabled information management capability with three important organizational capabilities (customer management capability, process management capability, and performance management capability). We argue that these three capabilities mediate the relationship between information management capability and firm performance. We use a rare archival data set from a conglomerate business group that had adopted a model of performance excellence for organizational transformation based on the Baldrige criteria. This data set contains actual scores from high quality assessments of firms and intraorganizational units of the conglomerate, and hence provides unobtrusive measures of the key constructs to validate our conceptual model.We find that information management capability plays an important role in developing other firm capabilities for customer management, process management, and performance management. In turn, these capabilities favorably influence customer, financial, human resources, and organizational effectiveness measures of firm performance. Among key managerial implications, senior leaders must focus on creating necessary conditions for developing IT infrastructure and information management capability because they play a foundational role in building other capabilities for improved firm performance. The Baldrige model also needs some changes to more explicitly acknowledge the role and importance of information management capability so that senior leaders know where to begin in their journey toward business excellence.",[],"Ramasubbu, Narayan",N/A,"School of Information Systems, Singapore Management University, 80 Stamford Road, Singapore 178902 Singapore"
https://misq.umn.edu/misq/article/35/1/237/1446/How-Information-Management-Capability-Influences,MIS Quarterly,How Information Management Capability Influences Firm Performance1,"Volume 35, Issue 1",March 2011,"How do information technology capabilities contribute to firm performance? This study develops a conceptual model linking IT-enabled information management capability with three important organizational capabilities (customer management capability, process management capability, and performance management capability). We argue that these three capabilities mediate the relationship between information management capability and firm performance. We use a rare archival data set from a conglomerate business group that had adopted a model of performance excellence for organizational transformation based on the Baldrige criteria. This data set contains actual scores from high quality assessments of firms and intraorganizational units of the conglomerate, and hence provides unobtrusive measures of the key constructs to validate our conceptual model.We find that information management capability plays an important role in developing other firm capabilities for customer management, process management, and performance management. In turn, these capabilities favorably influence customer, financial, human resources, and organizational effectiveness measures of firm performance. Among key managerial implications, senior leaders must focus on creating necessary conditions for developing IT infrastructure and information management capability because they play a foundational role in building other capabilities for improved firm performance. The Baldrige model also needs some changes to more explicitly acknowledge the role and importance of information management capability so that senior leaders know where to begin in their journey toward business excellence.",[],"Sambamurthy, V.",N/A,"Eli Broad College of Business, Michigan State University. East Lansing, MI 48824 U.S.A."
https://misq.umn.edu/misq/article/35/1/iii/1431/Editor-s-CommentsRigor-and-Relevance-in-IS,MIS Quarterly,Editor’s CommentsRigor and Relevance in IS Research: Redefining the Debate and a Call for Future Research1,"Volume 35, Issue 1",March 2011,N/A,[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/35/1/iii/1431/Editor-s-CommentsRigor-and-Relevance-in-IS,MIS Quarterly,Editor’s CommentsRigor and Relevance in IS Research: Redefining the Debate and a Call for Future Research1,"Volume 35, Issue 1",March 2011,N/A,[],"Ang, Soon",N/A,Nanyang Technological University
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/35/2/261/1472/Measurement-and-Meaning-in-Information-Systems-and,MIS Quarterly,Measurement and Meaning in Information Systems and Organizational Research: Methodological and Philosophical Foundations1,"Volume 35, Issue 2",June 2011,"Despite renewed interest and many advances in methodology in recent years, information systems and organizational researchers face confusing and inconsistent guidance on how to choose amongst, implement, and interpret findings from the use of different measurement procedures. In this article, the related topics of measurement and construct validity are summarized and discussed, with particular focus on formative and reflective indicators and common method bias, and, where relevant, a number of allied issues are considered. The perspective taken is an eclectic and holistic one and attempts to address conceptual and philosophical essentials, raise salient questions, and pose plausible solutions to critical measurement dilemmas occurring in the managerial, behavioral, and social sciences.",[],"Bagozzi, Richard P.",N/A,"Ross School of Business, University of Michigan, 701 Tappan Street, Ann Arbor, MI 48109-1234 U.S.A."
https://misq.umn.edu/misq/article/35/2/293/1457/Construct-Measurement-and-Validation-Procedures-in,MIS Quarterly,Construct Measurement and Validation Procedures in MIS and Behavioral Research: Integrating New and Existing Techniques1,"Volume 35, Issue 2",June 2011,"Despite the fact that validating the measures of constructs is critical to building cumulative knowledge in MIS and the behavioral sciences, the process of scale development and validation continues to be a challenging activity. Undoubtedly, part of the problem is that many of the scale development procedures advocated in the literature are limited by the fact that they (1) fail to adequately discuss how to develop appropriate conceptual definitions of the focal construct, (2) often fail to properly specify the measurement model that relates the latent construct to its indicators, and (3) underutilize techniques that provide evidence that the set of items used to represent the focal construct actually measures what it purports to measure. Therefore, the purpose of the present paper is to integrate new and existing techniques into a comprehensive set of recommendations that can be used to give researchers in MIS and the behavioral sciences a framework for developing valid measures. First, we briefly elaborate upon some of the limitations of current scale development practices. Following this, we discuss each of the steps in the scale development process while paying particular attention to the differences that are required when one is attempting to develop scales for constructs with formative indicators as opposed to constructs with reflective indicators. Finally, we discuss several things that should be done after the initial development of a scale to examine its generalizability and to enhance its usefulness.",[],"MacKenzie, Scott B.",N/A,"Department of Marketing, Kelley School of Business, Indiana University, Bloomington, IN 47405 U.S.A."
https://misq.umn.edu/misq/article/35/2/293/1457/Construct-Measurement-and-Validation-Procedures-in,MIS Quarterly,Construct Measurement and Validation Procedures in MIS and Behavioral Research: Integrating New and Existing Techniques1,"Volume 35, Issue 2",June 2011,"Despite the fact that validating the measures of constructs is critical to building cumulative knowledge in MIS and the behavioral sciences, the process of scale development and validation continues to be a challenging activity. Undoubtedly, part of the problem is that many of the scale development procedures advocated in the literature are limited by the fact that they (1) fail to adequately discuss how to develop appropriate conceptual definitions of the focal construct, (2) often fail to properly specify the measurement model that relates the latent construct to its indicators, and (3) underutilize techniques that provide evidence that the set of items used to represent the focal construct actually measures what it purports to measure. Therefore, the purpose of the present paper is to integrate new and existing techniques into a comprehensive set of recommendations that can be used to give researchers in MIS and the behavioral sciences a framework for developing valid measures. First, we briefly elaborate upon some of the limitations of current scale development practices. Following this, we discuss each of the steps in the scale development process while paying particular attention to the differences that are required when one is attempting to develop scales for constructs with formative indicators as opposed to constructs with reflective indicators. Finally, we discuss several things that should be done after the initial development of a scale to examine its generalizability and to enhance its usefulness.",[],"Podsakoff, Philip M.",N/A,"Department of Management, Kelley School of Business, Indiana University, Bloomington, IN 47405 U.S.A."
https://misq.umn.edu/misq/article/35/2/293/1457/Construct-Measurement-and-Validation-Procedures-in,MIS Quarterly,Construct Measurement and Validation Procedures in MIS and Behavioral Research: Integrating New and Existing Techniques1,"Volume 35, Issue 2",June 2011,"Despite the fact that validating the measures of constructs is critical to building cumulative knowledge in MIS and the behavioral sciences, the process of scale development and validation continues to be a challenging activity. Undoubtedly, part of the problem is that many of the scale development procedures advocated in the literature are limited by the fact that they (1) fail to adequately discuss how to develop appropriate conceptual definitions of the focal construct, (2) often fail to properly specify the measurement model that relates the latent construct to its indicators, and (3) underutilize techniques that provide evidence that the set of items used to represent the focal construct actually measures what it purports to measure. Therefore, the purpose of the present paper is to integrate new and existing techniques into a comprehensive set of recommendations that can be used to give researchers in MIS and the behavioral sciences a framework for developing valid measures. First, we briefly elaborate upon some of the limitations of current scale development practices. Following this, we discuss each of the steps in the scale development process while paying particular attention to the differences that are required when one is attempting to develop scales for constructs with formative indicators as opposed to constructs with reflective indicators. Finally, we discuss several things that should be done after the initial development of a scale to examine its generalizability and to enhance its usefulness.",[],"Podsakoff, Nathan P.",N/A,"Department of Management and Organizations, Eller College of Management, University of Arizona, Tucson, AZ 85721 U.S.A."
https://misq.umn.edu/misq/article/35/2/335/1455/Incorporating-Formative-Measures-into-Covariance,MIS Quarterly,Incorporating Formative Measures into Covariance-Based Structural Equation Models1,"Volume 35, Issue 2",June 2011,"Formatively measured constructs have been increasingly used in information systems research. With few exceptions, however, extant studies have been relying on the partial least squares (PLS) approach to specify and estimate structural models involving constructs measured with formative indicators. This paper highlights the benefits of employing covariance structure analysis (CSA) when investigating such models and illustrates its application with the LISREL program. The aim is to provide practicing IS researchers with an understanding of key issues and potential problems associated with formatively measured constructs within a covariance-based modeling framework and encourage them to consider using CSA in their future research endeavors.",[],"Diamantopoulos, Adamantios",N/A,"International Marketing, University of Vienna, Bruenner Straße 72, Vienna, Austria"
https://misq.umn.edu/misq/article/35/2/359/1447/Evaluating-Effect-Composite-and-Causal-Indicators,MIS Quarterly,"Evaluating Effect, Composite, and Causal Indicators in Structural Equation Models1","Volume 35, Issue 2",June 2011,"Although the literature on alternatives to effect indicators is growing, there has been little attention given to evaluating causal and composite (formative) indicators. This paper provides an overview of this topic by contrasting ways of assessing the validity of effect and causal indicators in structural equation models (SEMs). It also draws a distinction between composite (formative) indicators and causal indicators and argues that validity is most relevant to the latter. Sound validity assessment of indicators is dependent on having an adequate overall model fit and on the relative stability of the parameter estimates for the latent variable and indicators as they appear in different models. If the overall fit and stability of estimates are adequate, then a researcher can assess validity using the unstandardized and standardized validity coefficients and the unique validity variance estimate. With multiple causal indicators or with effect indicators influenced by multiple latent variables, collinearity diagnostics are useful. These results are illustrated with a number of correctly and incorrectly specified hypothetical models.",[],"Bollen, Kenneth A.",N/A,"Carolina Population Center, Department of Sociology, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599-3210 U.S.A."
https://misq.umn.edu/misq/article/35/2/373/1449/What-Signal-Are-You-Sending-How-Website-Quality,MIS Quarterly,What Signal Are You Sending? How Website Quality Influences Perceptions of Product Quality and Purchase Intentions1,"Volume 35, Issue 2",June 2011,"An electronic commerce marketing channel is fully mediated by information technology, stripping away much of a product’s physical informational cues, and creating information asymmetries (i.e., limited information). These asymmetries may impede consumers’ ability to effectively assess certain types of products, thus creating challenges for online sellers. Signaling theory provides a framework for understanding how extrinsic cues— signals—can be used by sellers to convey product quality information to consumers, reducing uncertainty and facilitating a purchase or exchange. This research proposes a model to investigate website quality as a potential signal of product quality and consider the moderating effects of product information asymmetries and signal credibility. Three experiments are reported that examine the efficacy of signaling theory as a basis for predicting online consumer behavior with an experience good. The results indicate that website quality influences consumers’ perceptions of product quality, which subsequently affects online purchase intentions. Additionally, website quality was found to have a greater influence on perceived product quality when consumers had higher information asymmetries. Likewise, signal credibility was found to strengthen the relationship between website quality and product quality perceptions for a high quality website. Implications for future research and website design are examined.",[],"Wells, John D.",N/A,"Isenberg School of Management, University of Massachusetts Amherst, Amherst, MA 01003 U.S.A."
https://misq.umn.edu/misq/article/35/2/373/1449/What-Signal-Are-You-Sending-How-Website-Quality,MIS Quarterly,What Signal Are You Sending? How Website Quality Influences Perceptions of Product Quality and Purchase Intentions1,"Volume 35, Issue 2",June 2011,"An electronic commerce marketing channel is fully mediated by information technology, stripping away much of a product’s physical informational cues, and creating information asymmetries (i.e., limited information). These asymmetries may impede consumers’ ability to effectively assess certain types of products, thus creating challenges for online sellers. Signaling theory provides a framework for understanding how extrinsic cues— signals—can be used by sellers to convey product quality information to consumers, reducing uncertainty and facilitating a purchase or exchange. This research proposes a model to investigate website quality as a potential signal of product quality and consider the moderating effects of product information asymmetries and signal credibility. Three experiments are reported that examine the efficacy of signaling theory as a basis for predicting online consumer behavior with an experience good. The results indicate that website quality influences consumers’ perceptions of product quality, which subsequently affects online purchase intentions. Additionally, website quality was found to have a greater influence on perceived product quality when consumers had higher information asymmetries. Likewise, signal credibility was found to strengthen the relationship between website quality and product quality perceptions for a high quality website. Implications for future research and website design are examined.",[],"Valacich, Joseph S.",N/A,"Department of Entrepreneurship and Information Systems, College of Business, Washington State University, Pullman, WA 99164-4750 U.S.A."
https://misq.umn.edu/misq/article/35/2/373/1449/What-Signal-Are-You-Sending-How-Website-Quality,MIS Quarterly,What Signal Are You Sending? How Website Quality Influences Perceptions of Product Quality and Purchase Intentions1,"Volume 35, Issue 2",June 2011,"An electronic commerce marketing channel is fully mediated by information technology, stripping away much of a product’s physical informational cues, and creating information asymmetries (i.e., limited information). These asymmetries may impede consumers’ ability to effectively assess certain types of products, thus creating challenges for online sellers. Signaling theory provides a framework for understanding how extrinsic cues— signals—can be used by sellers to convey product quality information to consumers, reducing uncertainty and facilitating a purchase or exchange. This research proposes a model to investigate website quality as a potential signal of product quality and consider the moderating effects of product information asymmetries and signal credibility. Three experiments are reported that examine the efficacy of signaling theory as a basis for predicting online consumer behavior with an experience good. The results indicate that website quality influences consumers’ perceptions of product quality, which subsequently affects online purchase intentions. Additionally, website quality was found to have a greater influence on perceived product quality when consumers had higher information asymmetries. Likewise, signal credibility was found to strengthen the relationship between website quality and product quality perceptions for a high quality website. Implications for future research and website design are examined.",[],"Hess, Traci J.",N/A,"Isenberg School of Management, University of Massachusetts Amherst, Amherst, MA 01003 U.S.A."
https://misq.umn.edu/misq/article/35/2/397/1471/Correlated-Failures-Diversification-and,MIS Quarterly,"Correlated Failures, Diversification, and Information Security Risk Management1","Volume 35, Issue 2",June 2011,"The increasing dependence on information networks for business operations has focused managerial attention on managing risks posed by failure of these networks. In this paper, we develop models to assess the risk of failure on the availability of an information network due to attacks that exploit software vulnerabilities. Software vulnerabilities arise from software installed on the nodes of the network. When the same software stack is installed on multiple nodes on the network, software vulnerabilities are shared among them. These shared vulnerabilities can result in correlated failure of multiple nodes resulting in longer repair times and greater loss of availability of the network. Considering positive network effects (e.g., compatibility) alone without taking the risks of correlated failure and the resulting downtime into account would lead to overinvestment in homogeneous software deployment. Exploiting characteristics unique to information networks, we present a queuing model that allows us to quantify downtime loss faced by a rm as a function of (1) investment in security technologies to avert attacks, (2) software diversification to limit the risk of correlated failure under attacks, and (3) investment in IT resources to repair failures due to attacks. The novelty of this method is that we endogenize the failure distribution and the node correlation distribution, and show how the diversification strategy and other security measures/investments may impact these two distributions, which in turn determine the security loss faced by the firm. We analyze and discuss the effectiveness of diversification strategy under different operating conditions and in the presence of changing vulnerabilities. We also take into account the benefits and costs of a diversification strategy. Our analysis provides conditions under which diversification strategy is advantageous.",[],"Chen, Pei-yu",N/A,"Department of Management Information Systems, Fox School of Business and Management, Temple University, 1801 N. Broad Street, Philadelphia, PA 19122 U.S.A."
https://misq.umn.edu/misq/article/35/2/397/1471/Correlated-Failures-Diversification-and,MIS Quarterly,"Correlated Failures, Diversification, and Information Security Risk Management1","Volume 35, Issue 2",June 2011,"The increasing dependence on information networks for business operations has focused managerial attention on managing risks posed by failure of these networks. In this paper, we develop models to assess the risk of failure on the availability of an information network due to attacks that exploit software vulnerabilities. Software vulnerabilities arise from software installed on the nodes of the network. When the same software stack is installed on multiple nodes on the network, software vulnerabilities are shared among them. These shared vulnerabilities can result in correlated failure of multiple nodes resulting in longer repair times and greater loss of availability of the network. Considering positive network effects (e.g., compatibility) alone without taking the risks of correlated failure and the resulting downtime into account would lead to overinvestment in homogeneous software deployment. Exploiting characteristics unique to information networks, we present a queuing model that allows us to quantify downtime loss faced by a rm as a function of (1) investment in security technologies to avert attacks, (2) software diversification to limit the risk of correlated failure under attacks, and (3) investment in IT resources to repair failures due to attacks. The novelty of this method is that we endogenize the failure distribution and the node correlation distribution, and show how the diversification strategy and other security measures/investments may impact these two distributions, which in turn determine the security loss faced by the firm. We analyze and discuss the effectiveness of diversification strategy under different operating conditions and in the presence of changing vulnerabilities. We also take into account the benefits and costs of a diversification strategy. Our analysis provides conditions under which diversification strategy is advantageous.",[],"Kataria, Gaurav",N/A,"Booz & Co., 127 Public Squire, Suite 5300, Cleveland, OH 44114 U.S.A."
https://misq.umn.edu/misq/article/35/2/397/1471/Correlated-Failures-Diversification-and,MIS Quarterly,"Correlated Failures, Diversification, and Information Security Risk Management1","Volume 35, Issue 2",June 2011,"The increasing dependence on information networks for business operations has focused managerial attention on managing risks posed by failure of these networks. In this paper, we develop models to assess the risk of failure on the availability of an information network due to attacks that exploit software vulnerabilities. Software vulnerabilities arise from software installed on the nodes of the network. When the same software stack is installed on multiple nodes on the network, software vulnerabilities are shared among them. These shared vulnerabilities can result in correlated failure of multiple nodes resulting in longer repair times and greater loss of availability of the network. Considering positive network effects (e.g., compatibility) alone without taking the risks of correlated failure and the resulting downtime into account would lead to overinvestment in homogeneous software deployment. Exploiting characteristics unique to information networks, we present a queuing model that allows us to quantify downtime loss faced by a rm as a function of (1) investment in security technologies to avert attacks, (2) software diversification to limit the risk of correlated failure under attacks, and (3) investment in IT resources to repair failures due to attacks. The novelty of this method is that we endogenize the failure distribution and the node correlation distribution, and show how the diversification strategy and other security measures/investments may impact these two distributions, which in turn determine the security loss faced by the firm. We analyze and discuss the effectiveness of diversification strategy under different operating conditions and in the presence of changing vulnerabilities. We also take into account the benefits and costs of a diversification strategy. Our analysis provides conditions under which diversification strategy is advantageous.",[],"Krishnan, Ramayya",N/A,"School of Information Systems and Management, The Heinz College, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213 U.S.A."
https://misq.umn.edu/misq/article/35/2/423/1445/Managing-Consumer-Privacy-Concerns-in,MIS Quarterly,Managing Consumer Privacy Concerns in Personalization: A Strategic Analysis of Privacy Protection1,"Volume 35, Issue 2",June 2011,"Advances in information technology and e-commerce enable firms to make personalized offers to individual consumers based on information about the consumers. However, the collection and use of private information have caused serious concerns about privacy invasion by consumers, creating a personalization–privacy tradeoff. The key approach to address privacy concerns is via the protection of privacy through the implementation of fair information practices, a set of standards governing the collection and use of personal information. In this paper, we take a game-theoretic approach to explore the motivation of firms for privacy protection and its impact on competition and social welfare in the context of product and price personalization. We find that privacy protection can work as a competition-mitigating mechanism by generating asymmetry in the consumer segments to which firms offer personalization, enhancing the profit extraction abilities of the firms. In equilibrium, both symmetric and asymmetric choices of privacy protection by the firms can result, depending on the size of the personalization scope and the investment cost of protection. Further, as consumers become more concerned about their privacy, it is more likely that all firms adopt privacy protection. In the perspective of welfare, we show that autonomous choices of privacy protection by personalizing firms can improve social welfare at the expense of consumer welfare. We further find that regulation enforcing the implementation of fair information practices can be efficient from the social welfare perspective mainly by limiting the incentives of the firms to exploit the competition-mitigation effect.",[],"Lee, Dong-Joo",N/A,"Division of Management, Hansung University, 389 Samseon-dong 3-ga, Seongbuk-gu, Seoul 136-792 Korea"
https://misq.umn.edu/misq/article/35/2/423/1445/Managing-Consumer-Privacy-Concerns-in,MIS Quarterly,Managing Consumer Privacy Concerns in Personalization: A Strategic Analysis of Privacy Protection1,"Volume 35, Issue 2",June 2011,"Advances in information technology and e-commerce enable firms to make personalized offers to individual consumers based on information about the consumers. However, the collection and use of private information have caused serious concerns about privacy invasion by consumers, creating a personalization–privacy tradeoff. The key approach to address privacy concerns is via the protection of privacy through the implementation of fair information practices, a set of standards governing the collection and use of personal information. In this paper, we take a game-theoretic approach to explore the motivation of firms for privacy protection and its impact on competition and social welfare in the context of product and price personalization. We find that privacy protection can work as a competition-mitigating mechanism by generating asymmetry in the consumer segments to which firms offer personalization, enhancing the profit extraction abilities of the firms. In equilibrium, both symmetric and asymmetric choices of privacy protection by the firms can result, depending on the size of the personalization scope and the investment cost of protection. Further, as consumers become more concerned about their privacy, it is more likely that all firms adopt privacy protection. In the perspective of welfare, we show that autonomous choices of privacy protection by personalizing firms can improve social welfare at the expense of consumer welfare. We further find that regulation enforcing the implementation of fair information practices can be efficient from the social welfare perspective mainly by limiting the incentives of the firms to exploit the competition-mitigation effect.",[],"Ahn, Jae-Hyeon",N/A,"Business School, Korea Advanced Institute of Science and Technology, 207-43, Chongryangri-dong, Dongdaemoon-gu, Seoul 130-722 Korea"
https://misq.umn.edu/misq/article/35/2/423/1445/Managing-Consumer-Privacy-Concerns-in,MIS Quarterly,Managing Consumer Privacy Concerns in Personalization: A Strategic Analysis of Privacy Protection1,"Volume 35, Issue 2",June 2011,"Advances in information technology and e-commerce enable firms to make personalized offers to individual consumers based on information about the consumers. However, the collection and use of private information have caused serious concerns about privacy invasion by consumers, creating a personalization–privacy tradeoff. The key approach to address privacy concerns is via the protection of privacy through the implementation of fair information practices, a set of standards governing the collection and use of personal information. In this paper, we take a game-theoretic approach to explore the motivation of firms for privacy protection and its impact on competition and social welfare in the context of product and price personalization. We find that privacy protection can work as a competition-mitigating mechanism by generating asymmetry in the consumer segments to which firms offer personalization, enhancing the profit extraction abilities of the firms. In equilibrium, both symmetric and asymmetric choices of privacy protection by the firms can result, depending on the size of the personalization scope and the investment cost of protection. Further, as consumers become more concerned about their privacy, it is more likely that all firms adopt privacy protection. In the perspective of welfare, we show that autonomous choices of privacy protection by personalizing firms can improve social welfare at the expense of consumer welfare. We further find that regulation enforcing the implementation of fair information practices can be efficient from the social welfare perspective mainly by limiting the incentives of the firms to exploit the competition-mitigation effect.",[],"Bang, Youngsok",N/A,"Business School, Korea Advanced Institute of Science and Technology, 207-43, Chongryangri-dong, Dongdaemoon-gu, Seoul 130-722 Korea"
https://misq.umn.edu/misq/article/35/2/445/1448/The-More-the-Merrier-How-the-Number-of-Partners-in,MIS Quarterly,"The More, the Merrier? How the Number of Partners in a Standard-Setting Initiative Affects Shareholder’s Risk and Return1","Volume 35, Issue 2",June 2011,N/A,[],"Aggarwal, Nitin",N/A,"College of Business, San Jose State University, San Jose, CA 95192 U.S.A."
https://misq.umn.edu/misq/article/35/2/445/1448/The-More-the-Merrier-How-the-Number-of-Partners-in,MIS Quarterly,"The More, the Merrier? How the Number of Partners in a Standard-Setting Initiative Affects Shareholder’s Risk and Return1","Volume 35, Issue 2",June 2011,N/A,[],"Dai, Qizhi",N/A,"LeBow College of Business, Drexel University, Philadelphia, PA 19104 U.S.A."
https://misq.umn.edu/misq/article/35/2/445/1448/The-More-the-Merrier-How-the-Number-of-Partners-in,MIS Quarterly,"The More, the Merrier? How the Number of Partners in a Standard-Setting Initiative Affects Shareholder’s Risk and Return1","Volume 35, Issue 2",June 2011,N/A,[],"Walden, Eric A.",N/A,"Rawls College of Business Administration, Texas Tech University, Lubbock, TX 79409 U.S.A."
https://misq.umn.edu/misq/article/35/2/463/1451/Competing-Perspectives-on-the-Link-between,MIS Quarterly,Competing Perspectives on the Link between Strategic Information Technology Alignment and Organizational Agility: Insights from a Mediation Model1,"Volume 35, Issue 2",June 2011,"Strategic information technology alignment remains a top priority for business and IT executives. Yet with a recent rise in environmental volatility, firms are asking how to be more agile in identifying and responding to market-based threats and opportunities. Whether alignment helps or hurts agility is an unresolved issue. This paper presents a variety of arguments from the literature that alternately predict a positive or negative relationship between alignment and agility. This relationship is then tested using a model in which agility mediates the link between alignment and firm performance under varying conditions of IT infrastructure flexibility and environmental volatility. Using data from a matched survey of IT and business executives in 241 firms, we uncover a positive and significant link between alignment and agility and between agility and firm performance. We also show that the effect of alignment on performance is fully mediated by agility, that environmental volatility positively moderates the link between agility and firm performance, and that agility has a greater impact on firm performance in more volatile markets. While IT infrastructure flexibility does not moderate the link between alignment and agility, except in a volatile environment, we reveal that IT infrastructure flexibility has a positive and significant main effect on agility. In fact, the effect of IT infrastructure flexibility on agility is as strong as the effect of alignment on agility. This research extends and integrates the literature on strategic IT alignment and organizational agility at a time when both alignment and agility are recognized as critical and concurrent organizational goals.",[],"Tallon, Paul P.",N/A,"Sellinger School of Business and Management, Loyola University, 4501 N. Charles Street, Baltimore, MD 21210 U.S.A."
https://misq.umn.edu/misq/article/35/2/463/1451/Competing-Perspectives-on-the-Link-between,MIS Quarterly,Competing Perspectives on the Link between Strategic Information Technology Alignment and Organizational Agility: Insights from a Mediation Model1,"Volume 35, Issue 2",June 2011,"Strategic information technology alignment remains a top priority for business and IT executives. Yet with a recent rise in environmental volatility, firms are asking how to be more agile in identifying and responding to market-based threats and opportunities. Whether alignment helps or hurts agility is an unresolved issue. This paper presents a variety of arguments from the literature that alternately predict a positive or negative relationship between alignment and agility. This relationship is then tested using a model in which agility mediates the link between alignment and firm performance under varying conditions of IT infrastructure flexibility and environmental volatility. Using data from a matched survey of IT and business executives in 241 firms, we uncover a positive and significant link between alignment and agility and between agility and firm performance. We also show that the effect of alignment on performance is fully mediated by agility, that environmental volatility positively moderates the link between agility and firm performance, and that agility has a greater impact on firm performance in more volatile markets. While IT infrastructure flexibility does not moderate the link between alignment and agility, except in a volatile environment, we reveal that IT infrastructure flexibility has a positive and significant main effect on agility. In fact, the effect of IT infrastructure flexibility on agility is as strong as the effect of alignment on agility. This research extends and integrates the literature on strategic IT alignment and organizational agility at a time when both alignment and agility are recognized as critical and concurrent organizational goals.",[],"Pinsonneault, Alain",N/A,"Desautels Faculty of Management, McGill University, 1001 Sherbrooke Street West, Montreal, Quebec H3A 1G5 Canada"
https://misq.umn.edu/misq/article/35/2/487/1465/CIO-Reporting-Structure-Strategic-Positioning-and,MIS Quarterly,"CIO Reporting Structure, Strategic Positioning, and Firm Performance1","Volume 35, Issue 2",June 2011,"Almost 30 years after the introduction of the CIO position, the ideal CIO reporting structure (whether the CIO should report to the CEO or the CFO) is yet to be identified. There is an intuitive assumption among some proponents of IT that the CIO should always report to the CEO to promote the importance of IT and the CIO’s clout in the firm, while some adversaries of IT call for a CIO–CFO reporting structure to keep a tab on IT spending. However, we challenge these two ad hoc prescriptions by arguing that neither CIO reporting structure is necessarily optimal, and that the CIO reporting structure should not be used to gauge the strategic role of IT in the firm.First, extending the strategy–structure paradigm, we propose that a firm’s strategic positioning (differentiation or cost leadership) should be a primary determinant of its CIO reporting structure. We hypothesize that differentiators are more likely to have their CIO report to the CEO in order to pursue IT initiatives that help the firm’s differentiation strategy. We also hypothesize that cost leaders are more likely to have their CIO report to the CFO to lead IT initiatives to facilitate the firm’s cost leadership strategy. Second, extending the alignment–fit view, we propose that firms that align their CIO reporting structure with their strategic positioning (specifically, differentiation with a CIO–CEO reporting structure and cost leadership with a CIO–CFO reporting structure) will have superior future performance.Longitudinal data from two periods (1990-1993-2006) support the proposed hypotheses, validating the relationship between a firm’s strategic positioning and its CIO reporting structure, and also the positive impact of their alignment on firm performance. These results challenge the ad hoc prescriptions about the CIO reporting structure, demonstrating that a CIO–CEO reporting structure is only superior for differentiators and a CIO–CFO reporting structure is superior only for cost leaders. The CIO reporting structure must, therefore, be designed to align with the firm’s strategic positioning, independent of whether IT plays a key strategic role in the firm.",[],"Banker, Rajiv D.",N/A,"Fox School of Business, Temple University, 210C Speakman Hall, Philadelphia, PA 19122 U.S.A."
https://misq.umn.edu/misq/article/35/2/487/1465/CIO-Reporting-Structure-Strategic-Positioning-and,MIS Quarterly,"CIO Reporting Structure, Strategic Positioning, and Firm Performance1","Volume 35, Issue 2",June 2011,"Almost 30 years after the introduction of the CIO position, the ideal CIO reporting structure (whether the CIO should report to the CEO or the CFO) is yet to be identified. There is an intuitive assumption among some proponents of IT that the CIO should always report to the CEO to promote the importance of IT and the CIO’s clout in the firm, while some adversaries of IT call for a CIO–CFO reporting structure to keep a tab on IT spending. However, we challenge these two ad hoc prescriptions by arguing that neither CIO reporting structure is necessarily optimal, and that the CIO reporting structure should not be used to gauge the strategic role of IT in the firm.First, extending the strategy–structure paradigm, we propose that a firm’s strategic positioning (differentiation or cost leadership) should be a primary determinant of its CIO reporting structure. We hypothesize that differentiators are more likely to have their CIO report to the CEO in order to pursue IT initiatives that help the firm’s differentiation strategy. We also hypothesize that cost leaders are more likely to have their CIO report to the CFO to lead IT initiatives to facilitate the firm’s cost leadership strategy. Second, extending the alignment–fit view, we propose that firms that align their CIO reporting structure with their strategic positioning (specifically, differentiation with a CIO–CEO reporting structure and cost leadership with a CIO–CFO reporting structure) will have superior future performance.Longitudinal data from two periods (1990-1993-2006) support the proposed hypotheses, validating the relationship between a firm’s strategic positioning and its CIO reporting structure, and also the positive impact of their alignment on firm performance. These results challenge the ad hoc prescriptions about the CIO reporting structure, demonstrating that a CIO–CEO reporting structure is only superior for differentiators and a CIO–CFO reporting structure is superior only for cost leaders. The CIO reporting structure must, therefore, be designed to align with the firm’s strategic positioning, independent of whether IT plays a key strategic role in the firm.",[],"Hu, Nan",N/A,"School of Information Systems, Singapore Management Universy, 80 Stamford Road, Singapore 178902"
https://misq.umn.edu/misq/article/35/2/487/1465/CIO-Reporting-Structure-Strategic-Positioning-and,MIS Quarterly,"CIO Reporting Structure, Strategic Positioning, and Firm Performance1","Volume 35, Issue 2",June 2011,"Almost 30 years after the introduction of the CIO position, the ideal CIO reporting structure (whether the CIO should report to the CEO or the CFO) is yet to be identified. There is an intuitive assumption among some proponents of IT that the CIO should always report to the CEO to promote the importance of IT and the CIO’s clout in the firm, while some adversaries of IT call for a CIO–CFO reporting structure to keep a tab on IT spending. However, we challenge these two ad hoc prescriptions by arguing that neither CIO reporting structure is necessarily optimal, and that the CIO reporting structure should not be used to gauge the strategic role of IT in the firm.First, extending the strategy–structure paradigm, we propose that a firm’s strategic positioning (differentiation or cost leadership) should be a primary determinant of its CIO reporting structure. We hypothesize that differentiators are more likely to have their CIO report to the CEO in order to pursue IT initiatives that help the firm’s differentiation strategy. We also hypothesize that cost leaders are more likely to have their CIO report to the CFO to lead IT initiatives to facilitate the firm’s cost leadership strategy. Second, extending the alignment–fit view, we propose that firms that align their CIO reporting structure with their strategic positioning (specifically, differentiation with a CIO–CEO reporting structure and cost leadership with a CIO–CFO reporting structure) will have superior future performance.Longitudinal data from two periods (1990-1993-2006) support the proposed hypotheses, validating the relationship between a firm’s strategic positioning and its CIO reporting structure, and also the positive impact of their alignment on firm performance. These results challenge the ad hoc prescriptions about the CIO reporting structure, demonstrating that a CIO–CEO reporting structure is only superior for differentiators and a CIO–CFO reporting structure is superior only for cost leaders. The CIO reporting structure must, therefore, be designed to align with the firm’s strategic positioning, independent of whether IT plays a key strategic role in the firm.",[],"Pavlou, Paul A.",N/A,"Fox School of Business, Temple University, 210DSpeakman Hall, Philadelphia, PA 19122 U.S.A."
https://misq.umn.edu/misq/article/35/2/487/1465/CIO-Reporting-Structure-Strategic-Positioning-and,MIS Quarterly,"CIO Reporting Structure, Strategic Positioning, and Firm Performance1","Volume 35, Issue 2",June 2011,"Almost 30 years after the introduction of the CIO position, the ideal CIO reporting structure (whether the CIO should report to the CEO or the CFO) is yet to be identified. There is an intuitive assumption among some proponents of IT that the CIO should always report to the CEO to promote the importance of IT and the CIO’s clout in the firm, while some adversaries of IT call for a CIO–CFO reporting structure to keep a tab on IT spending. However, we challenge these two ad hoc prescriptions by arguing that neither CIO reporting structure is necessarily optimal, and that the CIO reporting structure should not be used to gauge the strategic role of IT in the firm.First, extending the strategy–structure paradigm, we propose that a firm’s strategic positioning (differentiation or cost leadership) should be a primary determinant of its CIO reporting structure. We hypothesize that differentiators are more likely to have their CIO report to the CEO in order to pursue IT initiatives that help the firm’s differentiation strategy. We also hypothesize that cost leaders are more likely to have their CIO report to the CFO to lead IT initiatives to facilitate the firm’s cost leadership strategy. Second, extending the alignment–fit view, we propose that firms that align their CIO reporting structure with their strategic positioning (specifically, differentiation with a CIO–CEO reporting structure and cost leadership with a CIO–CFO reporting structure) will have superior future performance.Longitudinal data from two periods (1990-1993-2006) support the proposed hypotheses, validating the relationship between a firm’s strategic positioning and its CIO reporting structure, and also the positive impact of their alignment on firm performance. These results challenge the ad hoc prescriptions about the CIO reporting structure, demonstrating that a CIO–CEO reporting structure is only superior for differentiators and a CIO–CFO reporting structure is superior only for cost leaders. The CIO reporting structure must, therefore, be designed to align with the firm’s strategic positioning, independent of whether IT plays a key strategic role in the firm.",[],"Luftman, Jerry",N/A,"Howe School of Technology Management, Stevens Institute of Technology, Babbio Center 407, 1 Castle Point on Hudson, Hoboken, NJ 07030 U.S.A."
https://misq.umn.edu/misq/article/35/2/505/1468/Capturing-Bottom-Up-Information-Technology-Use,MIS Quarterly,Capturing Bottom-Up Information Technology Use Processes: A Complex Adaptive Systems Model1,"Volume 35, Issue 2",June 2011,"Although information systems researchers have long recognized the possibility for collective- level information technology use patterns and outcomes to emerge from individual-level IT use behaviors, few have explored the key properties and mechanisms involved in this bottom-up IT use process. This paper seeks to build a theoretical framework drawing on the concepts and the analytical tool of complex adaptive systems (CAS) theory. The paper presents a CAS model of IT use that encodes a bottom-up IT use process into three interrelated elements: agents that consist of the basic entities of actions in an IT use process, interactions that refer to the mutually adaptive behaviors of agents, and an environment that represents the social organizational contexts of IT use. Agent-based modeling is introduced as the analytical tool for computationally representing and examining the CAS model of IT use. The operationability of the CAS model and the analytical tool are demonstrated through a theory-building exercise translating an interpretive case study of IT use to a specific version of the CAS model. While Orlikowski (1996) raised questions regarding the impacts of employee learning, IT flexibility, and workplace rigidity on IT-based organization transformation, the CAS model indicates that these factors in individual-level actions do not have a direct causal linkage with organizational- level IT use patterns and outcomes. This theory-building exercise manifests the intriguing nature of the bottom-up IT use process: collective-level IT use patterns and outcomes are the logical and yet often unintended or unforeseeable consequences of individual-level behaviors. The CAS model of IT use offers opportunities for expanding the theoretical and methodological scope of the IT use literature.",[],"Nan, Ning",N/A,"Michael F. Price College of Business, University of Oklahoma, 307 W. Brooks, Room 305D, Norman, OK 73019 U.S.A."
https://misq.umn.edu/misq/article/35/2/iii/1463/Editor-s-CommentsAn-Update-and-Extension-to-SEM,MIS Quarterly,Editor’s CommentsAn Update and Extension to SEM Guidelines for Administrative and Social Science Research1,"Volume 35, Issue 2",June 2011,N/A,[],"Gefen, David",N/A,"Professor of Management Information Systems, Drexel University"
https://misq.umn.edu/misq/article/35/2/iii/1463/Editor-s-CommentsAn-Update-and-Extension-to-SEM,MIS Quarterly,Editor’s CommentsAn Update and Extension to SEM Guidelines for Administrative and Social Science Research1,"Volume 35, Issue 2",June 2011,N/A,[],"Rigdon, Edward E.",N/A,Georgia State University
https://misq.umn.edu/misq/article/35/2/iii/1463/Editor-s-CommentsAn-Update-and-Extension-to-SEM,MIS Quarterly,Editor’s CommentsAn Update and Extension to SEM Guidelines for Administrative and Social Science Research1,"Volume 35, Issue 2",June 2011,N/A,[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/35/2/xv/1467/Editorial-IntroductionSpecial-Research-Commentary,MIS Quarterly,Editorial IntroductionSpecial Research Commentary Series on Advanced Methodological Thinking for Quantitative Research,"Volume 35, Issue 2",June 2011,N/A,[],"Stafford, Thomas",N/A,University of Memphis
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/35/3/553/2601/Information-Systems-Research-Behaviors-What-Are,MIS Quarterly,Information Systems Research Behaviors: What Are The Normative Standards?1,"Volume 35, Issue 3",September 2011,"Information systems researchers frequently face quandaries in their professional lives. We present the results of a study of academic IS researchers that assesses their judgments and the prevalence of 29 questionable research-related behaviors. We find that the focus and stages of researchers’ careers influence their judgments of these behaviors. Membership in the Association for Information Systems (AIS) and adherence to the AIS Code of Research Conduct are also associated with IS researchers’ judgments. There is strong evidence to suggest that IS researchers expect to engage in questionable behaviors more in the future than they report having done in the past. As a result of the study, we recommend that the IS community revisit the AIS Code of Research Conduct on a regular basis and take active steps to both educate its members on professional normative standards and to uphold the standards of our community.",[],"Allen, Gove N.",N/A,"Marriot School of Management, Brigham Young University, 783 Tanner Building, Provo, UT 84602 U.S.A."
https://misq.umn.edu/misq/article/35/3/553/2601/Information-Systems-Research-Behaviors-What-Are,MIS Quarterly,Information Systems Research Behaviors: What Are The Normative Standards?1,"Volume 35, Issue 3",September 2011,"Information systems researchers frequently face quandaries in their professional lives. We present the results of a study of academic IS researchers that assesses their judgments and the prevalence of 29 questionable research-related behaviors. We find that the focus and stages of researchers’ careers influence their judgments of these behaviors. Membership in the Association for Information Systems (AIS) and adherence to the AIS Code of Research Conduct are also associated with IS researchers’ judgments. There is strong evidence to suggest that IS researchers expect to engage in questionable behaviors more in the future than they report having done in the past. As a result of the study, we recommend that the IS community revisit the AIS Code of Research Conduct on a regular basis and take active steps to both educate its members on professional normative standards and to uphold the standards of our community.",[],"Ball, Nicholas L.",N/A,"Marriot School of Management, Brigham Young University, 783 Tanner Building, Provo, UT 84602 U.S.A."
https://misq.umn.edu/misq/article/35/3/553/2601/Information-Systems-Research-Behaviors-What-Are,MIS Quarterly,Information Systems Research Behaviors: What Are The Normative Standards?1,"Volume 35, Issue 3",September 2011,"Information systems researchers frequently face quandaries in their professional lives. We present the results of a study of academic IS researchers that assesses their judgments and the prevalence of 29 questionable research-related behaviors. We find that the focus and stages of researchers’ careers influence their judgments of these behaviors. Membership in the Association for Information Systems (AIS) and adherence to the AIS Code of Research Conduct are also associated with IS researchers’ judgments. There is strong evidence to suggest that IS researchers expect to engage in questionable behaviors more in the future than they report having done in the past. As a result of the study, we recommend that the IS community revisit the AIS Code of Research Conduct on a regular basis and take active steps to both educate its members on professional normative standards and to uphold the standards of our community.",[],"Smith, H. Jeff",N/A,"Department of Decision Sciences and MIS, Farmer School of Business, Miami University, 3095 FSB, Oxford, OH 45056 U.S.A."
https://misq.umn.edu/misq/article/35/3/553/529/Predictive-Analytics-in-Information-Systems,MIS Quarterly,Predictive Analytics in Information Systems Research1,"Volume 35, Issue 3",September 2011,"This research essay highlights the need to integrate predictive analytics into information systems research and shows several concrete ways in which this goal can be accomplished. Predictive analytics include empirical methods (statistical and other) that generate data predictions as well as methods for assessing predictive power. Predictive analytics not only assist in creating practically useful models, they also play an important role alongside explanatory modeling in theory building and theory testing. We describe six roles for predictive analytics: new theory generation, measurement development, comparison of competing theories, improvement of existing models, relevance assessment, and assessment of the predictability of empirical phenomena. Despite the importance of predictive analytics, we find that they are rare in the empirical IS literature. Extant IS literature relies nearly exclusively on explanatory statistical modeling, where statistical inference is used to test and evaluate the explanatory power of underlying causal models, and predictive power is assumed to follow automatically from the explanatory model. However, explanatory power does not imply predictive power and thus predictive analytics are necessary for assessing predictive power and for building empirical models that predict well. To show that predictive analytics and explanatory statistical modeling are fundamentally disparate, we show that they are different in each step of the modeling process. These differences translate into different final models, so that a pure explanatory statistical model is best tuned for testing causal hypotheses and a pure predictive model is best in terms of predictive power. We convert a well-known explanatory paper on TAM to a predictive context to illustrate these differences and show how predictive analytics can add theoretical and practical value to IS research.",[],"Shmueli, Galit",N/A,"Smith School of Business, University of Maryland, College Park, MD 20742 U.S.A."
https://misq.umn.edu/misq/article/35/3/553/529/Predictive-Analytics-in-Information-Systems,MIS Quarterly,Predictive Analytics in Information Systems Research1,"Volume 35, Issue 3",September 2011,"This research essay highlights the need to integrate predictive analytics into information systems research and shows several concrete ways in which this goal can be accomplished. Predictive analytics include empirical methods (statistical and other) that generate data predictions as well as methods for assessing predictive power. Predictive analytics not only assist in creating practically useful models, they also play an important role alongside explanatory modeling in theory building and theory testing. We describe six roles for predictive analytics: new theory generation, measurement development, comparison of competing theories, improvement of existing models, relevance assessment, and assessment of the predictability of empirical phenomena. Despite the importance of predictive analytics, we find that they are rare in the empirical IS literature. Extant IS literature relies nearly exclusively on explanatory statistical modeling, where statistical inference is used to test and evaluate the explanatory power of underlying causal models, and predictive power is assumed to follow automatically from the explanatory model. However, explanatory power does not imply predictive power and thus predictive analytics are necessary for assessing predictive power and for building empirical models that predict well. To show that predictive analytics and explanatory statistical modeling are fundamentally disparate, we show that they are different in each step of the modeling process. These differences translate into different final models, so that a pure explanatory statistical model is best tuned for testing causal hypotheses and a pure predictive model is best in terms of predictive power. We convert a well-known explanatory paper on TAM to a predictive context to illustrate these differences and show how predictive analytics can add theoretical and practical value to IS research.",[],"Koppius, Otto R.",N/A,"Rotterdam School of Management, Erasmus University, Rotterdam, The Netherlands"
https://misq.umn.edu/misq/article/35/3/573/530/An-Exploration-of-Organizational-Level-Information,MIS Quarterly,An Exploration of Organizational Level Information Systems Discontinuance Intentions1,"Volume 35, Issue 3",September 2011,"Limited attention has been directed toward examining post-adoption stages of the information system life cycle. In particular, the final stages of this life cycle have been largely ignored despite the fact that most systems eventually reach the end of their useful life. This oversight is somewhat surprising given that end-of-life decisions can have significant implications for user effectiveness, the value extracted from IS investments, and organizational performance. Given this apparent gap, a multi-method empirical study was undertaken to improve our understanding of organizational level information system discontinuance. Research commenced with the development of a broad theoretical framework consistent with the technology–organization– environment (TOE) paradigm. The resulting framework was then used to guide a series of semi-structured interviews with organizational decision makers in an effort to inductively identify salient influences on the formation of IS discontinuance intentions. A set of research hypotheses were formulated based on the understanding obtained during these interviews and subsequently tested via a random survey of senior IS decision makers at U.S. and Canadian organizations. Data obtained from the survey responses was analyzed using partial least squares (PLS). Results of this analysis suggest that system capability shortcomings, limited availability of system support, and low levels of technical integration were key determinants of increased intentions to replace an existing system. Notably, investments in existing systems did not appear to significantly undermine organizational replacement intentions despite support for this possibility from both theory and our semi-structured interviews.",[],"Furneaux, Brent",N/A,"School of Business and Economics, Maastricht University, P.O. Box 616, 6200 MD Maastricht, The Netherlands"
https://misq.umn.edu/misq/article/35/3/573/530/An-Exploration-of-Organizational-Level-Information,MIS Quarterly,An Exploration of Organizational Level Information Systems Discontinuance Intentions1,"Volume 35, Issue 3",September 2011,"Limited attention has been directed toward examining post-adoption stages of the information system life cycle. In particular, the final stages of this life cycle have been largely ignored despite the fact that most systems eventually reach the end of their useful life. This oversight is somewhat surprising given that end-of-life decisions can have significant implications for user effectiveness, the value extracted from IS investments, and organizational performance. Given this apparent gap, a multi-method empirical study was undertaken to improve our understanding of organizational level information system discontinuance. Research commenced with the development of a broad theoretical framework consistent with the technology–organization– environment (TOE) paradigm. The resulting framework was then used to guide a series of semi-structured interviews with organizational decision makers in an effort to inductively identify salient influences on the formation of IS discontinuance intentions. A set of research hypotheses were formulated based on the understanding obtained during these interviews and subsequently tested via a random survey of senior IS decision makers at U.S. and Canadian organizations. Data obtained from the survey responses was analyzed using partial least squares (PLS). Results of this analysis suggest that system capability shortcomings, limited availability of system support, and low levels of technical integration were key determinants of increased intentions to replace an existing system. Notably, investments in existing systems did not appear to significantly undermine organizational replacement intentions despite support for this possibility from both theory and our semi-structured interviews.",[],"Wade, Michael",N/A,"IMD, Ch. de Bellerive 23, P.O. Box 915, CH - 1001, Lausanne, Switzerland"
https://misq.umn.edu/misq/article/35/3/599/2590/The-Effects-of-Digital-Trading-Platforms-on,MIS Quarterly,The Effects of Digital Trading Platforms on Commodity Prices in Agricultural Supply Chains1,"Volume 35, Issue 3",September 2011,"Digital platforms for buying and selling agricultural commodities have generated significant interest in the trade literature as a way to link rural communities to the Internet. Yet, the extent to which these digital platforms actually translate into higher commodity prices for producers remains an open research question. We investigate this question by comparing transaction data on trading various grades of coffee from a recently implemented digital platform in India with similar transactions from a physical commodity auction held weekly, and farm-gate prices in the coffee producing regions of India. Although the digital platform prices closely track the physical commodity auction prices, producers obtain significantly higher prices when they sell the commodity through the digital platform rather than at the farm-gate through brokers who operate in their regions. However, coffee grades with higher price volatility and premium coffee grades that require face-to-face interactions to verify quality obtain lower prices on the digital platform. Our results also indicate that market participants who control the transaction obtain better prices. We discuss the implications of our findings for governments and platform providers.",[],"Banker, Rajiv",N/A,"Fox School of Business, Temple University, Philadelphia, PA 19122, U.S.A."
https://misq.umn.edu/misq/article/35/3/599/2590/The-Effects-of-Digital-Trading-Platforms-on,MIS Quarterly,The Effects of Digital Trading Platforms on Commodity Prices in Agricultural Supply Chains1,"Volume 35, Issue 3",September 2011,"Digital platforms for buying and selling agricultural commodities have generated significant interest in the trade literature as a way to link rural communities to the Internet. Yet, the extent to which these digital platforms actually translate into higher commodity prices for producers remains an open research question. We investigate this question by comparing transaction data on trading various grades of coffee from a recently implemented digital platform in India with similar transactions from a physical commodity auction held weekly, and farm-gate prices in the coffee producing regions of India. Although the digital platform prices closely track the physical commodity auction prices, producers obtain significantly higher prices when they sell the commodity through the digital platform rather than at the farm-gate through brokers who operate in their regions. However, coffee grades with higher price volatility and premium coffee grades that require face-to-face interactions to verify quality obtain lower prices on the digital platform. Our results also indicate that market participants who control the transaction obtain better prices. We discuss the implications of our findings for governments and platform providers.",[],"Mitra, Sabyasachi",N/A,"College of Management, Georgia Institute of Technology, Atlanta, GA 30308, U.S.A."
https://misq.umn.edu/misq/article/35/3/599/2590/The-Effects-of-Digital-Trading-Platforms-on,MIS Quarterly,The Effects of Digital Trading Platforms on Commodity Prices in Agricultural Supply Chains1,"Volume 35, Issue 3",September 2011,"Digital platforms for buying and selling agricultural commodities have generated significant interest in the trade literature as a way to link rural communities to the Internet. Yet, the extent to which these digital platforms actually translate into higher commodity prices for producers remains an open research question. We investigate this question by comparing transaction data on trading various grades of coffee from a recently implemented digital platform in India with similar transactions from a physical commodity auction held weekly, and farm-gate prices in the coffee producing regions of India. Although the digital platform prices closely track the physical commodity auction prices, producers obtain significantly higher prices when they sell the commodity through the digital platform rather than at the farm-gate through brokers who operate in their regions. However, coffee grades with higher price volatility and premium coffee grades that require face-to-face interactions to verify quality obtain lower prices on the digital platform. Our results also indicate that market participants who control the transaction obtain better prices. We discuss the implications of our findings for governments and platform providers.",[],"Sambamurthy, V.",N/A,"Eli Broad College of Business, Michigan State University, East Lansing, MI 48824, U.S.A."
https://misq.umn.edu/misq/article/35/3/613/526/Membership-Turnover-and-Collaboration-Success-in,MIS Quarterly,Membership Turnover and Collaboration Success in Online Communities: Explaining Rises and Falls from Grace in Wikipedia1,"Volume 35, Issue 3",September 2011,"Firms increasingly turn to online communities to create valuable information. These communities are empowered by new information technology-enabled collaborative tools, tools such as blogs, wikis, and social networks. Collaboration on these platforms is characterized by considerable membership turnover, which could have significant effects on collaborative outcomes. We hypothesize that membership retention relates in a curvilinear fashion to effective collaboration: positively up to a threshold and negatively thereafter. The longitudinal history of 2,065 featured articles on Wikipedia offers support for this hypotheses: Contributions from a mixture of new and experienced participants both increases the likelihood that an article will be promoted to featured article status and decreases the risk it will be demoted after having been promoted. These findings imply that, contrary to many of the assumptions in previous research, participant retention does not have a strictly positive effect on emerging collaborative environments. Further analysis of our data provides empirical evidence that knowledge creation and knowledge retention are actually distinct phases of community-based peer production, and that communities may on average experience more turnover than ideal during the knowledge retention phase.",[],"Ransbotham, Sam",N/A,"Carroll School of Management, Boston College, 140 Commonwealth Avenue,Chestnut Hill, MA 02467 U.S.A."
https://misq.umn.edu/misq/article/35/3/613/526/Membership-Turnover-and-Collaboration-Success-in,MIS Quarterly,Membership Turnover and Collaboration Success in Online Communities: Explaining Rises and Falls from Grace in Wikipedia1,"Volume 35, Issue 3",September 2011,"Firms increasingly turn to online communities to create valuable information. These communities are empowered by new information technology-enabled collaborative tools, tools such as blogs, wikis, and social networks. Collaboration on these platforms is characterized by considerable membership turnover, which could have significant effects on collaborative outcomes. We hypothesize that membership retention relates in a curvilinear fashion to effective collaboration: positively up to a threshold and negatively thereafter. The longitudinal history of 2,065 featured articles on Wikipedia offers support for this hypotheses: Contributions from a mixture of new and experienced participants both increases the likelihood that an article will be promoted to featured article status and decreases the risk it will be demoted after having been promoted. These findings imply that, contrary to many of the assumptions in previous research, participant retention does not have a strictly positive effect on emerging collaborative environments. Further analysis of our data provides empirical evidence that knowledge creation and knowledge retention are actually distinct phases of community-based peer production, and that communities may on average experience more turnover than ideal during the knowledge retention phase.",[],"Kane, Gerald C.",N/A,"Carroll School of Management, Boston College, 140 Commonwealth Avenue,Chestnut Hill, MA 02467 U.S.A."
https://misq.umn.edu/misq/article/35/3/629/527/Innovation-Impacts-of-Using-Social-Bookmarking,MIS Quarterly,Innovation Impacts of Using Social Bookmarking Systems1,"Volume 35, Issue 3",September 2011,"Many organizational innovations can be explained by the movement of ideas and information from one social context to another, “from where they are known to where they are not” (Hargadon 2002, p. 41). A relatively new technology, social bookmarking, is increasingly being used in many organizations (McAfee 2006), and may enhance employee innovativeness by providing a new, socially mediated channel for discovering information. Users of such systems create publicly viewable lists of bookmarks (each being a hyperlink to an information resource) and often assign searchable keywords (“tags”) to these bookmarks. We explore two different perspectives on how accessing others’ bookmarks could enhance how innovative an individual is at work. First, we develop two hypotheses around the idea that quantity may be a proxy for diversity, following a well established literature that holds that the more information obtained and the larger the number of sources consulted, the higher the likelihood an individual will come across novel ideas. Next, we offer two hypotheses adapted from social network research that argue that the shape of the network of connections that is created when individuals access each others’ bookmarks can reflect information novelty, and that individuals whose networks bridge more structural holes and have greater effective reach are likely to be more innovative. An analysis of bookmarking system use in a global professional services firm provides strong support for the social diversity of information sources as a predictor of employee innovativeness, but no support that the number of bookmarks accessed matters. By extending the social networks literature to theorize the functionalities offered by social bookmarking systems, this research establishes structural holes theory as a valuable lens through which social technologies may be understood.",[],"Gray, Peter H.",N/A,"McIntire School of Commerce, University of Virginia, 345 Rouss & Robertson Hall, East Lawn, Charlottesville, VA 22904-4173 U.S.A."
https://misq.umn.edu/misq/article/35/3/629/527/Innovation-Impacts-of-Using-Social-Bookmarking,MIS Quarterly,Innovation Impacts of Using Social Bookmarking Systems1,"Volume 35, Issue 3",September 2011,"Many organizational innovations can be explained by the movement of ideas and information from one social context to another, “from where they are known to where they are not” (Hargadon 2002, p. 41). A relatively new technology, social bookmarking, is increasingly being used in many organizations (McAfee 2006), and may enhance employee innovativeness by providing a new, socially mediated channel for discovering information. Users of such systems create publicly viewable lists of bookmarks (each being a hyperlink to an information resource) and often assign searchable keywords (“tags”) to these bookmarks. We explore two different perspectives on how accessing others’ bookmarks could enhance how innovative an individual is at work. First, we develop two hypotheses around the idea that quantity may be a proxy for diversity, following a well established literature that holds that the more information obtained and the larger the number of sources consulted, the higher the likelihood an individual will come across novel ideas. Next, we offer two hypotheses adapted from social network research that argue that the shape of the network of connections that is created when individuals access each others’ bookmarks can reflect information novelty, and that individuals whose networks bridge more structural holes and have greater effective reach are likely to be more innovative. An analysis of bookmarking system use in a global professional services firm provides strong support for the social diversity of information sources as a predictor of employee innovativeness, but no support that the number of bookmarks accessed matters. By extending the social networks literature to theorize the functionalities offered by social bookmarking systems, this research establishes structural holes theory as a valuable lens through which social technologies may be understood.",[],"Parise, Salvatore",N/A,"Technology, Operations and Information Management Division, Babson College, Babson Park, MA 02457-0310 U.S.A."
https://misq.umn.edu/misq/article/35/3/629/527/Innovation-Impacts-of-Using-Social-Bookmarking,MIS Quarterly,Innovation Impacts of Using Social Bookmarking Systems1,"Volume 35, Issue 3",September 2011,"Many organizational innovations can be explained by the movement of ideas and information from one social context to another, “from where they are known to where they are not” (Hargadon 2002, p. 41). A relatively new technology, social bookmarking, is increasingly being used in many organizations (McAfee 2006), and may enhance employee innovativeness by providing a new, socially mediated channel for discovering information. Users of such systems create publicly viewable lists of bookmarks (each being a hyperlink to an information resource) and often assign searchable keywords (“tags”) to these bookmarks. We explore two different perspectives on how accessing others’ bookmarks could enhance how innovative an individual is at work. First, we develop two hypotheses around the idea that quantity may be a proxy for diversity, following a well established literature that holds that the more information obtained and the larger the number of sources consulted, the higher the likelihood an individual will come across novel ideas. Next, we offer two hypotheses adapted from social network research that argue that the shape of the network of connections that is created when individuals access each others’ bookmarks can reflect information novelty, and that individuals whose networks bridge more structural holes and have greater effective reach are likely to be more innovative. An analysis of bookmarking system use in a global professional services firm provides strong support for the social diversity of information sources as a predictor of employee innovativeness, but no support that the number of bookmarks accessed matters. By extending the social networks literature to theorize the functionalities offered by social bookmarking systems, this research establishes structural holes theory as a valuable lens through which social technologies may be understood.",[],"Iyer, Bala",N/A,"Technology, Operations and Information Management Division, Babson College, Babson Park, MA 02457-0310 U.S.A."
https://misq.umn.edu/misq/article/35/3/645/521/Stepping-Into-the-Internet-New-Ventures-in-Virtual,MIS Quarterly,Stepping Into the Internet: New Ventures in Virtual Worlds,"Volume 35, Issue 3",September 2011,N/A,[],"Wasko, Molly",N/A,"School of Business, University of Alabama at Birmingham, Birmingham, AL 35294 U.S.A."
https://misq.umn.edu/misq/article/35/3/645/521/Stepping-Into-the-Internet-New-Ventures-in-Virtual,MIS Quarterly,Stepping Into the Internet: New Ventures in Virtual Worlds,"Volume 35, Issue 3",September 2011,N/A,[],"Teigland, Robin",N/A,"Center for Strategy and Competitiveness, Stockholm School of Economics, Holländargatan 32, Stockholm, Sweden"
https://misq.umn.edu/misq/article/35/3/645/521/Stepping-Into-the-Internet-New-Ventures-in-Virtual,MIS Quarterly,Stepping Into the Internet: New Ventures in Virtual Worlds,"Volume 35, Issue 3",September 2011,N/A,[],"Leidner, Dorothy",N/A,"Hankamer School of Business, Baylor University, Wako, TX 76798 U.S.A."
https://misq.umn.edu/misq/article/35/3/645/521/Stepping-Into-the-Internet-New-Ventures-in-Virtual,MIS Quarterly,Stepping Into the Internet: New Ventures in Virtual Worlds,"Volume 35, Issue 3",September 2011,N/A,[],"Jarvenpaa, Sirkka",N/A,"McCombs School of Business, University of Texas at Austin, Austin, TX 78712 U.S.A."
https://misq.umn.edu/misq/article/35/3/iii/520/Editor-s-CommentsHow-Reviews-Shape-MIS-Quarterly-A,MIS Quarterly,Editor’s CommentsHow Reviews Shape MIS Quarterly: A Primer for Reviewers and Editors,"Volume 35, Issue 3",September 2011,N/A,[],"Kohli, Rajiv",N/A,Mason School of Business The College of William and Mary
https://misq.umn.edu/misq/article/35/3/iii/520/Editor-s-CommentsHow-Reviews-Shape-MIS-Quarterly-A,MIS Quarterly,Editor’s CommentsHow Reviews Shape MIS Quarterly: A Primer for Reviewers and Editors,"Volume 35, Issue 3",September 2011,N/A,[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/35/3/653/525/Control-Over-Virtual-Worlds-by-Game-Companies,MIS Quarterly,Control Over Virtual Worlds by Game Companies: Issues And Recommendations1,"Volume 35, Issue 3",September 2011,"Game companies use five components—four core components and one complementary one—in a 5Cs model to ensure the control and development of virtual worlds. A multidisciplinary review of the literature reveals that game companies make use of copyright, codes, creativity, and community to do this. They use the contract as a complementary component to reinforce their control over the four basic components and to compensate for the lacunae they present. In order to examine the extent to which game companies use the contract in this way, an analysis is performed of all contractual documents from a sample of 20 virtual worlds, providing evidence of general trends and emphasizing any differences between the virtual worlds in terms of the business and gaming models sought by each game company. An explanation is provided of why these contracts do not constitute a sustainable model for the game companies, given the high level of legal insecurity they present. Some basic recommendations can be made in order to improve the sustainability of the 5Cs model by modifying these contracts in such a way that they are enforceable and by matching their content with appropriate business and gaming models. This could lead to further studies aimed at providing answers to some of the intriguing issues affecting scholars and practitioners.",[],"Roquilly, Christophe",N/A,"EDHEC Business School, 58 rue du Port, 59046 Lille cédex, France"
https://misq.umn.edu/misq/article/35/3/673/522/Design-Principles-for-Virtual-Worlds1,MIS Quarterly,Design Principles for Virtual Worlds1,"Volume 35, Issue 3",September 2011,"In this research note, we examine the design, development, validation, and use of virtual worlds. Our purpose in doing so is to extend the design science paradigm by developing a set of design principles applicable to the context of virtual environments, particularly those using agent-based simulation as their underlying technology. Our central argument is that virtual worlds comprise a new class of information system, one that combines the structural aspects of traditional modeling and simulation systems in concert with emergent user dynamics of systems supporting emergent knowledge processes. Our approach involves two components. First, we review the characteristics of agent-based virtual worlds (ABVWs) to discern design requirements that may challenge current design theory. From this review, we derive a set of design principles based on deep versus emergent structures where deep structures reflect conventional modeling and simulation system architectures and emergent structures capture the unpredictable user–system dynamics inherent in emergent knowledge processes, which increasingly characterize virtual worlds. We illustrate how these design challenges are addressed with an exemplar of a complex mirror world, a large-scale ABVW we developed called Sentient World. Our contribution is the insight of partitioning ABVW architectures into deep and emergent structures that mirror modeling systems and emergent knowledge processes respectively, while developing extended design principles to facilitate their integration. We conclude with a discussion of the implications of our design principles for informing and guiding future research and practice.",[],"Chaturvedi, Alok R.",N/A,"Krannert School of Management, Purdue University, 403 W. State Street, West Lafayette, IN 47907 U.S.A."
https://misq.umn.edu/misq/article/35/3/673/522/Design-Principles-for-Virtual-Worlds1,MIS Quarterly,Design Principles for Virtual Worlds1,"Volume 35, Issue 3",September 2011,"In this research note, we examine the design, development, validation, and use of virtual worlds. Our purpose in doing so is to extend the design science paradigm by developing a set of design principles applicable to the context of virtual environments, particularly those using agent-based simulation as their underlying technology. Our central argument is that virtual worlds comprise a new class of information system, one that combines the structural aspects of traditional modeling and simulation systems in concert with emergent user dynamics of systems supporting emergent knowledge processes. Our approach involves two components. First, we review the characteristics of agent-based virtual worlds (ABVWs) to discern design requirements that may challenge current design theory. From this review, we derive a set of design principles based on deep versus emergent structures where deep structures reflect conventional modeling and simulation system architectures and emergent structures capture the unpredictable user–system dynamics inherent in emergent knowledge processes, which increasingly characterize virtual worlds. We illustrate how these design challenges are addressed with an exemplar of a complex mirror world, a large-scale ABVW we developed called Sentient World. Our contribution is the insight of partitioning ABVW architectures into deep and emergent structures that mirror modeling systems and emergent knowledge processes respectively, while developing extended design principles to facilitate their integration. We conclude with a discussion of the implications of our design principles for informing and guiding future research and practice.",[],"Dolk, Daniel R.",N/A,"Naval Postgraduate School, 589 Dyer Road, Mailcode 06/IS, Monterey, CA 93943 U.S.A."
https://misq.umn.edu/misq/article/35/3/673/522/Design-Principles-for-Virtual-Worlds1,MIS Quarterly,Design Principles for Virtual Worlds1,"Volume 35, Issue 3",September 2011,"In this research note, we examine the design, development, validation, and use of virtual worlds. Our purpose in doing so is to extend the design science paradigm by developing a set of design principles applicable to the context of virtual environments, particularly those using agent-based simulation as their underlying technology. Our central argument is that virtual worlds comprise a new class of information system, one that combines the structural aspects of traditional modeling and simulation systems in concert with emergent user dynamics of systems supporting emergent knowledge processes. Our approach involves two components. First, we review the characteristics of agent-based virtual worlds (ABVWs) to discern design requirements that may challenge current design theory. From this review, we derive a set of design principles based on deep versus emergent structures where deep structures reflect conventional modeling and simulation system architectures and emergent structures capture the unpredictable user–system dynamics inherent in emergent knowledge processes, which increasingly characterize virtual worlds. We illustrate how these design challenges are addressed with an exemplar of a complex mirror world, a large-scale ABVW we developed called Sentient World. Our contribution is the insight of partitioning ABVW architectures into deep and emergent structures that mirror modeling systems and emergent knowledge processes respectively, while developing extended design principles to facilitate their integration. We conclude with a discussion of the implications of our design principles for informing and guiding future research and practice.",[],"Drnevich, Paul L.",N/A,"Culverhouse College of Commerce, University of Alabama, 361 Stadium Drive, Tuscaloosa, AL 35487 U.S.A."
https://misq.umn.edu/misq/article/35/3/685/524/Arguing-the-Value-of-Virtual-Worlds-Patterns-of,MIS Quarterly,Arguing the Value of Virtual Worlds: Patterns of Discursive Sensemaking of an Innovative Technology1,"Volume 35, Issue 3",September 2011,"With the rapid pace of technological development, individuals are frequently challenged to make sense of equivocal innovative technology while being given limited information. Virtual worlds are a prime example of such an equivocal innovative technology, and this affords researchers an opportunity to study sensemaking and the construction of perspectives about the organizational value of virtual worlds. This study reports on an analysis of the written assessments of 59 business professionals who spent an extended period of time in Second Life, a popular virtual world, and discursively made sense of the organizational value of virtual worlds. Through a Toulminian analysis of the claims, grounds, and warrants used in the texts they generated, we identify 12 common patterns of sensemaking and indicate that themes of confirmation, open-ended rhetoric, demographics, and control are evident in the different types of claims that were addressed. Further, we assert that the Toulminian approach we employ is a useful methodology for the study of sensemaking and one that is not bound to any particular theoretical perspective.",[],"Berente, Nicholas",N/A,"Terry College of Business, University of Georgia, 312 Brooks Hall, Athens, GA 30602 U.S.A."
https://misq.umn.edu/misq/article/35/3/685/524/Arguing-the-Value-of-Virtual-Worlds-Patterns-of,MIS Quarterly,Arguing the Value of Virtual Worlds: Patterns of Discursive Sensemaking of an Innovative Technology1,"Volume 35, Issue 3",September 2011,"With the rapid pace of technological development, individuals are frequently challenged to make sense of equivocal innovative technology while being given limited information. Virtual worlds are a prime example of such an equivocal innovative technology, and this affords researchers an opportunity to study sensemaking and the construction of perspectives about the organizational value of virtual worlds. This study reports on an analysis of the written assessments of 59 business professionals who spent an extended period of time in Second Life, a popular virtual world, and discursively made sense of the organizational value of virtual worlds. Through a Toulminian analysis of the claims, grounds, and warrants used in the texts they generated, we identify 12 common patterns of sensemaking and indicate that themes of confirmation, open-ended rhetoric, demographics, and control are evident in the different types of claims that were addressed. Further, we assert that the Toulminian approach we employ is a useful methodology for the study of sensemaking and one that is not bound to any particular theoretical perspective.",[],"Hansen, Sean",N/A,"Saunders College of Business, Rochester Institute of Technology, 105 Lomb Memorial Drive, Rochester, NY 14623-5608 U.S.A."
https://misq.umn.edu/misq/article/35/3/685/524/Arguing-the-Value-of-Virtual-Worlds-Patterns-of,MIS Quarterly,Arguing the Value of Virtual Worlds: Patterns of Discursive Sensemaking of an Innovative Technology1,"Volume 35, Issue 3",September 2011,"With the rapid pace of technological development, individuals are frequently challenged to make sense of equivocal innovative technology while being given limited information. Virtual worlds are a prime example of such an equivocal innovative technology, and this affords researchers an opportunity to study sensemaking and the construction of perspectives about the organizational value of virtual worlds. This study reports on an analysis of the written assessments of 59 business professionals who spent an extended period of time in Second Life, a popular virtual world, and discursively made sense of the organizational value of virtual worlds. Through a Toulminian analysis of the claims, grounds, and warrants used in the texts they generated, we identify 12 common patterns of sensemaking and indicate that themes of confirmation, open-ended rhetoric, demographics, and control are evident in the different types of claims that were addressed. Further, we assert that the Toulminian approach we employ is a useful methodology for the study of sensemaking and one that is not bound to any particular theoretical perspective.",[],"Pike, Jacqueline C.",N/A,"Palumbo-Donahue School of Business, Duquesne University, 600 Forbes Avenue, Pittsburgh, PA 15282 U.S.A."
https://misq.umn.edu/misq/article/35/3/685/524/Arguing-the-Value-of-Virtual-Worlds-Patterns-of,MIS Quarterly,Arguing the Value of Virtual Worlds: Patterns of Discursive Sensemaking of an Innovative Technology1,"Volume 35, Issue 3",September 2011,"With the rapid pace of technological development, individuals are frequently challenged to make sense of equivocal innovative technology while being given limited information. Virtual worlds are a prime example of such an equivocal innovative technology, and this affords researchers an opportunity to study sensemaking and the construction of perspectives about the organizational value of virtual worlds. This study reports on an analysis of the written assessments of 59 business professionals who spent an extended period of time in Second Life, a popular virtual world, and discursively made sense of the organizational value of virtual worlds. Through a Toulminian analysis of the claims, grounds, and warrants used in the texts they generated, we identify 12 common patterns of sensemaking and indicate that themes of confirmation, open-ended rhetoric, demographics, and control are evident in the different types of claims that were addressed. Further, we assert that the Toulminian approach we employ is a useful methodology for the study of sensemaking and one that is not bound to any particular theoretical perspective.",[],"Bateman, Patrick J.",N/A,"Williamson College of Business Administration, Youngstown State University, One University Plaza, Youngstown, OH 44515 U.S.A."
https://misq.umn.edu/misq/article/35/3/711/2600/What-if-Your-Avatar-Looks-Like-You-Dual-Congruity,MIS Quarterly,What if Your Avatar Looks Like You? Dual-Congruity Perspectives for Avatar Use1,"Volume 35, Issue 3",September 2011,"As broadband Internet access and virtual reality technology rapidly expand, virtual worlds and three-dimensional avatars will become more pervasive and widely adopted. In virtual worlds, people assume an identity as an avatar and interact with each other. The objective of this study is to theorize how users form attitudes and intentions regarding avatars in realistic, task-focused virtual world settings. To investigate these effects, this study proposes a conceptual framework based on dual-congruity perspectives (self-congruity and functional congruity). The results show that the more closely an avatar resembles its user, the more the user is likely to have positive attitudes (e.g., affection, connection, and passion) toward the avatar, and the better able to evaluate the quality and performance of apparel products. In the end, these positive attitudes toward an avatar and its usefulness positively affect users’ intentions to use the avatar. Based on this study, we propose that avatars representing users’ actual appearance may be helpful in experiencing and evaluating some business areas related to users’ lives in the real world (e.g., virtual apparel shopping, matchmaking, plastic surgery, fitness clubs, etc.); utilization of such avatars may be a new business opportunity likely to thrive in virtual worlds.",[],"Suh, Kil-Soo",N/A,"School of Business, Yonsei Univesrity, 262 Seongsan-no, Seodaemun-gu, Seoul 120-749 Korea"
https://misq.umn.edu/misq/article/35/3/711/2600/What-if-Your-Avatar-Looks-Like-You-Dual-Congruity,MIS Quarterly,What if Your Avatar Looks Like You? Dual-Congruity Perspectives for Avatar Use1,"Volume 35, Issue 3",September 2011,"As broadband Internet access and virtual reality technology rapidly expand, virtual worlds and three-dimensional avatars will become more pervasive and widely adopted. In virtual worlds, people assume an identity as an avatar and interact with each other. The objective of this study is to theorize how users form attitudes and intentions regarding avatars in realistic, task-focused virtual world settings. To investigate these effects, this study proposes a conceptual framework based on dual-congruity perspectives (self-congruity and functional congruity). The results show that the more closely an avatar resembles its user, the more the user is likely to have positive attitudes (e.g., affection, connection, and passion) toward the avatar, and the better able to evaluate the quality and performance of apparel products. In the end, these positive attitudes toward an avatar and its usefulness positively affect users’ intentions to use the avatar. Based on this study, we propose that avatars representing users’ actual appearance may be helpful in experiencing and evaluating some business areas related to users’ lives in the real world (e.g., virtual apparel shopping, matchmaking, plastic surgery, fitness clubs, etc.); utilization of such avatars may be a new business opportunity likely to thrive in virtual worlds.",[],"Kim, Hongki",N/A,"School of Business, Yonsei Univesrity, 262 Seongsan-no, Seodaemun-gu, Seoul 120-749 Korea"
https://misq.umn.edu/misq/article/35/3/711/2600/What-if-Your-Avatar-Looks-Like-You-Dual-Congruity,MIS Quarterly,What if Your Avatar Looks Like You? Dual-Congruity Perspectives for Avatar Use1,"Volume 35, Issue 3",September 2011,"As broadband Internet access and virtual reality technology rapidly expand, virtual worlds and three-dimensional avatars will become more pervasive and widely adopted. In virtual worlds, people assume an identity as an avatar and interact with each other. The objective of this study is to theorize how users form attitudes and intentions regarding avatars in realistic, task-focused virtual world settings. To investigate these effects, this study proposes a conceptual framework based on dual-congruity perspectives (self-congruity and functional congruity). The results show that the more closely an avatar resembles its user, the more the user is likely to have positive attitudes (e.g., affection, connection, and passion) toward the avatar, and the better able to evaluate the quality and performance of apparel products. In the end, these positive attitudes toward an avatar and its usefulness positively affect users’ intentions to use the avatar. Based on this study, we propose that avatars representing users’ actual appearance may be helpful in experiencing and evaluating some business areas related to users’ lives in the real world (e.g., virtual apparel shopping, matchmaking, plastic surgery, fitness clubs, etc.); utilization of such avatars may be a new business opportunity likely to thrive in virtual worlds.",[],"Suh, Eung Kyo",N/A,"School of Business, Yonsei Univesrity, 262 Seongsan-no, Seodaemun-gu, Seoul 120-749 Korea"
https://misq.umn.edu/misq/article/35/3/731/2597/Enhancing-Brand-Equity-through-Flow-and,MIS Quarterly,Enhancing Brand Equity through Flow and Telepresence: A Comparison of 2d and 3d Virtual Worlds1,"Volume 35, Issue 3",September 2011,"This research uses theories of flow, telepresence, positive emotions, and brand equity to examine the effect of using two-dimensional versus three-dimensional virtual world environments on telepresence, enjoyment, brand equity, and behavioral intention. The findings suggest that the 3D virtual world environment produces both positive and negative effects on brand equity when compared to the 2D environment. The positive effect of the 3D virtual world environment on brand equity occurs through telepresence, a specific aspect of flow, as well as enjoyment. The negative effect on brand equity can be explained using distraction–conflict theory in which attentional conflicts faced by users of a highly interactive and rich medium resulted in distractions from attending to the brand. Brand equity, in turn, has a positive effect on behavioral intention. The results suggest that although the 3D virtual world environment has the potential to increase brand equity by offering an immersive and enjoyable virtual product experience, the rich environment can also be a distraction. Therefore, developers of virtual world branding sites need to take into account limitations in the information processing capacity and attention span of users when designing their sites in order to avoid cognitive overload, which can lead to users being distracted from branding information. This paper not only provides a theoretical foundation for explaining users’ experience with 2D versus 3D virtual world branding sites, but also provides insights to practitioners for designing 3D virtual world sites to enhance brand equity and intentions through user engagement.",[],"Nah, Fiona Fui-Hoon",N/A,"College of Business Administration, University of Nebraska–Lincoln, Lincoln, NE 68588 U.S.A."
https://misq.umn.edu/misq/article/35/3/731/2597/Enhancing-Brand-Equity-through-Flow-and,MIS Quarterly,Enhancing Brand Equity through Flow and Telepresence: A Comparison of 2d and 3d Virtual Worlds1,"Volume 35, Issue 3",September 2011,"This research uses theories of flow, telepresence, positive emotions, and brand equity to examine the effect of using two-dimensional versus three-dimensional virtual world environments on telepresence, enjoyment, brand equity, and behavioral intention. The findings suggest that the 3D virtual world environment produces both positive and negative effects on brand equity when compared to the 2D environment. The positive effect of the 3D virtual world environment on brand equity occurs through telepresence, a specific aspect of flow, as well as enjoyment. The negative effect on brand equity can be explained using distraction–conflict theory in which attentional conflicts faced by users of a highly interactive and rich medium resulted in distractions from attending to the brand. Brand equity, in turn, has a positive effect on behavioral intention. The results suggest that although the 3D virtual world environment has the potential to increase brand equity by offering an immersive and enjoyable virtual product experience, the rich environment can also be a distraction. Therefore, developers of virtual world branding sites need to take into account limitations in the information processing capacity and attention span of users when designing their sites in order to avoid cognitive overload, which can lead to users being distracted from branding information. This paper not only provides a theoretical foundation for explaining users’ experience with 2D versus 3D virtual world branding sites, but also provides insights to practitioners for designing 3D virtual world sites to enhance brand equity and intentions through user engagement.",[],"Eschenbrenner, Brenda",N/A,"College of Business and Technology, University of Nebraska at Kearney, Kearney, NE 68849 U.S.A."
https://misq.umn.edu/misq/article/35/3/731/2597/Enhancing-Brand-Equity-through-Flow-and,MIS Quarterly,Enhancing Brand Equity through Flow and Telepresence: A Comparison of 2d and 3d Virtual Worlds1,"Volume 35, Issue 3",September 2011,"This research uses theories of flow, telepresence, positive emotions, and brand equity to examine the effect of using two-dimensional versus three-dimensional virtual world environments on telepresence, enjoyment, brand equity, and behavioral intention. The findings suggest that the 3D virtual world environment produces both positive and negative effects on brand equity when compared to the 2D environment. The positive effect of the 3D virtual world environment on brand equity occurs through telepresence, a specific aspect of flow, as well as enjoyment. The negative effect on brand equity can be explained using distraction–conflict theory in which attentional conflicts faced by users of a highly interactive and rich medium resulted in distractions from attending to the brand. Brand equity, in turn, has a positive effect on behavioral intention. The results suggest that although the 3D virtual world environment has the potential to increase brand equity by offering an immersive and enjoyable virtual product experience, the rich environment can also be a distraction. Therefore, developers of virtual world branding sites need to take into account limitations in the information processing capacity and attention span of users when designing their sites in order to avoid cognitive overload, which can lead to users being distracted from branding information. This paper not only provides a theoretical foundation for explaining users’ experience with 2D versus 3D virtual world branding sites, but also provides insights to practitioners for designing 3D virtual world sites to enhance brand equity and intentions through user engagement.",[],"DeWester, David",N/A,"College of Arts and Sciences, University of Nebraska–Lincoln, Lincoln, NE 68588 U.S.A."
https://misq.umn.edu/misq/article/35/3/749/2599/From-Space-to-Place-Predicting-Users-Intentions-to,MIS Quarterly,From Space to Place: Predicting Users’ Intentions to Return to Virtual Worlds1,"Volume 35, Issue 3",September 2011,"Virtual worlds have received considerable attention as platforms for entertainment, education, and commerce. But organizations are experiencing failures in their early attempts to lure customers, employees, or partners into these worlds. Among the more grievous problems is the inability to attract users back into a virtual environment. In this study, we propose and test a model to predict users’ intentions to return to a virtual world. Our model is based on the idea that users intend to return to a virtual world having conceived of it as a “place” in which they have had meaningful experiences. We rely on the interactionist theory of place attachment to explain the links among the constructs of our model. Our model is tested via a lab experiment. We find that users’ intentions to return to a virtual world is determined by a state of deep involvement (termed cognitive absorption) that users experience as they perform an activity and tend to lose track of time. In turn, cognitive absorption is determined by users’ awareness of whom they interact with and how they interact within a virtual world, what they interact about, and where, in a virtual sense, such interaction occurs. Our work contributes to theory in the following ways: it identifies state predictors of cognitive absorption, it conceives of virtual worlds in such a way as to account for users’ experiences through the notion of place, and it explains how the properties of a virtual world contribute to users’ awareness.",[],"Goel, Lakshmi",N/A,"Coggin College of Business, University of North Florida, 1UNF Drive, Jacksonville, FL 32224 U.S.A."
https://misq.umn.edu/misq/article/35/3/749/2599/From-Space-to-Place-Predicting-Users-Intentions-to,MIS Quarterly,From Space to Place: Predicting Users’ Intentions to Return to Virtual Worlds1,"Volume 35, Issue 3",September 2011,"Virtual worlds have received considerable attention as platforms for entertainment, education, and commerce. But organizations are experiencing failures in their early attempts to lure customers, employees, or partners into these worlds. Among the more grievous problems is the inability to attract users back into a virtual environment. In this study, we propose and test a model to predict users’ intentions to return to a virtual world. Our model is based on the idea that users intend to return to a virtual world having conceived of it as a “place” in which they have had meaningful experiences. We rely on the interactionist theory of place attachment to explain the links among the constructs of our model. Our model is tested via a lab experiment. We find that users’ intentions to return to a virtual world is determined by a state of deep involvement (termed cognitive absorption) that users experience as they perform an activity and tend to lose track of time. In turn, cognitive absorption is determined by users’ awareness of whom they interact with and how they interact within a virtual world, what they interact about, and where, in a virtual sense, such interaction occurs. Our work contributes to theory in the following ways: it identifies state predictors of cognitive absorption, it conceives of virtual worlds in such a way as to account for users’ experiences through the notion of place, and it explains how the properties of a virtual world contribute to users’ awareness.",[],"Johnson, Norman A.",N/A,"C. T. Bauer College of Business, University of Houston, 4800 Calhoun Road, Houston, TX 77004 U.S.A."
https://misq.umn.edu/misq/article/35/3/749/2599/From-Space-to-Place-Predicting-Users-Intentions-to,MIS Quarterly,From Space to Place: Predicting Users’ Intentions to Return to Virtual Worlds1,"Volume 35, Issue 3",September 2011,"Virtual worlds have received considerable attention as platforms for entertainment, education, and commerce. But organizations are experiencing failures in their early attempts to lure customers, employees, or partners into these worlds. Among the more grievous problems is the inability to attract users back into a virtual environment. In this study, we propose and test a model to predict users’ intentions to return to a virtual world. Our model is based on the idea that users intend to return to a virtual world having conceived of it as a “place” in which they have had meaningful experiences. We rely on the interactionist theory of place attachment to explain the links among the constructs of our model. Our model is tested via a lab experiment. We find that users’ intentions to return to a virtual world is determined by a state of deep involvement (termed cognitive absorption) that users experience as they perform an activity and tend to lose track of time. In turn, cognitive absorption is determined by users’ awareness of whom they interact with and how they interact within a virtual world, what they interact about, and where, in a virtual sense, such interaction occurs. Our work contributes to theory in the following ways: it identifies state predictors of cognitive absorption, it conceives of virtual worlds in such a way as to account for users’ experiences through the notion of place, and it explains how the properties of a virtual world contribute to users’ awareness.",[],"Junglas, Iris",N/A,"Accenture Institute for High Performance, Prudential Tower, 800 Boylston Street, Suite 2300, Boston, MA 02199 U.S.A."
https://misq.umn.edu/misq/article/35/3/749/2599/From-Space-to-Place-Predicting-Users-Intentions-to,MIS Quarterly,From Space to Place: Predicting Users’ Intentions to Return to Virtual Worlds1,"Volume 35, Issue 3",September 2011,"Virtual worlds have received considerable attention as platforms for entertainment, education, and commerce. But organizations are experiencing failures in their early attempts to lure customers, employees, or partners into these worlds. Among the more grievous problems is the inability to attract users back into a virtual environment. In this study, we propose and test a model to predict users’ intentions to return to a virtual world. Our model is based on the idea that users intend to return to a virtual world having conceived of it as a “place” in which they have had meaningful experiences. We rely on the interactionist theory of place attachment to explain the links among the constructs of our model. Our model is tested via a lab experiment. We find that users’ intentions to return to a virtual world is determined by a state of deep involvement (termed cognitive absorption) that users experience as they perform an activity and tend to lose track of time. In turn, cognitive absorption is determined by users’ awareness of whom they interact with and how they interact within a virtual world, what they interact about, and where, in a virtual sense, such interaction occurs. Our work contributes to theory in the following ways: it identifies state predictors of cognitive absorption, it conceives of virtual worlds in such a way as to account for users’ experiences through the notion of place, and it explains how the properties of a virtual world contribute to users’ awareness.",[],"Ives, Blake",N/A,"C. T. Bauer College of Business, University of Houston, 4800 Calhoun Road, Houston, TX 77004 U.S.A."
https://misq.umn.edu/misq/article/35/3/773/523/Co-Creation-in-Virtual-Worlds-The-Design-of-the,MIS Quarterly,Co-Creation in Virtual Worlds: The Design of the User Experience1,"Volume 35, Issue 3",September 2011,"Emerging virtual worlds, such as the prominent Second Life, offer unprecedented opportunities for companies to collaborate with co-creating users. However, pioneering corporate co-creation systems fail to attract a satisfying level of participation and engagement. The experience users have with the co-creation system is the key to making virtual places a vibrant source of great connections, creativity, and co-creation. While prior research on co-creation serves as a foundation for this work, it does not provide adequate guidance on how to design co-creation systems in virtual worlds. To address this shortcoming, a 20-month action research project was conducted to study the user’s experience and to identify design principles for virtual co-creation systems. In two action research cycles, a virtual co-creation system called Ideation Quest was created, deployed, evaluated, and improved. The study reveals how to design co-creation systems and enriches research on co-creation to fit the virtual world context. Practitioners receive a helpful framework to leverage virtual worlds for co-creation.",[],"Kohler, Thomas",N/A,"Department of Management and Marketing, College of Business Administration, Hawaii Pacific University, Honolulu, HI 96813 U.S.A."
https://misq.umn.edu/misq/article/35/3/773/523/Co-Creation-in-Virtual-Worlds-The-Design-of-the,MIS Quarterly,Co-Creation in Virtual Worlds: The Design of the User Experience1,"Volume 35, Issue 3",September 2011,"Emerging virtual worlds, such as the prominent Second Life, offer unprecedented opportunities for companies to collaborate with co-creating users. However, pioneering corporate co-creation systems fail to attract a satisfying level of participation and engagement. The experience users have with the co-creation system is the key to making virtual places a vibrant source of great connections, creativity, and co-creation. While prior research on co-creation serves as a foundation for this work, it does not provide adequate guidance on how to design co-creation systems in virtual worlds. To address this shortcoming, a 20-month action research project was conducted to study the user’s experience and to identify design principles for virtual co-creation systems. In two action research cycles, a virtual co-creation system called Ideation Quest was created, deployed, evaluated, and improved. The study reveals how to design co-creation systems and enriches research on co-creation to fit the virtual world context. Practitioners receive a helpful framework to leverage virtual worlds for co-creation.",[],"Fueller, Johann",N/A,"Department of Strategic Management, Marketing, and Tourism, Innsbruck University School of Management, 6020 Innsbruck, Austria"
https://misq.umn.edu/misq/article/35/3/773/523/Co-Creation-in-Virtual-Worlds-The-Design-of-the,MIS Quarterly,Co-Creation in Virtual Worlds: The Design of the User Experience1,"Volume 35, Issue 3",September 2011,"Emerging virtual worlds, such as the prominent Second Life, offer unprecedented opportunities for companies to collaborate with co-creating users. However, pioneering corporate co-creation systems fail to attract a satisfying level of participation and engagement. The experience users have with the co-creation system is the key to making virtual places a vibrant source of great connections, creativity, and co-creation. While prior research on co-creation serves as a foundation for this work, it does not provide adequate guidance on how to design co-creation systems in virtual worlds. To address this shortcoming, a 20-month action research project was conducted to study the user’s experience and to identify design principles for virtual co-creation systems. In two action research cycles, a virtual co-creation system called Ideation Quest was created, deployed, evaluated, and improved. The study reveals how to design co-creation systems and enriches research on co-creation to fit the virtual world context. Practitioners receive a helpful framework to leverage virtual worlds for co-creation.",[],"Matzler, Kurt",N/A,"Department of Strategic Management, Marketing, and Tourism, Innsbruck University School of Management, 6020 Innsbruck, Austria"
https://misq.umn.edu/misq/article/35/3/773/523/Co-Creation-in-Virtual-Worlds-The-Design-of-the,MIS Quarterly,Co-Creation in Virtual Worlds: The Design of the User Experience1,"Volume 35, Issue 3",September 2011,"Emerging virtual worlds, such as the prominent Second Life, offer unprecedented opportunities for companies to collaborate with co-creating users. However, pioneering corporate co-creation systems fail to attract a satisfying level of participation and engagement. The experience users have with the co-creation system is the key to making virtual places a vibrant source of great connections, creativity, and co-creation. While prior research on co-creation serves as a foundation for this work, it does not provide adequate guidance on how to design co-creation systems in virtual worlds. To address this shortcoming, a 20-month action research project was conducted to study the user’s experience and to identify design principles for virtual co-creation systems. In two action research cycles, a virtual co-creation system called Ideation Quest was created, deployed, evaluated, and improved. The study reveals how to design co-creation systems and enriches research on co-creation to fit the virtual world context. Practitioners receive a helpful framework to leverage virtual worlds for co-creation.",[],"Stieger, Daniel",N/A,"Department of Strategic Management, Marketing, and Tourism, Innsbruck University School of Management, 6020 Innsbruck, Austria"
https://misq.umn.edu/misq/article/35/3/789/2594/An-Odyssey-into-Virtual-Worlds-Exploring-the,MIS Quarterly,An Odyssey into Virtual Worlds: Exploring the Impacts of Technological and Spatial Environments on Intention to Purchase Virtual Products1,"Volume 35, Issue 3",September 2011,"Although research on three-dimensional virtual environments abounds, little is known about the social and business aspects of virtual worlds. Given the emergence of large-scale social virtual worlds, such as Second Life, and the dramatic growth in sales of virtual goods, it is important to understand the dynamics that govern the purchase of virtual goods in virtual worlds. Employing the stimulus–organism–response (S-O-R) framework, we investigate how technological (interactivity and sociability) and spatial (density and stability) environments in virtual worlds influence the participants’ virtual experiences (telepresence, social presence, and flow), and how experiences subsequently affect their response (intention to purchase virtual goods). The results of our survey of 354 Second Life residents indicate that interactivity, which enhances the interaction with objects, has a significant positive impact on telepresence and flow. Also, sociability, which fosters interactions with participants, is significantly associated with social presence, although no such significant impact was observed on flow. Furthermore, both density and stability are found to significantly influence participants’ virtual experiences; stability helps users to develop strong social bonds, thereby increasing both social presence and flow. However, contrary to our prediction of curvilinear patterns, density is linearly associated with flow and social presence. Interestingly, the results exhibit two opposing effects of density: while it reduces the extent of flow, density increases the amount of social presence. Since social presence is found to increase flow, the net impact of density on flow depends heavily on the relative strength of the associations involving these three constructs. Finally, we find that flow mediates the impacts of technological and spatial environments on intention to purchase virtual products. We conclude the paper with a discussion of the theoretical and practical contributions of our findings.",[],"Animesh, Animesh",N/A,"Desautels Faculty of Management, McGill University, 1001 Sherbrooke Street West, Montréal, QC H3A 1G5 Canada"
https://misq.umn.edu/misq/article/35/3/789/2594/An-Odyssey-into-Virtual-Worlds-Exploring-the,MIS Quarterly,An Odyssey into Virtual Worlds: Exploring the Impacts of Technological and Spatial Environments on Intention to Purchase Virtual Products1,"Volume 35, Issue 3",September 2011,"Although research on three-dimensional virtual environments abounds, little is known about the social and business aspects of virtual worlds. Given the emergence of large-scale social virtual worlds, such as Second Life, and the dramatic growth in sales of virtual goods, it is important to understand the dynamics that govern the purchase of virtual goods in virtual worlds. Employing the stimulus–organism–response (S-O-R) framework, we investigate how technological (interactivity and sociability) and spatial (density and stability) environments in virtual worlds influence the participants’ virtual experiences (telepresence, social presence, and flow), and how experiences subsequently affect their response (intention to purchase virtual goods). The results of our survey of 354 Second Life residents indicate that interactivity, which enhances the interaction with objects, has a significant positive impact on telepresence and flow. Also, sociability, which fosters interactions with participants, is significantly associated with social presence, although no such significant impact was observed on flow. Furthermore, both density and stability are found to significantly influence participants’ virtual experiences; stability helps users to develop strong social bonds, thereby increasing both social presence and flow. However, contrary to our prediction of curvilinear patterns, density is linearly associated with flow and social presence. Interestingly, the results exhibit two opposing effects of density: while it reduces the extent of flow, density increases the amount of social presence. Since social presence is found to increase flow, the net impact of density on flow depends heavily on the relative strength of the associations involving these three constructs. Finally, we find that flow mediates the impacts of technological and spatial environments on intention to purchase virtual products. We conclude the paper with a discussion of the theoretical and practical contributions of our findings.",[],"Pinsonneault, Alain",N/A,"Desautels Faculty of Management, McGill University, 1001 Sherbrooke Street West, Montréal, QC H3A 1G5 Canada"
https://misq.umn.edu/misq/article/35/3/789/2594/An-Odyssey-into-Virtual-Worlds-Exploring-the,MIS Quarterly,An Odyssey into Virtual Worlds: Exploring the Impacts of Technological and Spatial Environments on Intention to Purchase Virtual Products1,"Volume 35, Issue 3",September 2011,"Although research on three-dimensional virtual environments abounds, little is known about the social and business aspects of virtual worlds. Given the emergence of large-scale social virtual worlds, such as Second Life, and the dramatic growth in sales of virtual goods, it is important to understand the dynamics that govern the purchase of virtual goods in virtual worlds. Employing the stimulus–organism–response (S-O-R) framework, we investigate how technological (interactivity and sociability) and spatial (density and stability) environments in virtual worlds influence the participants’ virtual experiences (telepresence, social presence, and flow), and how experiences subsequently affect their response (intention to purchase virtual goods). The results of our survey of 354 Second Life residents indicate that interactivity, which enhances the interaction with objects, has a significant positive impact on telepresence and flow. Also, sociability, which fosters interactions with participants, is significantly associated with social presence, although no such significant impact was observed on flow. Furthermore, both density and stability are found to significantly influence participants’ virtual experiences; stability helps users to develop strong social bonds, thereby increasing both social presence and flow. However, contrary to our prediction of curvilinear patterns, density is linearly associated with flow and social presence. Interestingly, the results exhibit two opposing effects of density: while it reduces the extent of flow, density increases the amount of social presence. Since social presence is found to increase flow, the net impact of density on flow depends heavily on the relative strength of the associations involving these three constructs. Finally, we find that flow mediates the impacts of technological and spatial environments on intention to purchase virtual products. We conclude the paper with a discussion of the theoretical and practical contributions of our findings.",[],"Yang, Sung-Byung",N/A,"School of Business Administration, Hansung University, 389 Samseon-dong 2-ga, Seongbuk-gu, Seoul 136-792 Korea"
https://misq.umn.edu/misq/article/35/3/789/2594/An-Odyssey-into-Virtual-Worlds-Exploring-the,MIS Quarterly,An Odyssey into Virtual Worlds: Exploring the Impacts of Technological and Spatial Environments on Intention to Purchase Virtual Products1,"Volume 35, Issue 3",September 2011,"Although research on three-dimensional virtual environments abounds, little is known about the social and business aspects of virtual worlds. Given the emergence of large-scale social virtual worlds, such as Second Life, and the dramatic growth in sales of virtual goods, it is important to understand the dynamics that govern the purchase of virtual goods in virtual worlds. Employing the stimulus–organism–response (S-O-R) framework, we investigate how technological (interactivity and sociability) and spatial (density and stability) environments in virtual worlds influence the participants’ virtual experiences (telepresence, social presence, and flow), and how experiences subsequently affect their response (intention to purchase virtual goods). The results of our survey of 354 Second Life residents indicate that interactivity, which enhances the interaction with objects, has a significant positive impact on telepresence and flow. Also, sociability, which fosters interactions with participants, is significantly associated with social presence, although no such significant impact was observed on flow. Furthermore, both density and stability are found to significantly influence participants’ virtual experiences; stability helps users to develop strong social bonds, thereby increasing both social presence and flow. However, contrary to our prediction of curvilinear patterns, density is linearly associated with flow and social presence. Interestingly, the results exhibit two opposing effects of density: while it reduces the extent of flow, density increases the amount of social presence. Since social presence is found to increase flow, the net impact of density on flow depends heavily on the relative strength of the associations involving these three constructs. Finally, we find that flow mediates the impacts of technological and spatial environments on intention to purchase virtual products. We conclude the paper with a discussion of the theoretical and practical contributions of our findings.",[],"Oh, Wonseok",N/A,"School of Business, Yonsei University, 262, Seongsanno, Seodaemun-gu, Seoul 120-749 Korea"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/35/4/813/2605/Network-Effects-The-Influence-of-Structural,MIS Quarterly,Network Effects: The Influence of Structural Capital on Open Source Project Success1,"Volume 35, Issue 4",December 2011,"What determines the success of open source projects? In this study, we investigate the impact of network social capital on open source project success. We define network social capital as the benefits open source developers secure from their membership in developer collaboration networks. We focus on one specific type of success as measured by the rate of knowledge creation in an open source project. Specific hypotheses are developed and tested using a longitudinal panel of 2,378 projects hosted at SourceForge. We find that network social capital is not equally accessible to or appropriated by all projects. Our main results are as follows. First, projects with greater internal cohesion (that is, cohesion among the project members) are more successful. Second, external cohesion (that is, cohesion among the external contacts of a project) has an inverse U-shaped relationship with the project’s success; moderate levels of external cohesion are best for a project’s success rather than very low or very high levels. Third, the technological diversity of the external network of a project also has the greatest benefit when it is neither too low nor too high. Fourth, the number of direct and indirect external contacts positively affects a project’s success such that the effect of the number of direct contacts is moderated by the number of indirect contacts. These results are robust to several control variables and alternate model specifications. Several theoretical and managerial implications are provided.",[],"Singh, Param Vir",N/A,"David A. Tepper School of Business, Carnegie Mellon University, Pittsburgh, PA 15213 U.S.A."
https://misq.umn.edu/misq/article/35/4/813/2605/Network-Effects-The-Influence-of-Structural,MIS Quarterly,Network Effects: The Influence of Structural Capital on Open Source Project Success1,"Volume 35, Issue 4",December 2011,"What determines the success of open source projects? In this study, we investigate the impact of network social capital on open source project success. We define network social capital as the benefits open source developers secure from their membership in developer collaboration networks. We focus on one specific type of success as measured by the rate of knowledge creation in an open source project. Specific hypotheses are developed and tested using a longitudinal panel of 2,378 projects hosted at SourceForge. We find that network social capital is not equally accessible to or appropriated by all projects. Our main results are as follows. First, projects with greater internal cohesion (that is, cohesion among the project members) are more successful. Second, external cohesion (that is, cohesion among the external contacts of a project) has an inverse U-shaped relationship with the project’s success; moderate levels of external cohesion are best for a project’s success rather than very low or very high levels. Third, the technological diversity of the external network of a project also has the greatest benefit when it is neither too low nor too high. Fourth, the number of direct and indirect external contacts positively affects a project’s success such that the effect of the number of direct contacts is moderated by the number of indirect contacts. These results are robust to several control variables and alternate model specifications. Several theoretical and managerial implications are provided.",[],"Tan, Yong",N/A,"Michael G. Foster School of Business, University of Washington, Seattle, WA 98195 U.S.A."
https://misq.umn.edu/misq/article/35/4/813/2605/Network-Effects-The-Influence-of-Structural,MIS Quarterly,Network Effects: The Influence of Structural Capital on Open Source Project Success1,"Volume 35, Issue 4",December 2011,"What determines the success of open source projects? In this study, we investigate the impact of network social capital on open source project success. We define network social capital as the benefits open source developers secure from their membership in developer collaboration networks. We focus on one specific type of success as measured by the rate of knowledge creation in an open source project. Specific hypotheses are developed and tested using a longitudinal panel of 2,378 projects hosted at SourceForge. We find that network social capital is not equally accessible to or appropriated by all projects. Our main results are as follows. First, projects with greater internal cohesion (that is, cohesion among the project members) are more successful. Second, external cohesion (that is, cohesion among the external contacts of a project) has an inverse U-shaped relationship with the project’s success; moderate levels of external cohesion are best for a project’s success rather than very low or very high levels. Third, the technological diversity of the external network of a project also has the greatest benefit when it is neither too low nor too high. Fourth, the number of direct and indirect external contacts positively affects a project’s success such that the effect of the number of direct contacts is moderated by the number of indirect contacts. These results are robust to several control variables and alternate model specifications. Several theoretical and managerial implications are provided.",[],"Mookerjee, Vijay",N/A,"School of Management, University of Texas at Dallas, Dallas, TX 52242 U.S.A."
https://misq.umn.edu/misq/article/35/4/831/2610/Technostress-Technological-Antecedents-and,MIS Quarterly,Technostress: Technological Antecedents and Implications1,"Volume 35, Issue 4",December 2011,"With the proliferation and ubiquity of information and communication technologies (ICTs), it is becoming imperative for individuals to constantly engage with these technologies in order to get work accomplished. Academic literature, popular press, and anecdotal evidence suggest that ICTs are responsible for increased stress levels in individuals (known as technostress). However, despite the influence of stress on health costs and productivity, it is not very clear which characteristics of ICTs create stress. We draw from IS and stress research to build and test a model of technostress. The person–environment fit model is used as a theoretical lens. The research model proposes that certain technology characteristics—like usability (usefulness, complexity, and reliability), intrusiveness (presenteeism, anonymity), and dynamism (pace of change)—are related to stressors (work overload, role ambiguity, invasion of privacy, work–home conflict, and job insecurity). Field data from 661 working professionals was obtained and analyzed. The results clearly suggest the prevalence of technostress and the hypotheses from the model are generally supported. Work overload and role ambiguity are found to be the two most dominant stressors, whereas intrusive technology characteristics are found to be the dominant predictors of stressors. The results open up new avenues for research by highlighting the incidence of technostress in organizations and possible interventions to alleviate it.",[],"Ayyagari, Ramakrishna",N/A,"University of Massachusetts, Boston, 100 Morrissey Boulevard, Boston, MA 02125-3393 U.S.A."
https://misq.umn.edu/misq/article/35/4/831/2610/Technostress-Technological-Antecedents-and,MIS Quarterly,Technostress: Technological Antecedents and Implications1,"Volume 35, Issue 4",December 2011,"With the proliferation and ubiquity of information and communication technologies (ICTs), it is becoming imperative for individuals to constantly engage with these technologies in order to get work accomplished. Academic literature, popular press, and anecdotal evidence suggest that ICTs are responsible for increased stress levels in individuals (known as technostress). However, despite the influence of stress on health costs and productivity, it is not very clear which characteristics of ICTs create stress. We draw from IS and stress research to build and test a model of technostress. The person–environment fit model is used as a theoretical lens. The research model proposes that certain technology characteristics—like usability (usefulness, complexity, and reliability), intrusiveness (presenteeism, anonymity), and dynamism (pace of change)—are related to stressors (work overload, role ambiguity, invasion of privacy, work–home conflict, and job insecurity). Field data from 661 working professionals was obtained and analyzed. The results clearly suggest the prevalence of technostress and the hypotheses from the model are generally supported. Work overload and role ambiguity are found to be the two most dominant stressors, whereas intrusive technology characteristics are found to be the dominant predictors of stressors. The results open up new avenues for research by highlighting the incidence of technostress in organizations and possible interventions to alleviate it.",[],"Grover, Varun",N/A,"Clemson University, 101 Sirrine Hall, Clemson, SC 29634-1305 U.S.A."
https://misq.umn.edu/misq/article/35/4/831/2610/Technostress-Technological-Antecedents-and,MIS Quarterly,Technostress: Technological Antecedents and Implications1,"Volume 35, Issue 4",December 2011,"With the proliferation and ubiquity of information and communication technologies (ICTs), it is becoming imperative for individuals to constantly engage with these technologies in order to get work accomplished. Academic literature, popular press, and anecdotal evidence suggest that ICTs are responsible for increased stress levels in individuals (known as technostress). However, despite the influence of stress on health costs and productivity, it is not very clear which characteristics of ICTs create stress. We draw from IS and stress research to build and test a model of technostress. The person–environment fit model is used as a theoretical lens. The research model proposes that certain technology characteristics—like usability (usefulness, complexity, and reliability), intrusiveness (presenteeism, anonymity), and dynamism (pace of change)—are related to stressors (work overload, role ambiguity, invasion of privacy, work–home conflict, and job insecurity). Field data from 661 working professionals was obtained and analyzed. The results clearly suggest the prevalence of technostress and the hypotheses from the model are generally supported. Work overload and role ambiguity are found to be the two most dominant stressors, whereas intrusive technology characteristics are found to be the dominant predictors of stressors. The results open up new avenues for research by highlighting the incidence of technostress in organizations and possible interventions to alleviate it.",[],"Purvis, Russell",N/A,"Clemson University, 101 Sirrine Hall, Clemson, SC 29634-1305 U.S.A."
https://misq.umn.edu/misq/article/35/4/859/2616/The-Value-of-It-Enabled-Retailer-Learning,MIS Quarterly,The Value of It-Enabled Retailer Learning: Personalized Product Recommendations and Customer Store Loyalty in Electronic Markets1,"Volume 35, Issue 4",December 2011,"Recent research has acknowledged the key role of information technology in helping build stronger and more enduring customer relationships. Personalized product recommendations (PPRs) adapted to individual customers’ preferences and tastes are one IT-enabled strategy that has been widely adopted by online retailers to enhance customers’ shopping experience. Although many online retailers have implemented PPRs on their electronic storefronts to improve customer retention, empirical evidence for the effects of PPRs on retention is sparse, and the limited anecdotal evidence is contradictory. We draw upon the household production function model in the consumer economics literature to develop a theoretical framework that explains the mechanisms through which PPRs influence customer store loyalty in electronic markets. We suggest that retailer learning that occurs as a result of customer knowledge obtained to enable personalization influences the efficiency of the online product brokering activity. Data collected from a two-phase lab experiment with 253 student subjects where the quality of PPRs was manipulated are used to empirically test the predictions of the theoretical model. Empirical analyses of the data indicate that retailer learning reflected in higher quality PPRs is associated with lower product screening cost, but higher product evaluation cost. We further find that higher quality PPRs are associated with greater value derived by consumers from the online product brokering activity in terms of higher decision making quality, which is positively associated with repurchase intention. The paper presents the implications, limitations, and contributions of this study along with areas for future research.",[],"Zhang, Tongxiao (Catherine)",N/A,"Gatton College of Business and Economics, University of Kentucky Lexington, KY 40506-0034 U.S.A."
https://misq.umn.edu/misq/article/35/4/859/2616/The-Value-of-It-Enabled-Retailer-Learning,MIS Quarterly,The Value of It-Enabled Retailer Learning: Personalized Product Recommendations and Customer Store Loyalty in Electronic Markets1,"Volume 35, Issue 4",December 2011,"Recent research has acknowledged the key role of information technology in helping build stronger and more enduring customer relationships. Personalized product recommendations (PPRs) adapted to individual customers’ preferences and tastes are one IT-enabled strategy that has been widely adopted by online retailers to enhance customers’ shopping experience. Although many online retailers have implemented PPRs on their electronic storefronts to improve customer retention, empirical evidence for the effects of PPRs on retention is sparse, and the limited anecdotal evidence is contradictory. We draw upon the household production function model in the consumer economics literature to develop a theoretical framework that explains the mechanisms through which PPRs influence customer store loyalty in electronic markets. We suggest that retailer learning that occurs as a result of customer knowledge obtained to enable personalization influences the efficiency of the online product brokering activity. Data collected from a two-phase lab experiment with 253 student subjects where the quality of PPRs was manipulated are used to empirically test the predictions of the theoretical model. Empirical analyses of the data indicate that retailer learning reflected in higher quality PPRs is associated with lower product screening cost, but higher product evaluation cost. We further find that higher quality PPRs are associated with greater value derived by consumers from the online product brokering activity in terms of higher decision making quality, which is positively associated with repurchase intention. The paper presents the implications, limitations, and contributions of this study along with areas for future research.",[],"Agarwal, Ritu",N/A,"Robert H. Smith School of Business, University of Maryland, College Park, MD 20742-1815 U.S.A."
https://misq.umn.edu/misq/article/35/4/859/2616/The-Value-of-It-Enabled-Retailer-Learning,MIS Quarterly,The Value of It-Enabled Retailer Learning: Personalized Product Recommendations and Customer Store Loyalty in Electronic Markets1,"Volume 35, Issue 4",December 2011,"Recent research has acknowledged the key role of information technology in helping build stronger and more enduring customer relationships. Personalized product recommendations (PPRs) adapted to individual customers’ preferences and tastes are one IT-enabled strategy that has been widely adopted by online retailers to enhance customers’ shopping experience. Although many online retailers have implemented PPRs on their electronic storefronts to improve customer retention, empirical evidence for the effects of PPRs on retention is sparse, and the limited anecdotal evidence is contradictory. We draw upon the household production function model in the consumer economics literature to develop a theoretical framework that explains the mechanisms through which PPRs influence customer store loyalty in electronic markets. We suggest that retailer learning that occurs as a result of customer knowledge obtained to enable personalization influences the efficiency of the online product brokering activity. Data collected from a two-phase lab experiment with 253 student subjects where the quality of PPRs was manipulated are used to empirically test the predictions of the theoretical model. Empirical analyses of the data indicate that retailer learning reflected in higher quality PPRs is associated with lower product screening cost, but higher product evaluation cost. We further find that higher quality PPRs are associated with greater value derived by consumers from the online product brokering activity in terms of higher decision making quality, which is positively associated with repurchase intention. The paper presents the implications, limitations, and contributions of this study along with areas for future research.",[],"Lucas, Henry C.",N/A,"Robert H. Smith School of Business, University of Maryland, College Park, MD 20742-1815 U.S.A."
https://misq.umn.edu/misq/article/35/4/883/2614/Guidelines-for-Designing-Visual-Ontologies-to,MIS Quarterly,Guidelines for Designing Visual Ontologies to Support Knowledge Identification1,"Volume 35, Issue 4",December 2011,"Organizations often provide workers with knowledge management systems to help them obtain knowledge they need. A significant constraint on the effectiveness of such systems is that they assume workers know what knowledge they need (they know what they don’t know) when, in fact, they often do not know what knowledge they need (they don’t know what they don’t know). A way to overcome this problem is to use visual ontologies to help users learn relevant concepts and relationships in the knowledge domain, enabling them to search the knowledge base in a more educated manner. However, no guidelines exist for designing such ontologies. To fill this gap, we draw on theories of philosophical ontology and cognition to propose guidelines for designing visual ontologies for knowledge identification.We conducted three experiments to compare the effectiveness of guided ontologies, visual ontologies that followed our guidelines, to unguided ontologies, visual ontologies that violated our guidelines. We found that subjects performed considerably better with the guided ontologies, and that subjects could perceive the benefits of using guided ontologies, at least in some circumstances. On the basis of these results, we conclude that the way visual ontologies are presented makes a difference in knowledge identification and that theories of philosophical ontology and cognition can guide the construction of more effective visual representations. Furthermore, we propose that the principles we used to create the guided visual ontologies can be generalized for other cases where visual models are used to inform users about application domains.",[],"Bera, Palash",N/A,"A. R. Sanchez, Jr. School of Business, Texas A&M International University, Laredo, TX 78041 U.S.A."
https://misq.umn.edu/misq/article/35/4/883/2614/Guidelines-for-Designing-Visual-Ontologies-to,MIS Quarterly,Guidelines for Designing Visual Ontologies to Support Knowledge Identification1,"Volume 35, Issue 4",December 2011,"Organizations often provide workers with knowledge management systems to help them obtain knowledge they need. A significant constraint on the effectiveness of such systems is that they assume workers know what knowledge they need (they know what they don’t know) when, in fact, they often do not know what knowledge they need (they don’t know what they don’t know). A way to overcome this problem is to use visual ontologies to help users learn relevant concepts and relationships in the knowledge domain, enabling them to search the knowledge base in a more educated manner. However, no guidelines exist for designing such ontologies. To fill this gap, we draw on theories of philosophical ontology and cognition to propose guidelines for designing visual ontologies for knowledge identification.We conducted three experiments to compare the effectiveness of guided ontologies, visual ontologies that followed our guidelines, to unguided ontologies, visual ontologies that violated our guidelines. We found that subjects performed considerably better with the guided ontologies, and that subjects could perceive the benefits of using guided ontologies, at least in some circumstances. On the basis of these results, we conclude that the way visual ontologies are presented makes a difference in knowledge identification and that theories of philosophical ontology and cognition can guide the construction of more effective visual representations. Furthermore, we propose that the principles we used to create the guided visual ontologies can be generalized for other cases where visual models are used to inform users about application domains.",[],"Burton-Jones, Andrew",N/A,"Sauder School of Business, The University of British Columbia, Vancouver, BC V6T 1Z2 Canada"
https://misq.umn.edu/misq/article/35/4/883/2614/Guidelines-for-Designing-Visual-Ontologies-to,MIS Quarterly,Guidelines for Designing Visual Ontologies to Support Knowledge Identification1,"Volume 35, Issue 4",December 2011,"Organizations often provide workers with knowledge management systems to help them obtain knowledge they need. A significant constraint on the effectiveness of such systems is that they assume workers know what knowledge they need (they know what they don’t know) when, in fact, they often do not know what knowledge they need (they don’t know what they don’t know). A way to overcome this problem is to use visual ontologies to help users learn relevant concepts and relationships in the knowledge domain, enabling them to search the knowledge base in a more educated manner. However, no guidelines exist for designing such ontologies. To fill this gap, we draw on theories of philosophical ontology and cognition to propose guidelines for designing visual ontologies for knowledge identification.We conducted three experiments to compare the effectiveness of guided ontologies, visual ontologies that followed our guidelines, to unguided ontologies, visual ontologies that violated our guidelines. We found that subjects performed considerably better with the guided ontologies, and that subjects could perceive the benefits of using guided ontologies, at least in some circumstances. On the basis of these results, we conclude that the way visual ontologies are presented makes a difference in knowledge identification and that theories of philosophical ontology and cognition can guide the construction of more effective visual representations. Furthermore, we propose that the principles we used to create the guided visual ontologies can be generalized for other cases where visual models are used to inform users about application domains.",[],"Wand, Yair",N/A,"Sauder School of Business, The University of British Columbia, Vancouver, BC V6T 1Z2 Canada"
https://misq.umn.edu/misq/article/35/4/909/2606/A-Multilevel-Model-for-Measuring-Fit-Between-a,MIS Quarterly,A Multilevel Model for Measuring Fit Between a Firm’s Competitive Strategies and Information Systems Capabilities1,"Volume 35, Issue 4",December 2011,"To compete in a highly dynamic marketplace, firms must frequently adapt and align their competitive strategies and information systems. The dominant literature on the strategic fit of a firm’s information systems focuses primarily on high-level measures of the strategic fit of a firm’s overall IS portfolio and the impact of fit on business performance. This paper addresses the need for a more fine-grained approach for assessing the specific areas of misfit between a firm’s competitive strategies and IS capabilities. We describe the design and evaluation of a multilevel strategic fit (MSF) measurement model that enables researchers and practitioners to measure the strategic fit of a firm’s information systems at both an overall and a detailed level. The steps in the model include identifying the relevant IS capabilities according to the type of system; measuring the current level of support for each capability using a capabilities instrument; identifying the ideal level of support for each capability using an adaptation of Conant et al.’s (1990) instrument to assess strategic archetype; and comparing the ideal and realized level of support for each capability. Evidence from a multiple case study analysis indicates that the fine-grained assessment of strategic fit can strengthen the validity, utility, and ease of corroboration of the strategic fit measurement outputs. The paper also demonstrates how an iterative design science research approach, with its emphasis on evaluating the utility of prototype artifacts, is well suited to developing field-tested and theoretically grounded measurement models and instruments that are accessible to practitioners. This focus on practical utility in turn provides researchers with results that can be more readily corroborated, thus improving the quality and usefulness of the research findings.",[],"McLaren, Tim S.",N/A,"Ted Rogers School of Management, Ryerson University, 350 Victoria Street, Toronto, ON M5B 2K3 Canada"
https://misq.umn.edu/misq/article/35/4/909/2606/A-Multilevel-Model-for-Measuring-Fit-Between-a,MIS Quarterly,A Multilevel Model for Measuring Fit Between a Firm’s Competitive Strategies and Information Systems Capabilities1,"Volume 35, Issue 4",December 2011,"To compete in a highly dynamic marketplace, firms must frequently adapt and align their competitive strategies and information systems. The dominant literature on the strategic fit of a firm’s information systems focuses primarily on high-level measures of the strategic fit of a firm’s overall IS portfolio and the impact of fit on business performance. This paper addresses the need for a more fine-grained approach for assessing the specific areas of misfit between a firm’s competitive strategies and IS capabilities. We describe the design and evaluation of a multilevel strategic fit (MSF) measurement model that enables researchers and practitioners to measure the strategic fit of a firm’s information systems at both an overall and a detailed level. The steps in the model include identifying the relevant IS capabilities according to the type of system; measuring the current level of support for each capability using a capabilities instrument; identifying the ideal level of support for each capability using an adaptation of Conant et al.’s (1990) instrument to assess strategic archetype; and comparing the ideal and realized level of support for each capability. Evidence from a multiple case study analysis indicates that the fine-grained assessment of strategic fit can strengthen the validity, utility, and ease of corroboration of the strategic fit measurement outputs. The paper also demonstrates how an iterative design science research approach, with its emphasis on evaluating the utility of prototype artifacts, is well suited to developing field-tested and theoretically grounded measurement models and instruments that are accessible to practitioners. This focus on practical utility in turn provides researchers with results that can be more readily corroborated, thus improving the quality and usefulness of the research findings.",[],"Head, Milena M.",N/A,"DeGroote School of Business, McMaster University, 1280 Main Street West, Hamilton, ON L8S 4M4 Canada"
https://misq.umn.edu/misq/article/35/4/909/2606/A-Multilevel-Model-for-Measuring-Fit-Between-a,MIS Quarterly,A Multilevel Model for Measuring Fit Between a Firm’s Competitive Strategies and Information Systems Capabilities1,"Volume 35, Issue 4",December 2011,"To compete in a highly dynamic marketplace, firms must frequently adapt and align their competitive strategies and information systems. The dominant literature on the strategic fit of a firm’s information systems focuses primarily on high-level measures of the strategic fit of a firm’s overall IS portfolio and the impact of fit on business performance. This paper addresses the need for a more fine-grained approach for assessing the specific areas of misfit between a firm’s competitive strategies and IS capabilities. We describe the design and evaluation of a multilevel strategic fit (MSF) measurement model that enables researchers and practitioners to measure the strategic fit of a firm’s information systems at both an overall and a detailed level. The steps in the model include identifying the relevant IS capabilities according to the type of system; measuring the current level of support for each capability using a capabilities instrument; identifying the ideal level of support for each capability using an adaptation of Conant et al.’s (1990) instrument to assess strategic archetype; and comparing the ideal and realized level of support for each capability. Evidence from a multiple case study analysis indicates that the fine-grained assessment of strategic fit can strengthen the validity, utility, and ease of corroboration of the strategic fit measurement outputs. The paper also demonstrates how an iterative design science research approach, with its emphasis on evaluating the utility of prototype artifacts, is well suited to developing field-tested and theoretically grounded measurement models and instruments that are accessible to practitioners. This focus on practical utility in turn provides researchers with results that can be more readily corroborated, thus improving the quality and usefulness of the research findings.",[],"Yuan, Yufei",N/A,"DeGroote School of Business, McMaster University, 1280 Main Street West, Hamilton, ON L8S 4M4 Canada"
https://misq.umn.edu/misq/article/35/4/909/2606/A-Multilevel-Model-for-Measuring-Fit-Between-a,MIS Quarterly,A Multilevel Model for Measuring Fit Between a Firm’s Competitive Strategies and Information Systems Capabilities1,"Volume 35, Issue 4",December 2011,"To compete in a highly dynamic marketplace, firms must frequently adapt and align their competitive strategies and information systems. The dominant literature on the strategic fit of a firm’s information systems focuses primarily on high-level measures of the strategic fit of a firm’s overall IS portfolio and the impact of fit on business performance. This paper addresses the need for a more fine-grained approach for assessing the specific areas of misfit between a firm’s competitive strategies and IS capabilities. We describe the design and evaluation of a multilevel strategic fit (MSF) measurement model that enables researchers and practitioners to measure the strategic fit of a firm’s information systems at both an overall and a detailed level. The steps in the model include identifying the relevant IS capabilities according to the type of system; measuring the current level of support for each capability using a capabilities instrument; identifying the ideal level of support for each capability using an adaptation of Conant et al.’s (1990) instrument to assess strategic archetype; and comparing the ideal and realized level of support for each capability. Evidence from a multiple case study analysis indicates that the fine-grained assessment of strategic fit can strengthen the validity, utility, and ease of corroboration of the strategic fit measurement outputs. The paper also demonstrates how an iterative design science research approach, with its emphasis on evaluating the utility of prototype artifacts, is well suited to developing field-tested and theoretically grounded measurement models and instruments that are accessible to practitioners. This focus on practical utility in turn provides researchers with results that can be more readily corroborated, thus improving the quality and usefulness of the research findings.",[],"Chan, Yolande E.",N/A,"Queen’s School of Business, Queen’s University, 143 Union Street, Kingston, ON K7L 3N6 Canada"
https://misq.umn.edu/misq/article/35/4/931/1454/Understanding-the-Link-Between-Information,MIS Quarterly,Understanding the Link Between Information Technology Capability and Organizational Agility: An Empirical Examination1,"Volume 35, Issue 4",December 2011,"Information technology is generally considered an enabler of a firm’s agility. A typical premise is that greater IT investment enables a firm to be more agile. However, it is not uncommon that IT can also hinder and sometimes even impede organizational agility. We propose and theorize this frequently observed but understudied IT–agility contradiction by which IT may enable or impede agility. We develop the premise that organizations need to develop superior firm-wide IT capability to successfully manage their IT resources to realize agility. We refine the conceptualization and measurement of IT capability as a latent construct reflected in its three dimensions: IT infrastructure capability, IT business spanning capability, and IT proactive stance. We also conceptualize two types of organizational agility: market capitalizing agility and operational adjustment agility. We then conduct a matched-pair field survey of business and information systems executives in 128 organizations to empirically examine the link between a firm’s IT capability and agility. Business executives responded to measurement scales of the two types of agility and organizational context variables, and IS executives responded to measurement scales of IT capabilities and IS context variables. The results show a significant positive relationship between IT capability and the two types of organizational agility. We also find a significant positive joint effect of IT capability and IT spending on operational adjustment agility but not on market capitalizing agility. The findings suggest a possible resolution to the contradictory effect of IT on agility: while more IT spending does not lead to greater agility, spending it in such a way as to enhance and foster IT capabilities does. Our study provides initial empirical evidence to better understand essential IT capabilities and their relationship with organizational agility. Our findings provide a number of useful implications for research and managerial practices.",[],"Lu, Ying",N/A,"School of Business, Pfeiffer University, Misenheimer, NC 28109 U.S.A."
https://misq.umn.edu/misq/article/35/4/931/1454/Understanding-the-Link-Between-Information,MIS Quarterly,Understanding the Link Between Information Technology Capability and Organizational Agility: An Empirical Examination1,"Volume 35, Issue 4",December 2011,"Information technology is generally considered an enabler of a firm’s agility. A typical premise is that greater IT investment enables a firm to be more agile. However, it is not uncommon that IT can also hinder and sometimes even impede organizational agility. We propose and theorize this frequently observed but understudied IT–agility contradiction by which IT may enable or impede agility. We develop the premise that organizations need to develop superior firm-wide IT capability to successfully manage their IT resources to realize agility. We refine the conceptualization and measurement of IT capability as a latent construct reflected in its three dimensions: IT infrastructure capability, IT business spanning capability, and IT proactive stance. We also conceptualize two types of organizational agility: market capitalizing agility and operational adjustment agility. We then conduct a matched-pair field survey of business and information systems executives in 128 organizations to empirically examine the link between a firm’s IT capability and agility. Business executives responded to measurement scales of the two types of agility and organizational context variables, and IS executives responded to measurement scales of IT capabilities and IS context variables. The results show a significant positive relationship between IT capability and the two types of organizational agility. We also find a significant positive joint effect of IT capability and IT spending on operational adjustment agility but not on market capitalizing agility. The findings suggest a possible resolution to the contradictory effect of IT on agility: while more IT spending does not lead to greater agility, spending it in such a way as to enhance and foster IT capabilities does. Our study provides initial empirical evidence to better understand essential IT capabilities and their relationship with organizational agility. Our findings provide a number of useful implications for research and managerial practices.",[],"Ramamurthy, K. (Ram)",N/A,"Sheldon B. Lubar School of Business, University of Wisconsin–Milwaukee, Milwaukee, WI 54302 U.S.A."
https://misq.umn.edu/misq/article/35/4/955/2604/Freedom-of-Choice-Ease-of-Use-and-the-Formation-of,MIS Quarterly,"Freedom of Choice, Ease of Use, and the Formation of Interface Preferences1","Volume 35, Issue 4",December 2011,"How does users’ freedom of choice, or the lack thereof, affect interface preferences? The research reported in this article approaches this question from two theoretical perspectives. The first of these argues that an interface with a dominant market share benefits from the absence of competition because users acquire skills that are specific to that particular interface, which in turn reduces the probability that they will switch to a new competitor interface in the future. By contrast, the second perspective proposes that the advantage that a market leader has in being able to install a set of non-transferable skills in its user base is offset by a psychological force that causes humans to react against perceived constraints on their freedom of choice. We test a research model that incorporates the key predictions of these two theoretical perspectives in an experiment involving consequential interface choices. We find strong support for the second perspective, which builds upon the theory of psychological reactance.",[],"Murray, Kyle B.",N/A,"School of Business, University of Alberta, Edmonton, Alberta T6G 2R6 Canada"
https://misq.umn.edu/misq/article/35/4/955/2604/Freedom-of-Choice-Ease-of-Use-and-the-Formation-of,MIS Quarterly,"Freedom of Choice, Ease of Use, and the Formation of Interface Preferences1","Volume 35, Issue 4",December 2011,"How does users’ freedom of choice, or the lack thereof, affect interface preferences? The research reported in this article approaches this question from two theoretical perspectives. The first of these argues that an interface with a dominant market share benefits from the absence of competition because users acquire skills that are specific to that particular interface, which in turn reduces the probability that they will switch to a new competitor interface in the future. By contrast, the second perspective proposes that the advantage that a market leader has in being able to install a set of non-transferable skills in its user base is offset by a psychological force that causes humans to react against perceived constraints on their freedom of choice. We test a research model that incorporates the key predictions of these two theoretical perspectives in an experiment involving consequential interface choices. We find strong support for the second perspective, which builds upon the theory of psychological reactance.",[],"Häubl, Gerald",N/A,"School of Business, University of Alberta, Edmonton, Alberta T6G 2R6 Canada"
https://misq.umn.edu/misq/article/35/4/977/1452/State-of-the-Information-Privacy-Literature-Where,MIS Quarterly,State of the Information Privacy Literature: Where Are We Now and Where Should We Go?,"Volume 35, Issue 4",December 2011,"While information privacy has been studied in multiple disciplines over the years, the advent of the information age has both elevated the importance of privacy in theory and practice, and increased the relevance of information privacy literature for Information Systems, which has taken a leading role in the theoretical and practical study of information privacy. There is an impressive body of literature on information privacy in IS, and the two Theory and Review articles in this issue of MIS Quarterly review this literature. By integrating these two articles, this paper evaluates the current state of the IS literature on information privacy (where are we now?) and identifies promising research directions for advancing IS research on information privacy (where should we go?). Additional thoughts on further expanding the information privacy research in IS by drawing on related disciplines to enable a multidisciplinary study of information privacy are discussed.",[],"Pavlou, Paul A.",N/A,"Fox School of Business, Temple University, 334 Alter Hall, Philadelphia, PA 19122 U.S.A."
https://misq.umn.edu/misq/article/35/4/iii/2603/Editor-s-CommentsIT-and-Entrepreneurism-An-On,MIS Quarterly,"Editor’s CommentsIT and Entrepreneurism: An On-Again, Off-Again Love Affair or a Marriage?","Volume 35, Issue 4",December 2011,"Advances in information technologies and the growth of a knowledge-based service economy are transforming the basis of technological innovation and corporate competition, and....this transformation requires taking a broader, institutional and political view of information technology and knowledge management (Van de Ven 2005, p. 365).",[],"Giudice, Manlio Del",N/A,Second University of Naples
https://misq.umn.edu/misq/article/35/4/iii/2603/Editor-s-CommentsIT-and-Entrepreneurism-An-On,MIS Quarterly,"Editor’s CommentsIT and Entrepreneurism: An On-Again, Off-Again Love Affair or a Marriage?","Volume 35, Issue 4",December 2011,"Advances in information technologies and the growth of a knowledge-based service economy are transforming the basis of technological innovation and corporate competition, and....this transformation requires taking a broader, institutional and political view of information technology and knowledge management (Van de Ven 2005, p. 365).",[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/35/4/989/2607/Information-Privacy-Research-an-Interdisciplinary,MIS Quarterly,Information Privacy Research: an Interdisciplinary Review1,"Volume 35, Issue 4",December 2011,"To date, many important threads of information privacy research have developed, but these threads have not been woven together into a cohesive fabric. This paper provides an interdisciplinary review of privacy-related research in order to enable a more cohesive treatment. With a sample of 320 privacy articles and 128 books and book sections, we classify previous literature in two ways: (1) using an ethics-based nomenclature of normative, purely descriptive, and empirically descriptive, and (2) based on their level of analysis: individual, group, organizational, and societal.Based upon our analyses via these two classification approaches, we identify three major areas in which previous research contributions reside: the conceptualization of information privacy, the relationship between information privacy and other constructs, and the contextual nature of these relationships.As we consider these major areas, we draw three overarching conclusions. First, there are many theoretical developments in the body of normative and purely descriptive studies that have not been addressed in empirical research on privacy. Rigorous studies that either trace processes associated with, or test implied assertions from, these value-laden arguments could add great value. Second, some of the levels of analysis have received less attention in certain contexts than have others in the research to date. Future empirical studies—both positivist and interpretive—could profitably be targeted to these under-researched levels of analysis. Third, positivist empirical studies will add the greatest value if they focus on antecedents to privacy concerns and on actual outcomes. In that light, we recommend that researchers be alert to an overarching macro model that we term APCO (Antecedents → Privacy Concerns → Outcomes).",[],"Smith, H. Jeff",N/A,"Department of Decision Sciences and Management Information Systems, Farmer School of Business, Miami University, Oxford, OH 45056 U.S.A."
https://misq.umn.edu/misq/article/35/4/989/2607/Information-Privacy-Research-an-Interdisciplinary,MIS Quarterly,Information Privacy Research: an Interdisciplinary Review1,"Volume 35, Issue 4",December 2011,"To date, many important threads of information privacy research have developed, but these threads have not been woven together into a cohesive fabric. This paper provides an interdisciplinary review of privacy-related research in order to enable a more cohesive treatment. With a sample of 320 privacy articles and 128 books and book sections, we classify previous literature in two ways: (1) using an ethics-based nomenclature of normative, purely descriptive, and empirically descriptive, and (2) based on their level of analysis: individual, group, organizational, and societal.Based upon our analyses via these two classification approaches, we identify three major areas in which previous research contributions reside: the conceptualization of information privacy, the relationship between information privacy and other constructs, and the contextual nature of these relationships.As we consider these major areas, we draw three overarching conclusions. First, there are many theoretical developments in the body of normative and purely descriptive studies that have not been addressed in empirical research on privacy. Rigorous studies that either trace processes associated with, or test implied assertions from, these value-laden arguments could add great value. Second, some of the levels of analysis have received less attention in certain contexts than have others in the research to date. Future empirical studies—both positivist and interpretive—could profitably be targeted to these under-researched levels of analysis. Third, positivist empirical studies will add the greatest value if they focus on antecedents to privacy concerns and on actual outcomes. In that light, we recommend that researchers be alert to an overarching macro model that we term APCO (Antecedents → Privacy Concerns → Outcomes).",[],"Dinev, Tamara",N/A,"Department of Information Technology & Operations Management, College of Business, Florida Atlantic University, Boca Raton, FL 33431 U.S.A."
https://misq.umn.edu/misq/article/35/4/989/2607/Information-Privacy-Research-an-Interdisciplinary,MIS Quarterly,Information Privacy Research: an Interdisciplinary Review1,"Volume 35, Issue 4",December 2011,"To date, many important threads of information privacy research have developed, but these threads have not been woven together into a cohesive fabric. This paper provides an interdisciplinary review of privacy-related research in order to enable a more cohesive treatment. With a sample of 320 privacy articles and 128 books and book sections, we classify previous literature in two ways: (1) using an ethics-based nomenclature of normative, purely descriptive, and empirically descriptive, and (2) based on their level of analysis: individual, group, organizational, and societal.Based upon our analyses via these two classification approaches, we identify three major areas in which previous research contributions reside: the conceptualization of information privacy, the relationship between information privacy and other constructs, and the contextual nature of these relationships.As we consider these major areas, we draw three overarching conclusions. First, there are many theoretical developments in the body of normative and purely descriptive studies that have not been addressed in empirical research on privacy. Rigorous studies that either trace processes associated with, or test implied assertions from, these value-laden arguments could add great value. Second, some of the levels of analysis have received less attention in certain contexts than have others in the research to date. Future empirical studies—both positivist and interpretive—could profitably be targeted to these under-researched levels of analysis. Third, positivist empirical studies will add the greatest value if they focus on antecedents to privacy concerns and on actual outcomes. In that light, we recommend that researchers be alert to an overarching macro model that we term APCO (Antecedents → Privacy Concerns → Outcomes).",[],"Xu, Heng",N/A,"College of Information Sciences and Technology, Pennsylvania State University, University Park, PA 16802 U.S.A."
https://misq.umn.edu/misq/article/35/4/1017/2622/Privacy-in-the-Digital-Age-a-Review-of-Information,MIS Quarterly,Privacy in the Digital Age: a Review of Information Privacy Research in Information Systems1,"Volume 35, Issue 4",December 2011,"Information privacy refers to the desire of individuals to control or have some influence over data about themselves. Advances in information technology have raised concerns about information privacy and its impacts, and have motivated Information Systems researchers to explore information privacy issues, including technical solutions to address these concerns. In this paper, we inform researchers about the current state of information privacy research in IS through a critical analysis of the IS literature that considers information privacy as a key construct. The review of the literature reveals that information privacy is a multilevel concept, but rarely studied as such. We also find that information privacy research has been heavily reliant on student-based and USA-centric samples, which results in findings of limited generalizability. Information privacy research focuses on explaining and predicting theoretical contributions, with few studies in journal articles focusing on design and action contributions. We recommend that future research should consider different levels of analysis as well as multilevel effects of information privacy. We illustrate this with a multilevel framework for information privacy concerns. We call for research on information privacy to use a broader diversity of sampling populations, and for more design and action information privacy research to be published in journal articles that can result in IT artifacts for protection or control of information privacy.",[],"Bélanger, France",N/A,"Pamplin College of Business, Virginia Tech, 3007 Pamplin Hall, Blacksburg, VA 24061-0101 U.S.A."
https://misq.umn.edu/misq/article/35/4/1017/2622/Privacy-in-the-Digital-Age-a-Review-of-Information,MIS Quarterly,Privacy in the Digital Age: a Review of Information Privacy Research in Information Systems1,"Volume 35, Issue 4",December 2011,"Information privacy refers to the desire of individuals to control or have some influence over data about themselves. Advances in information technology have raised concerns about information privacy and its impacts, and have motivated Information Systems researchers to explore information privacy issues, including technical solutions to address these concerns. In this paper, we inform researchers about the current state of information privacy research in IS through a critical analysis of the IS literature that considers information privacy as a key construct. The review of the literature reveals that information privacy is a multilevel concept, but rarely studied as such. We also find that information privacy research has been heavily reliant on student-based and USA-centric samples, which results in findings of limited generalizability. Information privacy research focuses on explaining and predicting theoretical contributions, with few studies in journal articles focusing on design and action contributions. We recommend that future research should consider different levels of analysis as well as multilevel effects of information privacy. We illustrate this with a multilevel framework for information privacy concerns. We call for research on information privacy to use a broader diversity of sampling populations, and for more design and action information privacy research to be published in journal articles that can result in IT artifacts for protection or control of information privacy.",[],"Crossler, Robert E.",N/A,"Department of Management and Information Systems, College of Business, Mississippi State University, P.O. Box 9581, Mississippi State, MS 39762-9581 U.S.A."
https://misq.umn.edu/misq/article/35/4/1043/2618/Integrating-Technology-Addiction-and-Use-an,MIS Quarterly,Integrating Technology Addiction and Use: an Empirical Investigation of Online Auction Users1,"Volume 35, Issue 4",December 2011,"Technology addiction is a relatively new mental condition that has not yet been well integrated into mainstream MIS models. This study bridges this gap and incorporates technology addiction into technology use processes in the context of online auctions. It examines how user cognition and ultimately usage intentions toward an information technology are distorted by addiction to the technology. The findings from two empirical studies of 132 and 223 eBay users, using three different operationalizations of addiction, indicate that the level of online auction addiction distorts the way the IT artifact is perceived. Informing a range of cognition-modification processes, addiction to online auctions augments user perceptions of enjoyment, usefulness, and ease of use attributed to the technology, which in turn influence usage intentions. Overall, consistent with behavioral addiction models, the findings indicate that users’ levels of online auction addiction influence their reasoned IT usage decisions by altering users’ belief systems. The formation of maladaptive perceptions is driven by a combination of memory-, learning-, and bias-based cognition modification processes. Implications of the findings are discussed.",[],"Turel, Ofir",N/A,"Steven G. Mihaylo College of Business and Economics, California State University, Fullerton, 800 N. State College Boulevard, Fullerton, CA 92834 U.S.A."
https://misq.umn.edu/misq/article/35/4/1043/2618/Integrating-Technology-Addiction-and-Use-an,MIS Quarterly,Integrating Technology Addiction and Use: an Empirical Investigation of Online Auction Users1,"Volume 35, Issue 4",December 2011,"Technology addiction is a relatively new mental condition that has not yet been well integrated into mainstream MIS models. This study bridges this gap and incorporates technology addiction into technology use processes in the context of online auctions. It examines how user cognition and ultimately usage intentions toward an information technology are distorted by addiction to the technology. The findings from two empirical studies of 132 and 223 eBay users, using three different operationalizations of addiction, indicate that the level of online auction addiction distorts the way the IT artifact is perceived. Informing a range of cognition-modification processes, addiction to online auctions augments user perceptions of enjoyment, usefulness, and ease of use attributed to the technology, which in turn influence usage intentions. Overall, consistent with behavioral addiction models, the findings indicate that users’ levels of online auction addiction influence their reasoned IT usage decisions by altering users’ belief systems. The formation of maladaptive perceptions is driven by a combination of memory-, learning-, and bias-based cognition modification processes. Implications of the findings are discussed.",[],"Serenko, Alexander",N/A,"Faculty of Business Administration, Lakehead University, 955 Oliver Road, Thunder Bay, ON P7B 5E1 Canada"
https://misq.umn.edu/misq/article/35/4/1043/2618/Integrating-Technology-Addiction-and-Use-an,MIS Quarterly,Integrating Technology Addiction and Use: an Empirical Investigation of Online Auction Users1,"Volume 35, Issue 4",December 2011,"Technology addiction is a relatively new mental condition that has not yet been well integrated into mainstream MIS models. This study bridges this gap and incorporates technology addiction into technology use processes in the context of online auctions. It examines how user cognition and ultimately usage intentions toward an information technology are distorted by addiction to the technology. The findings from two empirical studies of 132 and 223 eBay users, using three different operationalizations of addiction, indicate that the level of online auction addiction distorts the way the IT artifact is perceived. Informing a range of cognition-modification processes, addiction to online auctions augments user perceptions of enjoyment, usefulness, and ease of use attributed to the technology, which in turn influence usage intentions. Overall, consistent with behavioral addiction models, the findings indicate that users’ levels of online auction addiction influence their reasoned IT usage decisions by altering users’ belief systems. The formation of maladaptive perceptions is driven by a combination of memory-, learning-, and bias-based cognition modification processes. Implications of the findings are discussed.",[],"Giles, Paul",N/A,"Hardy Giles Consulting, 656 City Road, Thunder Bay, ON P7G 1K3 Canada"
https://misq.umn.edu/misq/article/35/4/1063/1453/Centrality-is-Proficiency-Alignment-and-Workgroup,MIS Quarterly,Centrality–is Proficiency Alignment and Workgroup Performance1,"Volume 35, Issue 4",December 2011,"Virtually all of the extensive previous research investigating the effect of information systems proficiency on performance has been conducted at the individual level. Little research has investigated the relationship between IS proficiency and performance at the group level. In this paper, we argue that IS proficiency at the group level may be more than the simple sum or average of the IS proficiency of individual group members. Rather, effective group-level IS proficiency may also be a function of how a group’s IS proficiency is distributed across its members. Relying on concepts associated with social network analysis (SNA), we introduce the concept of centrality–IS proficiency alignment. We argue that groups will perform better if their more proficient members are highly central in the group’s communication and workflows network. Data from 468 employees in 32 workgroups show that centrality–IS proficiency alignment is significantly and positively related to performance across multiple systems examined individually and with the portfolio of systems examined as a whole. This approach effectively integrates the structural and resource perspectives of SNA, providing a roadmap so that others may follow a similar approach to address broader questions of group-level user–system interactions in the IS literature and more general questions of central resource alignment in the broader organizational literature.",[],"Kane, Gerald C.",N/A,"Carroll School of Management, Boston College, Chestnut Hill, MA 02467 U.S.A."
https://misq.umn.edu/misq/article/35/4/1063/1453/Centrality-is-Proficiency-Alignment-and-Workgroup,MIS Quarterly,Centrality–is Proficiency Alignment and Workgroup Performance1,"Volume 35, Issue 4",December 2011,"Virtually all of the extensive previous research investigating the effect of information systems proficiency on performance has been conducted at the individual level. Little research has investigated the relationship between IS proficiency and performance at the group level. In this paper, we argue that IS proficiency at the group level may be more than the simple sum or average of the IS proficiency of individual group members. Rather, effective group-level IS proficiency may also be a function of how a group’s IS proficiency is distributed across its members. Relying on concepts associated with social network analysis (SNA), we introduce the concept of centrality–IS proficiency alignment. We argue that groups will perform better if their more proficient members are highly central in the group’s communication and workflows network. Data from 468 employees in 32 workgroups show that centrality–IS proficiency alignment is significantly and positively related to performance across multiple systems examined individually and with the portfolio of systems examined as a whole. This approach effectively integrates the structural and resource perspectives of SNA, providing a roadmap so that others may follow a similar approach to address broader questions of group-level user–system interactions in the IS literature and more general questions of central resource alignment in the broader organizational literature.",[],"Borgatti, Stephen P.",N/A,"LINKS Center for Network Analysis of Organizations, Gatton College of Business & Economics, University of Kentucky, 550 S. Limestone St., Lexington, KY 40506-0034 U.S.A."
https://misq.umn.edu/misq/article/35/4/1079/2620/Virtual-Space-and-Place-Theory-and-Test1,MIS Quarterly,Virtual Space and Place: Theory and Test1,"Volume 35, Issue 4",December 2011,"Little is known about how individuals come to relate to settings in virtual worlds (VWs), which are defined as digital environments in which individuals, groups, and even organizations interact in virtual (that is to say, nonphysical) spaces. This research develops a theory of virtual space and place (VSP), specifically relating this to the setting of Second Life (SL), a prominent social virtual world. We explore how three-dimensional space, as perceived by users, is able to provide them with an interactive experience with virtual objects, as well as with other VW denizens. To test our theory, we build interactive work tools in SL that are designed to reflect various degrees of motion range and to influence presence. The three information technology tools are evaluated by 150 business professionals who are either familiar or unfamiliar with SL. Implications for practice and directions for future research are discussed.",[],"Saunders, Carol",N/A,"Department of Management, College of Business Administration, University of Central Florida, Orlando, FL 32816-1400 U.S.A."
https://misq.umn.edu/misq/article/35/4/1079/2620/Virtual-Space-and-Place-Theory-and-Test1,MIS Quarterly,Virtual Space and Place: Theory and Test1,"Volume 35, Issue 4",December 2011,"Little is known about how individuals come to relate to settings in virtual worlds (VWs), which are defined as digital environments in which individuals, groups, and even organizations interact in virtual (that is to say, nonphysical) spaces. This research develops a theory of virtual space and place (VSP), specifically relating this to the setting of Second Life (SL), a prominent social virtual world. We explore how three-dimensional space, as perceived by users, is able to provide them with an interactive experience with virtual objects, as well as with other VW denizens. To test our theory, we build interactive work tools in SL that are designed to reflect various degrees of motion range and to influence presence. The three information technology tools are evaluated by 150 business professionals who are either familiar or unfamiliar with SL. Implications for practice and directions for future research are discussed.",[],"Rutkowski, Anne F.",N/A,"Department of Information Management, Tilburg University, Tilburg, The Netherlands"
https://misq.umn.edu/misq/article/35/4/1079/2620/Virtual-Space-and-Place-Theory-and-Test1,MIS Quarterly,Virtual Space and Place: Theory and Test1,"Volume 35, Issue 4",December 2011,"Little is known about how individuals come to relate to settings in virtual worlds (VWs), which are defined as digital environments in which individuals, groups, and even organizations interact in virtual (that is to say, nonphysical) spaces. This research develops a theory of virtual space and place (VSP), specifically relating this to the setting of Second Life (SL), a prominent social virtual world. We explore how three-dimensional space, as perceived by users, is able to provide them with an interactive experience with virtual objects, as well as with other VW denizens. To test our theory, we build interactive work tools in SL that are designed to reflect various degrees of motion range and to influence presence. The three information technology tools are evaluated by 150 business professionals who are either familiar or unfamiliar with SL. Implications for practice and directions for future research are discussed.",[],"van Genuchten, Michiel",N/A,"Open Digital Dentistry, Zug, Switzerland"
https://misq.umn.edu/misq/article/35/4/1079/2620/Virtual-Space-and-Place-Theory-and-Test1,MIS Quarterly,Virtual Space and Place: Theory and Test1,"Volume 35, Issue 4",December 2011,"Little is known about how individuals come to relate to settings in virtual worlds (VWs), which are defined as digital environments in which individuals, groups, and even organizations interact in virtual (that is to say, nonphysical) spaces. This research develops a theory of virtual space and place (VSP), specifically relating this to the setting of Second Life (SL), a prominent social virtual world. We explore how three-dimensional space, as perceived by users, is able to provide them with an interactive experience with virtual objects, as well as with other VW denizens. To test our theory, we build interactive work tools in SL that are designed to reflect various degrees of motion range and to influence presence. The three information technology tools are evaluated by 150 business professionals who are either familiar or unfamiliar with SL. Implications for practice and directions for future research are discussed.",[],"Vogel, Doug",N/A,"Department of Information Systems, City University of Hong Kong, Hong Kong"
https://misq.umn.edu/misq/article/35/4/1079/2620/Virtual-Space-and-Place-Theory-and-Test1,MIS Quarterly,Virtual Space and Place: Theory and Test1,"Volume 35, Issue 4",December 2011,"Little is known about how individuals come to relate to settings in virtual worlds (VWs), which are defined as digital environments in which individuals, groups, and even organizations interact in virtual (that is to say, nonphysical) spaces. This research develops a theory of virtual space and place (VSP), specifically relating this to the setting of Second Life (SL), a prominent social virtual world. We explore how three-dimensional space, as perceived by users, is able to provide them with an interactive experience with virtual objects, as well as with other VW denizens. To test our theory, we build interactive work tools in SL that are designed to reflect various degrees of motion range and to influence presence. The three information technology tools are evaluated by 150 business professionals who are either familiar or unfamiliar with SL. Implications for practice and directions for future research are discussed.",[],"Orrego, Julio Molina",N/A,"GDF Suez, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/36/1/1/1485/The-Ends-of-Information-Systems-Research-A,MIS Quarterly,The Ends of Information Systems Research: A Pragmatic Framework1,"Volume 36, Issue 1",March 2012,"In this paper, we argue that any effort to understand the state of the Information Systems field has to view IS research as a series of normative choices and value judgments about the ends of research. To assist a systematic questioning of the various ends of IS research, we propose a pragmatic framework that explores the choices IS researchers make around theories and methodologies, ethical methods of conduct, desirable outcomes, and the long-term impact of the research beyond a single site and topic area. We illustrate our framework by considering and questioning the explicit and implicit choices of topics, design and execution, and the representation of knowledge in experimental research—research often considered to be largely beyond value judgments and power relations. We conclude with the implications of our pragmatic framework by proposing practical questions for all IS researchers to consider in making choices about relevant topics, design and execution, and representation of findings in their research.",[],"Constantinides, Panos",N/A,"School of Economic Sciences & Administration, Frederick University, Limassol, Cyprus"
https://misq.umn.edu/misq/article/36/1/1/1485/The-Ends-of-Information-Systems-Research-A,MIS Quarterly,The Ends of Information Systems Research: A Pragmatic Framework1,"Volume 36, Issue 1",March 2012,"In this paper, we argue that any effort to understand the state of the Information Systems field has to view IS research as a series of normative choices and value judgments about the ends of research. To assist a systematic questioning of the various ends of IS research, we propose a pragmatic framework that explores the choices IS researchers make around theories and methodologies, ethical methods of conduct, desirable outcomes, and the long-term impact of the research beyond a single site and topic area. We illustrate our framework by considering and questioning the explicit and implicit choices of topics, design and execution, and the representation of knowledge in experimental research—research often considered to be largely beyond value judgments and power relations. We conclude with the implications of our pragmatic framework by proposing practical questions for all IS researchers to consider in making choices about relevant topics, design and execution, and representation of findings in their research.",[],"Chiasson, Mike W.",N/A,"Department of Management Science, Lancaster University Management School, Lancaster LA1 4YX United Kingdom"
https://misq.umn.edu/misq/article/36/1/1/1485/The-Ends-of-Information-Systems-Research-A,MIS Quarterly,The Ends of Information Systems Research: A Pragmatic Framework1,"Volume 36, Issue 1",March 2012,"In this paper, we argue that any effort to understand the state of the Information Systems field has to view IS research as a series of normative choices and value judgments about the ends of research. To assist a systematic questioning of the various ends of IS research, we propose a pragmatic framework that explores the choices IS researchers make around theories and methodologies, ethical methods of conduct, desirable outcomes, and the long-term impact of the research beyond a single site and topic area. We illustrate our framework by considering and questioning the explicit and implicit choices of topics, design and execution, and the representation of knowledge in experimental research—research often considered to be largely beyond value judgments and power relations. We conclude with the implications of our pragmatic framework by proposing practical questions for all IS researchers to consider in making choices about relevant topics, design and execution, and representation of findings in their research.",[],"Introna, Lucas D.",N/A,"Department of Organization, Work and Technology, Lancaster University Management School, Lancaster LA1 4YX United Kingdom"
https://misq.umn.edu/misq/article/36/1/21/1456/Shackled-to-the-Status-Quo-The-Inhibiting-Effects,MIS Quarterly,"Shackled to the Status Quo: The Inhibiting Effects of Incumbent System Habit, Switching Costs, and Inertia on New System Acceptance1","Volume 36, Issue 1",March 2012,"Given that adoption of a new system often implies fully or partly replacing an incumbent system, resistance is often manifested as failure of a user to switch from an incumbent technology to a newly introduced one. Thus, a potential source of resistance to adopting a new system lies in the use of an incumbent system. Using the status quo bias and habit literatures as theoretical lenses, the study explains how use of an incumbent system negatively impacts new system perceptions and usage intentions. We argue that habitual use of an incumbent system, rationalization due to perceived transition costs, and psychological commitment due to perceived sunk costs all encourage development of inertia. Inertia in turn fully mediates the impact of these incumbent system constructs on constructs related to acceptance of the new system via psychological commitment based on cognitive consistency and by increasing the importance of normative pressures. Specifically, we hypothesize that inertia leads to decreased perceptions of the ease of use and relative advantage of a newly introduced system and has a negative impact on intentions to use the new system, above and beyond its impact through perceptions. Finally, we hypothesize that inertia moderates the relationship between subjective norm and intention, such that normative pressures to use a new system become more important in the presence of inertia. Empirical results largely support the hypothesized relationships showing the inhibiting effect of incumbent-system habit, transition and sunk costs, and inertia on acceptance of a new system. Our study thus extends theoretical understanding of the role of incumbent system constructs such as habit and inertia in technology acceptance, and lays the foundations for further study of the interplay between perceptions and cognition with respect to the incumbent system and those with respect to a new system.",[],"Polites, Greta L.",N/A,"School of Management, Bucknell University, Lewisburg, PA 17837 U.S.A."
https://misq.umn.edu/misq/article/36/1/21/1456/Shackled-to-the-Status-Quo-The-Inhibiting-Effects,MIS Quarterly,"Shackled to the Status Quo: The Inhibiting Effects of Incumbent System Habit, Switching Costs, and Inertia on New System Acceptance1","Volume 36, Issue 1",March 2012,"Given that adoption of a new system often implies fully or partly replacing an incumbent system, resistance is often manifested as failure of a user to switch from an incumbent technology to a newly introduced one. Thus, a potential source of resistance to adopting a new system lies in the use of an incumbent system. Using the status quo bias and habit literatures as theoretical lenses, the study explains how use of an incumbent system negatively impacts new system perceptions and usage intentions. We argue that habitual use of an incumbent system, rationalization due to perceived transition costs, and psychological commitment due to perceived sunk costs all encourage development of inertia. Inertia in turn fully mediates the impact of these incumbent system constructs on constructs related to acceptance of the new system via psychological commitment based on cognitive consistency and by increasing the importance of normative pressures. Specifically, we hypothesize that inertia leads to decreased perceptions of the ease of use and relative advantage of a newly introduced system and has a negative impact on intentions to use the new system, above and beyond its impact through perceptions. Finally, we hypothesize that inertia moderates the relationship between subjective norm and intention, such that normative pressures to use a new system become more important in the presence of inertia. Empirical results largely support the hypothesized relationships showing the inhibiting effect of incumbent-system habit, transition and sunk costs, and inertia on acceptance of a new system. Our study thus extends theoretical understanding of the role of incumbent system constructs such as habit and inertia in technology acceptance, and lays the foundations for further study of the interplay between perceptions and cognition with respect to the incumbent system and those with respect to a new system.",[],"Karahanna, Elena",N/A,"Management Information Systems Department, Terry College of Business, University of Georgia, Athens, GA 30602 U.S.A."
https://misq.umn.edu/misq/article/36/1/43/1459/Are-Markets-For-Vulnerabilities-Effective-1,MIS Quarterly,Are Markets For Vulnerabilities Effective?1,"Volume 36, Issue 1",March 2012,"Current reward structures in security vulnerability disclosure may be skewed toward benefitting nefarious usage of vulnerability information rather than responsible disclosure. Recently suggested market-based mechanisms offer incentives to responsible security researchers for discovering and reporting vulnerabilities. However, concerns exist that any benefits gained through increased incentives for responsible discovery may be lost through information leakage. Using perspectives drawn from the diffusion of innovations literature, we examine the effectiveness of market-based vulnerability disclosure mechanisms. Empirical examination of two years of security alert data finds that market-based disclosure restricts the diffusion of vulnerability exploitations, reduces the risk of exploitation, and decreases the volume of exploitation attempts.",[],"Ransbotham, Sam",N/A,"Carroll School of Management, Boston College, Chestnut Hill, MA 02467 U.S.A."
https://misq.umn.edu/misq/article/36/1/43/1459/Are-Markets-For-Vulnerabilities-Effective-1,MIS Quarterly,Are Markets For Vulnerabilities Effective?1,"Volume 36, Issue 1",March 2012,"Current reward structures in security vulnerability disclosure may be skewed toward benefitting nefarious usage of vulnerability information rather than responsible disclosure. Recently suggested market-based mechanisms offer incentives to responsible security researchers for discovering and reporting vulnerabilities. However, concerns exist that any benefits gained through increased incentives for responsible discovery may be lost through information leakage. Using perspectives drawn from the diffusion of innovations literature, we examine the effectiveness of market-based vulnerability disclosure mechanisms. Empirical examination of two years of security alert data finds that market-based disclosure restricts the diffusion of vulnerability exploitations, reduces the risk of exploitation, and decreases the volume of exploitation attempts.",[],"Mitra, Sabyaschi",N/A,"College of Management, Georgia Institute of Technology, Atlanta, GA 30332 U.S.A."
https://misq.umn.edu/misq/article/36/1/43/1459/Are-Markets-For-Vulnerabilities-Effective-1,MIS Quarterly,Are Markets For Vulnerabilities Effective?1,"Volume 36, Issue 1",March 2012,"Current reward structures in security vulnerability disclosure may be skewed toward benefitting nefarious usage of vulnerability information rather than responsible disclosure. Recently suggested market-based mechanisms offer incentives to responsible security researchers for discovering and reporting vulnerabilities. However, concerns exist that any benefits gained through increased incentives for responsible discovery may be lost through information leakage. Using perspectives drawn from the diffusion of innovations literature, we examine the effectiveness of market-based vulnerability disclosure mechanisms. Empirical examination of two years of security alert data finds that market-based disclosure restricts the diffusion of vulnerability exploitations, reduces the risk of exploitation, and decreases the volume of exploitation attempts.",[],"Ramsey, Jon",N/A,"Dell SecureWorks, Atlanta, GA 30328 U.S.A."
https://misq.umn.edu/misq/article/36/1/65/1461/Recommendation-Networks-and-the-Long-Tail-of,MIS Quarterly,Recommendation Networks and the Long Tail of Electronic Commerce1,"Volume 36, Issue 1",March 2012,"It has been conjectured that the peer-based recommendations associated with electronic commerce lead to a redistribution of demand from popular products or “blockbusters” to less popular or “niche” products, and that electronic markets will therefore be characterized by a “long tail” of demand and revenue. We test this conjecture using the revenue distributions of books in over 200 distinct categories on Amazon.com and detailed daily snapshots of co-purchase recommendation networks in which the products of these categories are situated. We measure how much a product is influenced by its position in this hyperlinked network of recommendations using a variant of Google’s PageRank measure of centrality. We then associate the average influence of the network on each category with the inequality in the distribution of its demand and revenue, quantifying this inequality using the Gini coefficient derived from the category’s Lorenz curve. We establish that categories whose products are influenced more by the recommendation network have significantly flatter demand and revenue distributions, even after controlling for variation in average category demand, category size, and price differentials. Our empirical findings indicate that doubling the average network influence on a category is associated with an average increase of about 50 percent in the relative revenue for the least popular 20 percent of products, and with an average reduction of about 15 percent in the relative revenue for the most popular 20 percent of products. We also show that this effect is enhanced by higher assortative mixing and lower clustering in the network, and is greater in categories whose products are more evenly influenced by recommendations. The direction of these results persists over time, across both demand and revenue distributions, and across both daily and weekly demand aggregations. Our work illustrates how the microscopic economic data revealed by online networks can be used to define and answer new kinds of research questions, offers a fresh perspective on the influence of networked IT artifacts on business outcomes, and provides novel empirical evidence about the impact of visible recommendations on the long tail of electronic commerce.",[],"Oestreicher-Singer, Gal",N/A,"Recanati Graduate School of Business, Tel Aviv University, Tel Aviv 69978 Israel"
https://misq.umn.edu/misq/article/36/1/65/1461/Recommendation-Networks-and-the-Long-Tail-of,MIS Quarterly,Recommendation Networks and the Long Tail of Electronic Commerce1,"Volume 36, Issue 1",March 2012,"It has been conjectured that the peer-based recommendations associated with electronic commerce lead to a redistribution of demand from popular products or “blockbusters” to less popular or “niche” products, and that electronic markets will therefore be characterized by a “long tail” of demand and revenue. We test this conjecture using the revenue distributions of books in over 200 distinct categories on Amazon.com and detailed daily snapshots of co-purchase recommendation networks in which the products of these categories are situated. We measure how much a product is influenced by its position in this hyperlinked network of recommendations using a variant of Google’s PageRank measure of centrality. We then associate the average influence of the network on each category with the inequality in the distribution of its demand and revenue, quantifying this inequality using the Gini coefficient derived from the category’s Lorenz curve. We establish that categories whose products are influenced more by the recommendation network have significantly flatter demand and revenue distributions, even after controlling for variation in average category demand, category size, and price differentials. Our empirical findings indicate that doubling the average network influence on a category is associated with an average increase of about 50 percent in the relative revenue for the least popular 20 percent of products, and with an average reduction of about 15 percent in the relative revenue for the most popular 20 percent of products. We also show that this effect is enhanced by higher assortative mixing and lower clustering in the network, and is greater in categories whose products are more evenly influenced by recommendations. The direction of these results persists over time, across both demand and revenue distributions, and across both daily and weekly demand aggregations. Our work illustrates how the microscopic economic data revealed by online networks can be used to define and answer new kinds of research questions, offers a fresh perspective on the influence of networked IT artifacts on business outcomes, and provides novel empirical evidence about the impact of visible recommendations on the long tail of electronic commerce.",[],"Sundararajan, Arun",N/A,"Stern School of Business, New York University, 44 West 4th Street, New York, NY 10012 U.S.A."
https://misq.umn.edu/misq/article/36/1/85/1464/The-Impact-of-Analyst-Induced-Misinformation-on,MIS Quarterly,The Impact of Analyst-Induced Misinformation on the Requirements Elicitation Process1,"Volume 36, Issue 1",March 2012,"Information requirements determination (IRD) is concerned with developing accurate requirements for a proposed system, primarily by eliciting information from users and other organizational stakeholders. In this paper we build and test theory concerning a significant threat to the accuracy of information requirements, termed the misinformation effect. Misinformation is distorted, false, or other erroneous or misleading information that does not reflect the true state of the world or state of mind of the person communicating the information. The misinformation effect refers to the tendency of people to recall misleading or false information introduced to them following an event instead of original material learned or observed at the time the event occurred. During user–analyst communication in the IRD process, analysts may introduce misinformation in their discussions with users. We use the misinformation effect literature to hypothesize that in such circumstances users are likely to recall misinformation introduced by analysts rather than their true beliefs and knowledge of facts. Additionally, we use literature in social psychology to hypothesize that the misinformation effect will be stronger when misinformation is introduced using a social technique rather than a nonsocial technique. We conducted an experiment to test the misinformation effect in the requirements elicitation process. Results indicated that (1) introduction of misinformation reduces the accuracy of requirements provided by users, and (2) social techniques (interviews) are more vulnerable to the misinformation effect than nonsocial techniques (surveys). Our research contributes to the information systems literature by identifying an important reason that requirements provided by users may be inaccurate, and to IRD practice by identifying important dilemmas caused by the misinformation effect as well as potential solutions. We also contribute to the psychology literature by demonstrating the existence of the misinformation effect with users’ experiential factual knowledge and beliefs in a business context, and by aiding in understanding the underlying causes of the misinformation effect. We discuss implications of our findings and directions for future research to address challenges resulting from the misinformation effect.",[],"Appan, Radha",N/A,"Nance College of Business Administration, Cleveland State University, 2121 Euclid Avenue, Cleveland, OH 44115 U.S.A."
https://misq.umn.edu/misq/article/36/1/85/1464/The-Impact-of-Analyst-Induced-Misinformation-on,MIS Quarterly,The Impact of Analyst-Induced Misinformation on the Requirements Elicitation Process1,"Volume 36, Issue 1",March 2012,"Information requirements determination (IRD) is concerned with developing accurate requirements for a proposed system, primarily by eliciting information from users and other organizational stakeholders. In this paper we build and test theory concerning a significant threat to the accuracy of information requirements, termed the misinformation effect. Misinformation is distorted, false, or other erroneous or misleading information that does not reflect the true state of the world or state of mind of the person communicating the information. The misinformation effect refers to the tendency of people to recall misleading or false information introduced to them following an event instead of original material learned or observed at the time the event occurred. During user–analyst communication in the IRD process, analysts may introduce misinformation in their discussions with users. We use the misinformation effect literature to hypothesize that in such circumstances users are likely to recall misinformation introduced by analysts rather than their true beliefs and knowledge of facts. Additionally, we use literature in social psychology to hypothesize that the misinformation effect will be stronger when misinformation is introduced using a social technique rather than a nonsocial technique. We conducted an experiment to test the misinformation effect in the requirements elicitation process. Results indicated that (1) introduction of misinformation reduces the accuracy of requirements provided by users, and (2) social techniques (interviews) are more vulnerable to the misinformation effect than nonsocial techniques (surveys). Our research contributes to the information systems literature by identifying an important reason that requirements provided by users may be inaccurate, and to IRD practice by identifying important dilemmas caused by the misinformation effect as well as potential solutions. We also contribute to the psychology literature by demonstrating the existence of the misinformation effect with users’ experiential factual knowledge and beliefs in a business context, and by aiding in understanding the underlying causes of the misinformation effect. We discuss implications of our findings and directions for future research to address challenges resulting from the misinformation effect.",[],"Browne, Glenn J.",N/A,"Rawls College of Business Administration, Texas Tech University, Lubbock, TX 79409 U.S.A."
https://misq.umn.edu/misq/article/36/1/107/1469/Human-Capital-Development-for-Programmers-Using,MIS Quarterly,Human Capital Development for Programmers Using Open Source Software1,"Volume 36, Issue 1",March 2012,"A firm can upgrade relevant skills of its programmers by ensuring their participation in carefully chosen open source projects. Highly skilled programmers are more valuable for the firm but participating in open source projects reduces the time they spend doing the firm’s projects. This tradeoff determines the optimal extent of programmer participation in open source for the firm. The extent of open source participation may also be influenced by the minimum compensation that must be paid to hire a programmer in the labor market. This is because providing better skills is a way of compensating the programmers by improving their future market value. Hence the firm may want to increase open source participation to keep direct wage payments in check. We develop an analytical model based on optimal control theory to characterize the employment contract that features the best mix of open source participation and wage payments. We also find that the firm benefits more from the presence of open source in a tight labor market (i.e., when programmers have good options besides the employment offered by the firm). On the other hand, programmers are compensated better in the presence of open source opportunities when they have few outside options. This benefit is more for less skilled programmers.",[],"Mehra, Amit",N/A,"Indian School of Business, Hyderabad, India"
https://misq.umn.edu/misq/article/36/1/107/1469/Human-Capital-Development-for-Programmers-Using,MIS Quarterly,Human Capital Development for Programmers Using Open Source Software1,"Volume 36, Issue 1",March 2012,"A firm can upgrade relevant skills of its programmers by ensuring their participation in carefully chosen open source projects. Highly skilled programmers are more valuable for the firm but participating in open source projects reduces the time they spend doing the firm’s projects. This tradeoff determines the optimal extent of programmer participation in open source for the firm. The extent of open source participation may also be influenced by the minimum compensation that must be paid to hire a programmer in the labor market. This is because providing better skills is a way of compensating the programmers by improving their future market value. Hence the firm may want to increase open source participation to keep direct wage payments in check. We develop an analytical model based on optimal control theory to characterize the employment contract that features the best mix of open source participation and wage payments. We also find that the firm benefits more from the presence of open source in a tight labor market (i.e., when programmers have good options besides the employment offered by the firm). On the other hand, programmers are compensated better in the presence of open source opportunities when they have few outside options. This benefit is more for less skilled programmers.",[],"Mookerjee, Vijay",N/A,"School of Management, University of Texas at Dallas, Richardson, TX 75080 U.S.A."
https://misq.umn.edu/misq/article/36/1/123/1473/Revisiting-Bias-Due-to-Construct-Misspecification,MIS Quarterly,Revisiting Bias Due to Construct Misspecification: Different Results from Considering Coefficients in Standardized Form1,"Volume 36, Issue 1",March 2012,"Researchers in a number of disciplines, including Information Systems, have argued that much of past research may have incorrectly specified the relationship between latent variables and indicators as reflective when an understanding of a construct and its measures indicates that a formative specification would have been warranted. Coupled with the posited severe biasing effects of construct misspecification on structural parameters, these two assertions would lead to concluding that an important portion of our literature is largely invalid. While we do not delve into the issue of when one specification should be employed over another, our work here contends that construct misspecification, but with a particular exception, does not lead to severely biased estimates. We argue, and show through extensive simulations, that a lack of attention to the metric in which relationships are expressed is responsible for the current belief in the negative effects of misspecification.",[],"Aguirre-Urreta, Miguel I.",N/A,"School of Accountancy and MIS, College of Commerce, DePaul University, 1 East Jackson Boulevard, Chicago, IL 60604 U.S.A."
https://misq.umn.edu/misq/article/36/1/123/1473/Revisiting-Bias-Due-to-Construct-Misspecification,MIS Quarterly,Revisiting Bias Due to Construct Misspecification: Different Results from Considering Coefficients in Standardized Form1,"Volume 36, Issue 1",March 2012,"Researchers in a number of disciplines, including Information Systems, have argued that much of past research may have incorrectly specified the relationship between latent variables and indicators as reflective when an understanding of a construct and its measures indicates that a formative specification would have been warranted. Coupled with the posited severe biasing effects of construct misspecification on structural parameters, these two assertions would lead to concluding that an important portion of our literature is largely invalid. While we do not delve into the issue of when one specification should be employed over another, our work here contends that construct misspecification, but with a particular exception, does not lead to severely biased estimates. We argue, and show through extensive simulations, that a lack of attention to the metric in which relationships are expressed is responsible for the current belief in the negative effects of misspecification.",[],"Marakas, George M.",N/A,"School of Business, University of Kansas, Summerfield Hall, 1300 Sunnyside Avenue, Lawrence, KS 66045 U.S.A."
https://misq.umn.edu/misq/article/36/1/139/1480/The-Negative-Consequences-of-Measurement-Model,MIS Quarterly,The Negative Consequences of Measurement Model Misspecification: A Response to Aguirre-Urreta and Marakas,"Volume 36, Issue 1",March 2012,N/A,[],"Jarvis, Cheryl Burke",N/A,"College of Business, Southern Illinois University, 1025 Lincoln Drive, Carbondale, IL 62901 U.S.A."
https://misq.umn.edu/misq/article/36/1/139/1480/The-Negative-Consequences-of-Measurement-Model,MIS Quarterly,The Negative Consequences of Measurement Model Misspecification: A Response to Aguirre-Urreta and Marakas,"Volume 36, Issue 1",March 2012,N/A,[],"MacKenzie, Scott B.",N/A,"Kelley School of Business, Indiana University, 1309 East Tenth Street, Bloomington, IN 47405 U.S.A."
https://misq.umn.edu/misq/article/36/1/139/1480/The-Negative-Consequences-of-Measurement-Model,MIS Quarterly,The Negative Consequences of Measurement Model Misspecification: A Response to Aguirre-Urreta and Marakas,"Volume 36, Issue 1",March 2012,N/A,[],"Podsakoff, Philip M.",N/A,"Kelley School of Business, Indiana University, 1309 East Tenth Street, Bloomington, IN 47405 U.S.A."
https://misq.umn.edu/misq/article/36/1/147/1484/The-Critical-Importance-of-Construct-Measurement,MIS Quarterly,The Critical Importance of Construct Measurement Specification: A Response to Aguirre-Urreta and Marakas1,"Volume 36, Issue 1",March 2012,"Aguirre-Urreta and Marakas (A&M) suggest in their simulation “Revisiting Bias Due to Construct Misspecification: Different Results from Considering Coefficients in Standardized Form,” that, like Jarvis et al. (2003), MacKenzie et al. (2005), and Petter et al. (2007) before them, bias does occur when formative constructs are misspecified as reflective. But A&M argue that the level of bias in prior simulation studies has been exaggerated. They parameterize their simulation models using standardized coefficients in contrast to Jarvis et al., MacKenzie et al., and Petter et al., who parameterize their simulation models using unstandardized coefficients. Thus, across these four simulation studies, biases in parameter estimates are likely to result in misspecified measurement models (i.e., using either unstandardized or standardized coefficients); yet, the biases are greater in magnitude when unstandardized coefficients are used to parameterize the misspecified model. We believe that regardless of the extent of the bias, it is critically important for researchers to achieve correspondence between the measurement specification and the conceptual meaning of the construct so as to not alter the theoretical meaning of the construct at the operational layer of the model. Such alignment between theory and measurement will safeguard against threats to construct and statistical conclusion validity.",[],"Petter, Stacie",N/A,"Information Systems & Quantitative Analysis, University of Nebraska at Omaha, 1110 South 67th Street, Omaha, NE 68182-0392 U.S.A."
https://misq.umn.edu/misq/article/36/1/147/1484/The-Critical-Importance-of-Construct-Measurement,MIS Quarterly,The Critical Importance of Construct Measurement Specification: A Response to Aguirre-Urreta and Marakas1,"Volume 36, Issue 1",March 2012,"Aguirre-Urreta and Marakas (A&M) suggest in their simulation “Revisiting Bias Due to Construct Misspecification: Different Results from Considering Coefficients in Standardized Form,” that, like Jarvis et al. (2003), MacKenzie et al. (2005), and Petter et al. (2007) before them, bias does occur when formative constructs are misspecified as reflective. But A&M argue that the level of bias in prior simulation studies has been exaggerated. They parameterize their simulation models using standardized coefficients in contrast to Jarvis et al., MacKenzie et al., and Petter et al., who parameterize their simulation models using unstandardized coefficients. Thus, across these four simulation studies, biases in parameter estimates are likely to result in misspecified measurement models (i.e., using either unstandardized or standardized coefficients); yet, the biases are greater in magnitude when unstandardized coefficients are used to parameterize the misspecified model. We believe that regardless of the extent of the bias, it is critically important for researchers to achieve correspondence between the measurement specification and the conceptual meaning of the construct so as to not alter the theoretical meaning of the construct at the operational layer of the model. Such alignment between theory and measurement will safeguard against threats to construct and statistical conclusion validity.",[],"Rai, Arun",N/A,"Center for Process Innovation and Department of Computer Information Systems, Robinson College of Business, Georgia State University, Atlanta, GA 30303 U.S.A."
https://misq.umn.edu/misq/article/36/1/147/1484/The-Critical-Importance-of-Construct-Measurement,MIS Quarterly,The Critical Importance of Construct Measurement Specification: A Response to Aguirre-Urreta and Marakas1,"Volume 36, Issue 1",March 2012,"Aguirre-Urreta and Marakas (A&M) suggest in their simulation “Revisiting Bias Due to Construct Misspecification: Different Results from Considering Coefficients in Standardized Form,” that, like Jarvis et al. (2003), MacKenzie et al. (2005), and Petter et al. (2007) before them, bias does occur when formative constructs are misspecified as reflective. But A&M argue that the level of bias in prior simulation studies has been exaggerated. They parameterize their simulation models using standardized coefficients in contrast to Jarvis et al., MacKenzie et al., and Petter et al., who parameterize their simulation models using unstandardized coefficients. Thus, across these four simulation studies, biases in parameter estimates are likely to result in misspecified measurement models (i.e., using either unstandardized or standardized coefficients); yet, the biases are greater in magnitude when unstandardized coefficients are used to parameterize the misspecified model. We believe that regardless of the extent of the bias, it is critically important for researchers to achieve correspondence between the measurement specification and the conceptual meaning of the construct so as to not alter the theoretical meaning of the construct at the operational layer of the model. Such alignment between theory and measurement will safeguard against threats to construct and statistical conclusion validity.",[],"Straub, Detmar",N/A,"Center for Process Innovation and Department of Computer Information Systems, Robinson College of Business, Georgia State University, Atlanta, GA 30303 U.S.A."
https://misq.umn.edu/misq/article/36/1/157/1470/Consumer-Acceptance-and-Use-of-Information,MIS Quarterly,Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology1,"Volume 36, Issue 1",March 2012,"This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences—namely, age, gender, and experience—are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.",[],"Venkatesh, Viswanath",N/A,"Sam M. Walton College of Business, University of Arkansas, Fayetteville, AR 72701 U.S.A."
https://misq.umn.edu/misq/article/36/1/157/1470/Consumer-Acceptance-and-Use-of-Information,MIS Quarterly,Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology1,"Volume 36, Issue 1",March 2012,"This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences—namely, age, gender, and experience—are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.",[],"Thong, James Y. L.",N/A,"Department of ISOM, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong"
https://misq.umn.edu/misq/article/36/1/157/1470/Consumer-Acceptance-and-Use-of-Information,MIS Quarterly,Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology1,"Volume 36, Issue 1",March 2012,"This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences—namely, age, gender, and experience—are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.",[],"Xu, Xin",N/A,"Department of Management and Marketing, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong"
https://misq.umn.edu/misq/article/36/1/179/1458/The-Consequences-of-Information-Technology-Control,MIS Quarterly,The Consequences of Information Technology Control Weaknesses on Management Information Systems: The Case of Sarbanes–Oxley Internal Control Reports1,"Volume 36, Issue 1",March 2012,"In this article, the association between the strength of information technology controls over management information systems and the subsequent forecasting ability of the information produced by those systems is investigated. The Sarbanes–Oxley Act of 2002 highlights the importance of information system controls by requiring management and auditors to report on the effectiveness of internal controls over the financial reporting component of the firm’s management information systems. We hypothesize and find evidence that management forecasts are less accurate for firms with information technology material weaknesses in their financial reporting system than the forecasts for firms that do not have information technology material weaknesses. In addition, we examine three dimensions of information technology material weaknesses: data processing integrity, system access and security, and system structure and usage. We find that the association with forecast accuracy appears to be strongest for IT control weaknesses most directly related to data processing integrity. Our results support the contention that information technology controls, as a part of the management information system, affect the quality of the information produced by the system. We discuss the complementary nature of our findings to the information and systems quality literature.",[],"Li, Chan",N/A,"Katz Graduate School of Business, University of Pittsburgh, Pittsburgh, PA 15260 U.S.A."
https://misq.umn.edu/misq/article/36/1/179/1458/The-Consequences-of-Information-Technology-Control,MIS Quarterly,The Consequences of Information Technology Control Weaknesses on Management Information Systems: The Case of Sarbanes–Oxley Internal Control Reports1,"Volume 36, Issue 1",March 2012,"In this article, the association between the strength of information technology controls over management information systems and the subsequent forecasting ability of the information produced by those systems is investigated. The Sarbanes–Oxley Act of 2002 highlights the importance of information system controls by requiring management and auditors to report on the effectiveness of internal controls over the financial reporting component of the firm’s management information systems. We hypothesize and find evidence that management forecasts are less accurate for firms with information technology material weaknesses in their financial reporting system than the forecasts for firms that do not have information technology material weaknesses. In addition, we examine three dimensions of information technology material weaknesses: data processing integrity, system access and security, and system structure and usage. We find that the association with forecast accuracy appears to be strongest for IT control weaknesses most directly related to data processing integrity. Our results support the contention that information technology controls, as a part of the management information system, affect the quality of the information produced by the system. We discuss the complementary nature of our findings to the information and systems quality literature.",[],"Peters, Gary F.",N/A,"Sam M. Walton College of Business, University of Arkansas, Fayetteville, AR 72701 U.S.A."
https://misq.umn.edu/misq/article/36/1/179/1458/The-Consequences-of-Information-Technology-Control,MIS Quarterly,The Consequences of Information Technology Control Weaknesses on Management Information Systems: The Case of Sarbanes–Oxley Internal Control Reports1,"Volume 36, Issue 1",March 2012,"In this article, the association between the strength of information technology controls over management information systems and the subsequent forecasting ability of the information produced by those systems is investigated. The Sarbanes–Oxley Act of 2002 highlights the importance of information system controls by requiring management and auditors to report on the effectiveness of internal controls over the financial reporting component of the firm’s management information systems. We hypothesize and find evidence that management forecasts are less accurate for firms with information technology material weaknesses in their financial reporting system than the forecasts for firms that do not have information technology material weaknesses. In addition, we examine three dimensions of information technology material weaknesses: data processing integrity, system access and security, and system structure and usage. We find that the association with forecast accuracy appears to be strongest for IT control weaknesses most directly related to data processing integrity. Our results support the contention that information technology controls, as a part of the management information system, affect the quality of the information produced by the system. We discuss the complementary nature of our findings to the information and systems quality literature.",[],"Richardson, Vernon J.",N/A,"Sam M. Walton College of Business, University of Arkansas, Fayetteville, AR 72701 U.S.A."
https://misq.umn.edu/misq/article/36/1/179/1458/The-Consequences-of-Information-Technology-Control,MIS Quarterly,The Consequences of Information Technology Control Weaknesses on Management Information Systems: The Case of Sarbanes–Oxley Internal Control Reports1,"Volume 36, Issue 1",March 2012,"In this article, the association between the strength of information technology controls over management information systems and the subsequent forecasting ability of the information produced by those systems is investigated. The Sarbanes–Oxley Act of 2002 highlights the importance of information system controls by requiring management and auditors to report on the effectiveness of internal controls over the financial reporting component of the firm’s management information systems. We hypothesize and find evidence that management forecasts are less accurate for firms with information technology material weaknesses in their financial reporting system than the forecasts for firms that do not have information technology material weaknesses. In addition, we examine three dimensions of information technology material weaknesses: data processing integrity, system access and security, and system structure and usage. We find that the association with forecast accuracy appears to be strongest for IT control weaknesses most directly related to data processing integrity. Our results support the contention that information technology controls, as a part of the management information system, affect the quality of the information produced by the system. We discuss the complementary nature of our findings to the information and systems quality literature.",[],"Watson, Marcia Weidenmier",N/A,"Richard C. Adkerson School of Accountancy, Mississippi State University, Mississippi State, MS 39762 U.S.A."
https://misq.umn.edu/misq/article/36/1/205/1490/Information-Technology-and-Firm-Profitability,MIS Quarterly,Information Technology and Firm Profitability: Mechanisms and Empirical Evidence1,"Volume 36, Issue 1",March 2012,"Do information technology investments improve firm profitability? If so, is this effect because such investments help improve sales, or is it because they help reduce overall operating expenses? How does the effect of IT on profitability compare with that of advertising and of research and development? These are important questions because investments in IT constitute a large part of firms’ discretionary expenditures, and managers need to understand the likely impacts and mechanisms to justify and realize value from their IT and related resource allocation processes. The empirical evidence in this paper, derived using archival data from 1998 to 2003 for more than 400 global firms, suggests that IT has a positive impact on profitability. Importantly, the effect of IT investments on sales and profitability is higher than that of other discretionary investments, such as advertising and R&D. A significant portion of the impact of IT on firm profitability is accounted for by IT-enabled revenue growth, but there is no evidence for the effect of IT on profitability through operating cost reduction. Taken together, these findings suggest that firms have had greater success in achieving higher profitability through IT-enabled revenue growth than through IT-enabled cost reduction. They also provide important implications for managers to make allocations among discretionary expenditures such as IT, advertising, and R&D. With regard to IT expenditures, the results imply that firms should accord higher priority to IT projects that have revenue growth potential over those that focus mainly on cost savings.",[],"Mithas, Sunil",N/A,"Robert H. Smith School of Business, University of Maryland, Van Munching Hall, College Park, MD 20742 U.S.A."
https://misq.umn.edu/misq/article/36/1/205/1490/Information-Technology-and-Firm-Profitability,MIS Quarterly,Information Technology and Firm Profitability: Mechanisms and Empirical Evidence1,"Volume 36, Issue 1",March 2012,"Do information technology investments improve firm profitability? If so, is this effect because such investments help improve sales, or is it because they help reduce overall operating expenses? How does the effect of IT on profitability compare with that of advertising and of research and development? These are important questions because investments in IT constitute a large part of firms’ discretionary expenditures, and managers need to understand the likely impacts and mechanisms to justify and realize value from their IT and related resource allocation processes. The empirical evidence in this paper, derived using archival data from 1998 to 2003 for more than 400 global firms, suggests that IT has a positive impact on profitability. Importantly, the effect of IT investments on sales and profitability is higher than that of other discretionary investments, such as advertising and R&D. A significant portion of the impact of IT on firm profitability is accounted for by IT-enabled revenue growth, but there is no evidence for the effect of IT on profitability through operating cost reduction. Taken together, these findings suggest that firms have had greater success in achieving higher profitability through IT-enabled revenue growth than through IT-enabled cost reduction. They also provide important implications for managers to make allocations among discretionary expenditures such as IT, advertising, and R&D. With regard to IT expenditures, the results imply that firms should accord higher priority to IT projects that have revenue growth potential over those that focus mainly on cost savings.",[],"Tafti, Ali",N/A,"College of Business, University of Illinois at Urbana–Champaign, Champaign, IL 61820 U.S.A."
https://misq.umn.edu/misq/article/36/1/205/1490/Information-Technology-and-Firm-Profitability,MIS Quarterly,Information Technology and Firm Profitability: Mechanisms and Empirical Evidence1,"Volume 36, Issue 1",March 2012,"Do information technology investments improve firm profitability? If so, is this effect because such investments help improve sales, or is it because they help reduce overall operating expenses? How does the effect of IT on profitability compare with that of advertising and of research and development? These are important questions because investments in IT constitute a large part of firms’ discretionary expenditures, and managers need to understand the likely impacts and mechanisms to justify and realize value from their IT and related resource allocation processes. The empirical evidence in this paper, derived using archival data from 1998 to 2003 for more than 400 global firms, suggests that IT has a positive impact on profitability. Importantly, the effect of IT investments on sales and profitability is higher than that of other discretionary investments, such as advertising and R&D. A significant portion of the impact of IT on firm profitability is accounted for by IT-enabled revenue growth, but there is no evidence for the effect of IT on profitability through operating cost reduction. Taken together, these findings suggest that firms have had greater success in achieving higher profitability through IT-enabled revenue growth than through IT-enabled cost reduction. They also provide important implications for managers to make allocations among discretionary expenditures such as IT, advertising, and R&D. With regard to IT expenditures, the results imply that firms should accord higher priority to IT projects that have revenue growth potential over those that focus mainly on cost savings.",[],"Bardhan, Indranil",N/A,"Naveen Jindal School of Management, University of Texas at Dallas, 800 West Campbell Road, Richardson, TX 75080 U.S.A."
https://misq.umn.edu/misq/article/36/1/205/1490/Information-Technology-and-Firm-Profitability,MIS Quarterly,Information Technology and Firm Profitability: Mechanisms and Empirical Evidence1,"Volume 36, Issue 1",March 2012,"Do information technology investments improve firm profitability? If so, is this effect because such investments help improve sales, or is it because they help reduce overall operating expenses? How does the effect of IT on profitability compare with that of advertising and of research and development? These are important questions because investments in IT constitute a large part of firms’ discretionary expenditures, and managers need to understand the likely impacts and mechanisms to justify and realize value from their IT and related resource allocation processes. The empirical evidence in this paper, derived using archival data from 1998 to 2003 for more than 400 global firms, suggests that IT has a positive impact on profitability. Importantly, the effect of IT investments on sales and profitability is higher than that of other discretionary investments, such as advertising and R&D. A significant portion of the impact of IT on firm profitability is accounted for by IT-enabled revenue growth, but there is no evidence for the effect of IT on profitability through operating cost reduction. Taken together, these findings suggest that firms have had greater success in achieving higher profitability through IT-enabled revenue growth than through IT-enabled cost reduction. They also provide important implications for managers to make allocations among discretionary expenditures such as IT, advertising, and R&D. With regard to IT expenditures, the results imply that firms should accord higher priority to IT projects that have revenue growth potential over those that focus mainly on cost savings.",[],"Goh, Jie Mein",N/A,"IE Business School, Maria de Molina, 12 - 5, 28006 Madrid Spain"
https://misq.umn.edu/misq/article/36/1/225/1462/Cocreating-IT-Value-New-Capabilities-and-Metrics,MIS Quarterly,Cocreating IT Value: New Capabilities and Metrics for Multifirm Environments,"Volume 36, Issue 1",March 2012,"Most research on IT value has been from the vantage point of a single firm. Multifirm studies have largely been dyadic and emphasize transaction costs over cocreation of value. Contemporary environments involve IT investments being made by multiple companies in cooperative, platform-based, and relational arrangements where the objective is to cocreate value. If IT serves as a tool, an output, or is instrumental in generating this cocreated value, then it falls within the cocreation domain of this special issue. In this introductory article, we frame the discussion of cocreating IT value through four layers of relational arrangement between firms, describe the papers in the special issue with respect to this framework, and briefly describe an agenda for research in this important area.",[],"Grover, Varun",N/A,"Department of Management, Clemson University, Clemson, SC 29634 U.S.A."
https://misq.umn.edu/misq/article/36/1/225/1462/Cocreating-IT-Value-New-Capabilities-and-Metrics,MIS Quarterly,Cocreating IT Value: New Capabilities and Metrics for Multifirm Environments,"Volume 36, Issue 1",March 2012,"Most research on IT value has been from the vantage point of a single firm. Multifirm studies have largely been dyadic and emphasize transaction costs over cocreation of value. Contemporary environments involve IT investments being made by multiple companies in cooperative, platform-based, and relational arrangements where the objective is to cocreate value. If IT serves as a tool, an output, or is instrumental in generating this cocreated value, then it falls within the cocreation domain of this special issue. In this introductory article, we frame the discussion of cocreating IT value through four layers of relational arrangement between firms, describe the papers in the special issue with respect to this framework, and briefly describe an agenda for research in this important area.",[],"Kohli, Rajiv",N/A,"Mason School of Business, The College of William & Mary, Williamsburg, VA 23187-8795 U.S.A."
https://misq.umn.edu/misq/article/36/1/iii/1460/Editor-s-CommentsA-Critical-Look-at-the-Use-of-PLS,MIS Quarterly,Editor’s CommentsA Critical Look at the Use of PLS-SEM in MIS Quarterly,"Volume 36, Issue 1",March 2012,N/A,[],"Ringle, Christian M.",N/A,Hamburg University of Technology (TUHH) and University of Newcastle (Australia)
https://misq.umn.edu/misq/article/36/1/iii/1460/Editor-s-CommentsA-Critical-Look-at-the-Use-of-PLS,MIS Quarterly,Editor’s CommentsA Critical Look at the Use of PLS-SEM in MIS Quarterly,"Volume 36, Issue 1",March 2012,N/A,[],"Sarstedt, Marko",N/A,Ludwig-Maximilians-University Munich and University of Newcastle (Australia)
https://misq.umn.edu/misq/article/36/1/iii/1460/Editor-s-CommentsA-Critical-Look-at-the-Use-of-PLS,MIS Quarterly,Editor’s CommentsA Critical Look at the Use of PLS-SEM in MIS Quarterly,"Volume 36, Issue 1",March 2012,N/A,[],"Straub, Detmar W.",N/A,Georgia State University
https://misq.umn.edu/misq/article/36/1/233/1477/Interfirm-II-Capability-Profiles-and,MIS Quarterly,Interfirm II Capability Profiles and Communications for Cocreating Relational Value: Evidence from the Logistics Industry1,"Volume 36, Issue 1",March 2012,"This study seeks to identify the means by which information technology helps cocreate relational value in the context of interfirm relationships in the logistics industry—a large and information-intensive industry. We identify a set of IT functionalities—single-location shipping, multilocation shipping, supply chain visibility, and financial settlement—that can be used to manage the flows of physical goods, information, and finances across locations in interfirm logistics processes. Progressively more advanced sets of IT functionalities, when implemented and used in the interfirm relationship to execute logistics processes, are proposed to form four distinct IT capability profiles of increased sophistication. Interfirm IT capability profiles of higher sophistication are proposed to help cocreate greater relational value by facilitating the flows of physical goods, information, and finances across locations in the interfirm logistics process. Besides their direct role in helping cocreate relational value, these interfirm IT capability profiles are proposed to further enhance relational value cocreation when complemented by interfirm communications for business development and IT development.Our empirical study was situated in one of the world’s largest logistics suppliers and over 2,000 of its interfirm relationships with buyers across industries. Integrated data from four archival sources on the IT functionalities implemented and used in interfirm logistics relationships, interfirm communications, relational value (share of wallet and loyalty), and multiple control variables were collected. The results show that the proposed interfirm IT capability profiles and interfirm communications have both a direct and an interaction effect on relational value. Implications for cocreating relational value in interfirm relationships with the aid of IT are discussed.",[],"Rai, Arun",N/A,"Center for Process Innovation and Department of Computer Information Systems, Robinson College of Business, Georgia State University, Atlanta, GA 30303 U.S.A."
https://misq.umn.edu/misq/article/36/1/233/1477/Interfirm-II-Capability-Profiles-and,MIS Quarterly,Interfirm II Capability Profiles and Communications for Cocreating Relational Value: Evidence from the Logistics Industry1,"Volume 36, Issue 1",March 2012,"This study seeks to identify the means by which information technology helps cocreate relational value in the context of interfirm relationships in the logistics industry—a large and information-intensive industry. We identify a set of IT functionalities—single-location shipping, multilocation shipping, supply chain visibility, and financial settlement—that can be used to manage the flows of physical goods, information, and finances across locations in interfirm logistics processes. Progressively more advanced sets of IT functionalities, when implemented and used in the interfirm relationship to execute logistics processes, are proposed to form four distinct IT capability profiles of increased sophistication. Interfirm IT capability profiles of higher sophistication are proposed to help cocreate greater relational value by facilitating the flows of physical goods, information, and finances across locations in the interfirm logistics process. Besides their direct role in helping cocreate relational value, these interfirm IT capability profiles are proposed to further enhance relational value cocreation when complemented by interfirm communications for business development and IT development.Our empirical study was situated in one of the world’s largest logistics suppliers and over 2,000 of its interfirm relationships with buyers across industries. Integrated data from four archival sources on the IT functionalities implemented and used in interfirm logistics relationships, interfirm communications, relational value (share of wallet and loyalty), and multiple control variables were collected. The results show that the proposed interfirm IT capability profiles and interfirm communications have both a direct and an interaction effect on relational value. Implications for cocreating relational value in interfirm relationships with the aid of IT are discussed.",[],"Pavlou, Paul A.",N/A,"Fox School of Business, Temple University, Philadelphia, PA 19122 U.S.A."
https://misq.umn.edu/misq/article/36/1/233/1477/Interfirm-II-Capability-Profiles-and,MIS Quarterly,Interfirm II Capability Profiles and Communications for Cocreating Relational Value: Evidence from the Logistics Industry1,"Volume 36, Issue 1",March 2012,"This study seeks to identify the means by which information technology helps cocreate relational value in the context of interfirm relationships in the logistics industry—a large and information-intensive industry. We identify a set of IT functionalities—single-location shipping, multilocation shipping, supply chain visibility, and financial settlement—that can be used to manage the flows of physical goods, information, and finances across locations in interfirm logistics processes. Progressively more advanced sets of IT functionalities, when implemented and used in the interfirm relationship to execute logistics processes, are proposed to form four distinct IT capability profiles of increased sophistication. Interfirm IT capability profiles of higher sophistication are proposed to help cocreate greater relational value by facilitating the flows of physical goods, information, and finances across locations in the interfirm logistics process. Besides their direct role in helping cocreate relational value, these interfirm IT capability profiles are proposed to further enhance relational value cocreation when complemented by interfirm communications for business development and IT development.Our empirical study was situated in one of the world’s largest logistics suppliers and over 2,000 of its interfirm relationships with buyers across industries. Integrated data from four archival sources on the IT functionalities implemented and used in interfirm logistics relationships, interfirm communications, relational value (share of wallet and loyalty), and multiple control variables were collected. The results show that the proposed interfirm IT capability profiles and interfirm communications have both a direct and an interaction effect on relational value. Implications for cocreating relational value in interfirm relationships with the aid of IT are discussed.",[],"Im, Ghiyoung",N/A,"Computer Information Systems, College of Business, University of Louisville, Louisville, KY 40292 U.S.A."
https://misq.umn.edu/misq/article/36/1/233/1477/Interfirm-II-Capability-Profiles-and,MIS Quarterly,Interfirm II Capability Profiles and Communications for Cocreating Relational Value: Evidence from the Logistics Industry1,"Volume 36, Issue 1",March 2012,"This study seeks to identify the means by which information technology helps cocreate relational value in the context of interfirm relationships in the logistics industry—a large and information-intensive industry. We identify a set of IT functionalities—single-location shipping, multilocation shipping, supply chain visibility, and financial settlement—that can be used to manage the flows of physical goods, information, and finances across locations in interfirm logistics processes. Progressively more advanced sets of IT functionalities, when implemented and used in the interfirm relationship to execute logistics processes, are proposed to form four distinct IT capability profiles of increased sophistication. Interfirm IT capability profiles of higher sophistication are proposed to help cocreate greater relational value by facilitating the flows of physical goods, information, and finances across locations in the interfirm logistics process. Besides their direct role in helping cocreate relational value, these interfirm IT capability profiles are proposed to further enhance relational value cocreation when complemented by interfirm communications for business development and IT development.Our empirical study was situated in one of the world’s largest logistics suppliers and over 2,000 of its interfirm relationships with buyers across industries. Integrated data from four archival sources on the IT functionalities implemented and used in interfirm logistics relationships, interfirm communications, relational value (share of wallet and loyalty), and multiple control variables were collected. The results show that the proposed interfirm IT capability profiles and interfirm communications have both a direct and an interaction effect on relational value. Implications for cocreating relational value in interfirm relationships with the aid of IT are discussed.",[],"Du, Steve",N/A,"Center for Process Innovation, Robinson College of Business, Georgia State University, Atlanta, GA 30303 U.S.A."
https://misq.umn.edu/misq/article/36/1/263/1487/Cocreation-of-Value-in-a-Platform-Ecosystem-The,MIS Quarterly,Cocreation of Value in a Platform Ecosystem: The Case of Enterprise Software1,"Volume 36, Issue 1",March 2012,"It has been argued that platform technology owners cocreate business value with other firms in their platform ecosystems by encouraging complementary invention and exploiting indirect network effects. In this study, we examine whether participation in an ecosystem partnership improves the business performance of small independent software vendors (ISVs) in the enterprise software industry and how appropriability mechanisms influence the benefits of partnership. By analyzing the partnering activities and performance indicators of a sample of 1,210 small ISVs over the period 1996–2004, we find that joining a major platform owner’s platform ecosystem is associated with an increase in sales and a greater likelihood of issuing an initial public offering (IPO). In addition, we show that these impacts are greater when ISVs have greater intellectual property rights or stronger downstream capabilities. This research highlights the value of interoperability between software products, and stresses that value cocreation and appropriation are not mutually exclusive strategies in inter-firm collaboration.",[],"Ceccagnoli, Marco",N/A,"College of Management, Georgia Institute of Technology, Atlanta, GA 30332 U.S.A."
https://misq.umn.edu/misq/article/36/1/263/1487/Cocreation-of-Value-in-a-Platform-Ecosystem-The,MIS Quarterly,Cocreation of Value in a Platform Ecosystem: The Case of Enterprise Software1,"Volume 36, Issue 1",March 2012,"It has been argued that platform technology owners cocreate business value with other firms in their platform ecosystems by encouraging complementary invention and exploiting indirect network effects. In this study, we examine whether participation in an ecosystem partnership improves the business performance of small independent software vendors (ISVs) in the enterprise software industry and how appropriability mechanisms influence the benefits of partnership. By analyzing the partnering activities and performance indicators of a sample of 1,210 small ISVs over the period 1996–2004, we find that joining a major platform owner’s platform ecosystem is associated with an increase in sales and a greater likelihood of issuing an initial public offering (IPO). In addition, we show that these impacts are greater when ISVs have greater intellectual property rights or stronger downstream capabilities. This research highlights the value of interoperability between software products, and stresses that value cocreation and appropriation are not mutually exclusive strategies in inter-firm collaboration.",[],"Forman, Chris",N/A,"College of Management, Georgia Institute of Technology, Atlanta, GA 30332 U.S.A."
https://misq.umn.edu/misq/article/36/1/263/1487/Cocreation-of-Value-in-a-Platform-Ecosystem-The,MIS Quarterly,Cocreation of Value in a Platform Ecosystem: The Case of Enterprise Software1,"Volume 36, Issue 1",March 2012,"It has been argued that platform technology owners cocreate business value with other firms in their platform ecosystems by encouraging complementary invention and exploiting indirect network effects. In this study, we examine whether participation in an ecosystem partnership improves the business performance of small independent software vendors (ISVs) in the enterprise software industry and how appropriability mechanisms influence the benefits of partnership. By analyzing the partnering activities and performance indicators of a sample of 1,210 small ISVs over the period 1996–2004, we find that joining a major platform owner’s platform ecosystem is associated with an increase in sales and a greater likelihood of issuing an initial public offering (IPO). In addition, we show that these impacts are greater when ISVs have greater intellectual property rights or stronger downstream capabilities. This research highlights the value of interoperability between software products, and stresses that value cocreation and appropriation are not mutually exclusive strategies in inter-firm collaboration.",[],"Huang, Peng",N/A,"Robert H. Smith School of Business, University of Maryland, College Park, MD 20742 U.S.A."
https://misq.umn.edu/misq/article/36/1/263/1487/Cocreation-of-Value-in-a-Platform-Ecosystem-The,MIS Quarterly,Cocreation of Value in a Platform Ecosystem: The Case of Enterprise Software1,"Volume 36, Issue 1",March 2012,"It has been argued that platform technology owners cocreate business value with other firms in their platform ecosystems by encouraging complementary invention and exploiting indirect network effects. In this study, we examine whether participation in an ecosystem partnership improves the business performance of small independent software vendors (ISVs) in the enterprise software industry and how appropriability mechanisms influence the benefits of partnership. By analyzing the partnering activities and performance indicators of a sample of 1,210 small ISVs over the period 1996–2004, we find that joining a major platform owner’s platform ecosystem is associated with an increase in sales and a greater likelihood of issuing an initial public offering (IPO). In addition, we show that these impacts are greater when ISVs have greater intellectual property rights or stronger downstream capabilities. This research highlights the value of interoperability between software products, and stresses that value cocreation and appropriation are not mutually exclusive strategies in inter-firm collaboration.",[],"Wu, D. J.",N/A,"College of Management, Georgia Institute of Technology, Atlanta, GA 30332 U.S.A."
https://misq.umn.edu/misq/article/36/1/291/1481/Value-Cocreation-and-Wealth-Spillover-in-Open,MIS Quarterly,Value Cocreation and Wealth Spillover in Open Innovation Alliances1,"Volume 36, Issue 1",March 2012,"In this study, we investigate the economic and strategic value of open innovation alliances (OIAs), in which collaborators and competitors integrate in the pursuit of the codevelopment of technological innovations. Given that OIAs differ substantially from traditional, closed alliances in many aspects, including their strategic scope and scale, governing mechanisms, and member composition, it is important to understand and assess the potential value inherent in these new modes of collaboration. Furthermore, OIAs evolve over time as the participating members are free to enter and leave at will. Therefore, we also examine the on-going value creation and wealth spillover that result from changes in membership. Moreover, we investigate how a firm’s participation in an IT-based open alliance alters the market value of its rivals operating within the same marketplace. To gain additional insight into the factors that moderate the market valuation of OIA participation, several contextual factors, including the degree of partner heterogeneity, innovation type, and degree of openness of the OIAs are used to account for variability in abnormal returns. Based on 194 observations, we found that allying firms realize significant positive abnormal returns when their entry into an OIA is made public. The results also suggest that substantial excessive returns accrue to the allying firms with the belated entry of a market leader firm. Furthermore, we discovered that a firm’s entry into an OIA increases, rather than decreases, the market valuation of its rivals. Interestingly, an incumbent rival that did not participate in the alliance appears to gain greater “free-riding” benefits from the OIA, as compared to peer rivals. Innovation type and openness were significantly associated with the amount of abnormal returns accruing to allying firms, while no significance was found for partner heterogeneity. Finally, we conclude with a discussion of the implications of our findings for research and practice with respect to value cocreation in multifirm environments.",[],"Han, Kunsoo",N/A,"Desautels Faculty of Management, McGill University, 1001 Sherbrooke Street West, Montreal, QC Canada H3A 1G5"
https://misq.umn.edu/misq/article/36/1/291/1481/Value-Cocreation-and-Wealth-Spillover-in-Open,MIS Quarterly,Value Cocreation and Wealth Spillover in Open Innovation Alliances1,"Volume 36, Issue 1",March 2012,"In this study, we investigate the economic and strategic value of open innovation alliances (OIAs), in which collaborators and competitors integrate in the pursuit of the codevelopment of technological innovations. Given that OIAs differ substantially from traditional, closed alliances in many aspects, including their strategic scope and scale, governing mechanisms, and member composition, it is important to understand and assess the potential value inherent in these new modes of collaboration. Furthermore, OIAs evolve over time as the participating members are free to enter and leave at will. Therefore, we also examine the on-going value creation and wealth spillover that result from changes in membership. Moreover, we investigate how a firm’s participation in an IT-based open alliance alters the market value of its rivals operating within the same marketplace. To gain additional insight into the factors that moderate the market valuation of OIA participation, several contextual factors, including the degree of partner heterogeneity, innovation type, and degree of openness of the OIAs are used to account for variability in abnormal returns. Based on 194 observations, we found that allying firms realize significant positive abnormal returns when their entry into an OIA is made public. The results also suggest that substantial excessive returns accrue to the allying firms with the belated entry of a market leader firm. Furthermore, we discovered that a firm’s entry into an OIA increases, rather than decreases, the market valuation of its rivals. Interestingly, an incumbent rival that did not participate in the alliance appears to gain greater “free-riding” benefits from the OIA, as compared to peer rivals. Innovation type and openness were significantly associated with the amount of abnormal returns accruing to allying firms, while no significance was found for partner heterogeneity. Finally, we conclude with a discussion of the implications of our findings for research and practice with respect to value cocreation in multifirm environments.",[],"Oh, Wonseok",N/A,"School of Business, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul, Korea"
https://misq.umn.edu/misq/article/36/1/291/1481/Value-Cocreation-and-Wealth-Spillover-in-Open,MIS Quarterly,Value Cocreation and Wealth Spillover in Open Innovation Alliances1,"Volume 36, Issue 1",March 2012,"In this study, we investigate the economic and strategic value of open innovation alliances (OIAs), in which collaborators and competitors integrate in the pursuit of the codevelopment of technological innovations. Given that OIAs differ substantially from traditional, closed alliances in many aspects, including their strategic scope and scale, governing mechanisms, and member composition, it is important to understand and assess the potential value inherent in these new modes of collaboration. Furthermore, OIAs evolve over time as the participating members are free to enter and leave at will. Therefore, we also examine the on-going value creation and wealth spillover that result from changes in membership. Moreover, we investigate how a firm’s participation in an IT-based open alliance alters the market value of its rivals operating within the same marketplace. To gain additional insight into the factors that moderate the market valuation of OIA participation, several contextual factors, including the degree of partner heterogeneity, innovation type, and degree of openness of the OIAs are used to account for variability in abnormal returns. Based on 194 observations, we found that allying firms realize significant positive abnormal returns when their entry into an OIA is made public. The results also suggest that substantial excessive returns accrue to the allying firms with the belated entry of a market leader firm. Furthermore, we discovered that a firm’s entry into an OIA increases, rather than decreases, the market valuation of its rivals. Interestingly, an incumbent rival that did not participate in the alliance appears to gain greater “free-riding” benefits from the OIA, as compared to peer rivals. Innovation type and openness were significantly associated with the amount of abnormal returns accruing to allying firms, while no significance was found for partner heterogeneity. Finally, we conclude with a discussion of the implications of our findings for research and practice with respect to value cocreation in multifirm environments.",[],"Im, Kun Shin",N/A,"School of Business, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul, Korea"
https://misq.umn.edu/misq/article/36/1/291/1481/Value-Cocreation-and-Wealth-Spillover-in-Open,MIS Quarterly,Value Cocreation and Wealth Spillover in Open Innovation Alliances1,"Volume 36, Issue 1",March 2012,"In this study, we investigate the economic and strategic value of open innovation alliances (OIAs), in which collaborators and competitors integrate in the pursuit of the codevelopment of technological innovations. Given that OIAs differ substantially from traditional, closed alliances in many aspects, including their strategic scope and scale, governing mechanisms, and member composition, it is important to understand and assess the potential value inherent in these new modes of collaboration. Furthermore, OIAs evolve over time as the participating members are free to enter and leave at will. Therefore, we also examine the on-going value creation and wealth spillover that result from changes in membership. Moreover, we investigate how a firm’s participation in an IT-based open alliance alters the market value of its rivals operating within the same marketplace. To gain additional insight into the factors that moderate the market valuation of OIA participation, several contextual factors, including the degree of partner heterogeneity, innovation type, and degree of openness of the OIAs are used to account for variability in abnormal returns. Based on 194 observations, we found that allying firms realize significant positive abnormal returns when their entry into an OIA is made public. The results also suggest that substantial excessive returns accrue to the allying firms with the belated entry of a market leader firm. Furthermore, we discovered that a firm’s entry into an OIA increases, rather than decreases, the market valuation of its rivals. Interestingly, an incumbent rival that did not participate in the alliance appears to gain greater “free-riding” benefits from the OIA, as compared to peer rivals. Innovation type and openness were significantly associated with the amount of abnormal returns accruing to allying firms, while no significance was found for partner heterogeneity. Finally, we conclude with a discussion of the implications of our findings for research and practice with respect to value cocreation in multifirm environments.",[],"Chang, Ray M.",N/A,"Living Analytics Research Center, School of Information Systems, Singapore Management University, 80 Stamford Road, Singapore 178902 Singapore"
https://misq.umn.edu/misq/article/36/1/291/1481/Value-Cocreation-and-Wealth-Spillover-in-Open,MIS Quarterly,Value Cocreation and Wealth Spillover in Open Innovation Alliances1,"Volume 36, Issue 1",March 2012,"In this study, we investigate the economic and strategic value of open innovation alliances (OIAs), in which collaborators and competitors integrate in the pursuit of the codevelopment of technological innovations. Given that OIAs differ substantially from traditional, closed alliances in many aspects, including their strategic scope and scale, governing mechanisms, and member composition, it is important to understand and assess the potential value inherent in these new modes of collaboration. Furthermore, OIAs evolve over time as the participating members are free to enter and leave at will. Therefore, we also examine the on-going value creation and wealth spillover that result from changes in membership. Moreover, we investigate how a firm’s participation in an IT-based open alliance alters the market value of its rivals operating within the same marketplace. To gain additional insight into the factors that moderate the market valuation of OIA participation, several contextual factors, including the degree of partner heterogeneity, innovation type, and degree of openness of the OIAs are used to account for variability in abnormal returns. Based on 194 observations, we found that allying firms realize significant positive abnormal returns when their entry into an OIA is made public. The results also suggest that substantial excessive returns accrue to the allying firms with the belated entry of a market leader firm. Furthermore, we discovered that a firm’s entry into an OIA increases, rather than decreases, the market valuation of its rivals. Interestingly, an incumbent rival that did not participate in the alliance appears to gain greater “free-riding” benefits from the OIA, as compared to peer rivals. Innovation type and openness were significantly associated with the amount of abnormal returns accruing to allying firms, while no significance was found for partner heterogeneity. Finally, we conclude with a discussion of the implications of our findings for research and practice with respect to value cocreation in multifirm environments.",[],"Oh, Hyelim",N/A,"Desautels Faculty of Management, McGill University, 1001 Sherbrooke Street West, Montreal, QC Canada H3A 1G5"
https://misq.umn.edu/misq/article/36/1/291/1481/Value-Cocreation-and-Wealth-Spillover-in-Open,MIS Quarterly,Value Cocreation and Wealth Spillover in Open Innovation Alliances1,"Volume 36, Issue 1",March 2012,"In this study, we investigate the economic and strategic value of open innovation alliances (OIAs), in which collaborators and competitors integrate in the pursuit of the codevelopment of technological innovations. Given that OIAs differ substantially from traditional, closed alliances in many aspects, including their strategic scope and scale, governing mechanisms, and member composition, it is important to understand and assess the potential value inherent in these new modes of collaboration. Furthermore, OIAs evolve over time as the participating members are free to enter and leave at will. Therefore, we also examine the on-going value creation and wealth spillover that result from changes in membership. Moreover, we investigate how a firm’s participation in an IT-based open alliance alters the market value of its rivals operating within the same marketplace. To gain additional insight into the factors that moderate the market valuation of OIA participation, several contextual factors, including the degree of partner heterogeneity, innovation type, and degree of openness of the OIAs are used to account for variability in abnormal returns. Based on 194 observations, we found that allying firms realize significant positive abnormal returns when their entry into an OIA is made public. The results also suggest that substantial excessive returns accrue to the allying firms with the belated entry of a market leader firm. Furthermore, we discovered that a firm’s entry into an OIA increases, rather than decreases, the market valuation of its rivals. Interestingly, an incumbent rival that did not participate in the alliance appears to gain greater “free-riding” benefits from the OIA, as compared to peer rivals. Innovation type and openness were significantly associated with the amount of abnormal returns accruing to allying firms, while no significance was found for partner heterogeneity. Finally, we conclude with a discussion of the implications of our findings for research and practice with respect to value cocreation in multifirm environments.",[],"Pinsonneault, Alain",N/A,"Desautels Faculty of Management, McGill University, 1001 Sherbrooke Street West, Montreal, QC Canada H3A 1G5"
https://misq.umn.edu/misq/article/36/1/317/1466/Exploring-Value-Cocreation-in-Relationships,MIS Quarterly,Exploring Value Cocreation in Relationships Between an ERP Vendor and Its Partners: A Revelatory Case Study1,"Volume 36, Issue 1",March 2012,"Contemporary business organizations are increasingly turning their attention to jointly creating value with a variety of stakeholders, such as individual customers and other business organizations. However, a review of the literature reveals that very few studies have systematically examined value cocreation within business-to-business (B2B) contexts. Using a revelatory case study of the relationship between an ERP vendor with a global reputation and its partners, and informed by the resource-based view of the firm and related theoretical perspectives, we develop an understanding of value cocreation in B2B alliances associated with selling, extending, and implementing packaged software, specifically ERP systems. Our study reveals that there are different mechanisms underlying value cocreation within B2B alliances, and also points to several categories of contingency factors that influence these mechanisms. In addition to providing insights about the phenomenon of cocreation itself, the study contributes to the stream of packaged software literature, where the implications of value cocreation in alliances between packaged software vendors and their partners for the client organizations have not been sufficiently explored.",[],"Sarker, Suprateek",N/A,"College of Business, Washington State University, Pullman, WA 99164-4570 U.S.A."
https://misq.umn.edu/misq/article/36/1/317/1466/Exploring-Value-Cocreation-in-Relationships,MIS Quarterly,Exploring Value Cocreation in Relationships Between an ERP Vendor and Its Partners: A Revelatory Case Study1,"Volume 36, Issue 1",March 2012,"Contemporary business organizations are increasingly turning their attention to jointly creating value with a variety of stakeholders, such as individual customers and other business organizations. However, a review of the literature reveals that very few studies have systematically examined value cocreation within business-to-business (B2B) contexts. Using a revelatory case study of the relationship between an ERP vendor with a global reputation and its partners, and informed by the resource-based view of the firm and related theoretical perspectives, we develop an understanding of value cocreation in B2B alliances associated with selling, extending, and implementing packaged software, specifically ERP systems. Our study reveals that there are different mechanisms underlying value cocreation within B2B alliances, and also points to several categories of contingency factors that influence these mechanisms. In addition to providing insights about the phenomenon of cocreation itself, the study contributes to the stream of packaged software literature, where the implications of value cocreation in alliances between packaged software vendors and their partners for the client organizations have not been sufficiently explored.",[],"Sarker, Saonee",N/A,"College of Business, Washington State University, Pullman, WA 99164-4570 U.S.A."
https://misq.umn.edu/misq/article/36/1/317/1466/Exploring-Value-Cocreation-in-Relationships,MIS Quarterly,Exploring Value Cocreation in Relationships Between an ERP Vendor and Its Partners: A Revelatory Case Study1,"Volume 36, Issue 1",March 2012,"Contemporary business organizations are increasingly turning their attention to jointly creating value with a variety of stakeholders, such as individual customers and other business organizations. However, a review of the literature reveals that very few studies have systematically examined value cocreation within business-to-business (B2B) contexts. Using a revelatory case study of the relationship between an ERP vendor with a global reputation and its partners, and informed by the resource-based view of the firm and related theoretical perspectives, we develop an understanding of value cocreation in B2B alliances associated with selling, extending, and implementing packaged software, specifically ERP systems. Our study reveals that there are different mechanisms underlying value cocreation within B2B alliances, and also points to several categories of contingency factors that influence these mechanisms. In addition to providing insights about the phenomenon of cocreation itself, the study contributes to the stream of packaged software literature, where the implications of value cocreation in alliances between packaged software vendors and their partners for the client organizations have not been sufficiently explored.",[],"Sahaym, Arvin",N/A,"College of Business, Washington State University, Pullman, WA 99164-4570 U.S.A."
https://misq.umn.edu/misq/article/36/1/317/1466/Exploring-Value-Cocreation-in-Relationships,MIS Quarterly,Exploring Value Cocreation in Relationships Between an ERP Vendor and Its Partners: A Revelatory Case Study1,"Volume 36, Issue 1",March 2012,"Contemporary business organizations are increasingly turning their attention to jointly creating value with a variety of stakeholders, such as individual customers and other business organizations. However, a review of the literature reveals that very few studies have systematically examined value cocreation within business-to-business (B2B) contexts. Using a revelatory case study of the relationship between an ERP vendor with a global reputation and its partners, and informed by the resource-based view of the firm and related theoretical perspectives, we develop an understanding of value cocreation in B2B alliances associated with selling, extending, and implementing packaged software, specifically ERP systems. Our study reveals that there are different mechanisms underlying value cocreation within B2B alliances, and also points to several categories of contingency factors that influence these mechanisms. In addition to providing insights about the phenomenon of cocreation itself, the study contributes to the stream of packaged software literature, where the implications of value cocreation in alliances between packaged software vendors and their partners for the client organizations have not been sufficiently explored.",[],"Bjørn-Andersen, Niels",N/A,"Department of IT Management, Copenhagen Business School, DK-2000 Frederiksberg Denmark"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/36/2/339/556/Open-Knowledge-Creation-Bringing-Transparency-and,MIS Quarterly,Open Knowledge Creation: Bringing Transparency and Inclusiveness to the Peer Review Process1,"Volume 36, Issue 2",June 2012,"The peer review process that has been in place for many years has recognized shortcomings. The Internet provides a means for changing this process. This paper offers a more transparent and inclusive design for peer review referred to as open knowledge creation. The design proposed utilizes Google knol and group services. The open knowledge creation design consists of four stages: creation, review/revision, evaluation/adoption, and publication. It is intended to offer existing or new journals an alternative to the traditional peer review of research.",[],"Hardaway, Donald E.",N/A,"Decision Science and Information Technology Management, John Cook School of Business and Administration, Saint Louis University, St. Louis, MO 63108 U.S.A."
https://misq.umn.edu/misq/article/36/2/339/556/Open-Knowledge-Creation-Bringing-Transparency-and,MIS Quarterly,Open Knowledge Creation: Bringing Transparency and Inclusiveness to the Peer Review Process1,"Volume 36, Issue 2",June 2012,"The peer review process that has been in place for many years has recognized shortcomings. The Internet provides a means for changing this process. This paper offers a more transparent and inclusive design for peer review referred to as open knowledge creation. The design proposed utilizes Google knol and group services. The open knowledge creation design consists of four stages: creation, review/revision, evaluation/adoption, and publication. It is intended to offer existing or new journals an alternative to the traditional peer review of research.",[],"Scamell, Richard W.",N/A,"Decision and Information Sciences, C. T. Bauer College of Business, University of Houston, Houston, TX 77204-6282 U.S.A."
https://misq.umn.edu/misq/article/36/2/347/576/Style-Composition-in-Action-Research-Publication1,MIS Quarterly,Style Composition in Action Research Publication1,"Volume 36, Issue 2",June 2012,"Examining action research publications in leading Information Systems journals as a particular genre of research communication, we develop the notion of style composition to understand how authors structure their arguments for a research contribution. We define style composition as the activity through which authors select, emphasize, and present elements of their research to establish premises, develop inferences, and present contributions in publications. Drawing on this general notion, we identify a set of styles that is characteristic of how IS action researchers compose their argument. Premise styles relate to the dual goals of action research through practical or theoretical positioning of the argument; inference styles combine insights from the problem-solving and the research cycles through inductive or deductive reasoning; and contribution styles focus on different types of contributions—experience report, field study, theoretical development, problem-solving method, and research method. Based on the considered sample, we analyze the styles adopted in selected publications and show that authors have favored certain styles while leaving others underexplored; further, we reveal important strengths and weaknesses in the composition of styles within the IS discipline. Based on these insights, we discuss how action research practices and writing can be improved, as well as how to further develop style compositions to support the publication of engaged scholarship research.",[],"Mathiassen, Lars",N/A,"Computer Information Systems, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA 30303 U.S.A."
https://misq.umn.edu/misq/article/36/2/347/576/Style-Composition-in-Action-Research-Publication1,MIS Quarterly,Style Composition in Action Research Publication1,"Volume 36, Issue 2",June 2012,"Examining action research publications in leading Information Systems journals as a particular genre of research communication, we develop the notion of style composition to understand how authors structure their arguments for a research contribution. We define style composition as the activity through which authors select, emphasize, and present elements of their research to establish premises, develop inferences, and present contributions in publications. Drawing on this general notion, we identify a set of styles that is characteristic of how IS action researchers compose their argument. Premise styles relate to the dual goals of action research through practical or theoretical positioning of the argument; inference styles combine insights from the problem-solving and the research cycles through inductive or deductive reasoning; and contribution styles focus on different types of contributions—experience report, field study, theoretical development, problem-solving method, and research method. Based on the considered sample, we analyze the styles adopted in selected publications and show that authors have favored certain styles while leaving others underexplored; further, we reveal important strengths and weaknesses in the composition of styles within the IS discipline. Based on these insights, we discuss how action research practices and writing can be improved, as well as how to further develop style compositions to support the publication of engaged scholarship research.",[],"Chiasson, Mike",N/A,"Management Science, Management School, Lancaster University, Lancaster, United Kingdom"
https://misq.umn.edu/misq/article/36/2/347/576/Style-Composition-in-Action-Research-Publication1,MIS Quarterly,Style Composition in Action Research Publication1,"Volume 36, Issue 2",June 2012,"Examining action research publications in leading Information Systems journals as a particular genre of research communication, we develop the notion of style composition to understand how authors structure their arguments for a research contribution. We define style composition as the activity through which authors select, emphasize, and present elements of their research to establish premises, develop inferences, and present contributions in publications. Drawing on this general notion, we identify a set of styles that is characteristic of how IS action researchers compose their argument. Premise styles relate to the dual goals of action research through practical or theoretical positioning of the argument; inference styles combine insights from the problem-solving and the research cycles through inductive or deductive reasoning; and contribution styles focus on different types of contributions—experience report, field study, theoretical development, problem-solving method, and research method. Based on the considered sample, we analyze the styles adopted in selected publications and show that authors have favored certain styles while leaving others underexplored; further, we reveal important strengths and weaknesses in the composition of styles within the IS discipline. Based on these insights, we discuss how action research practices and writing can be improved, as well as how to further develop style compositions to support the publication of engaged scholarship research.",[],"Germonprez, Matt",N/A,"Information Systems & Quantitative Analysis, College of Information Science and Technology, University of Nebraska at Omaha, Omaha, NE 68182-0392 U.S.A."
https://misq.umn.edu/misq/article/36/2/365/582/Can-Online-Wait-Be-Managed-The-Effect-of-Filler,MIS Quarterly,Can Online Wait Be Managed? The Effect of Filler Interfaces and Presentation Modes on Perceived Waiting Time Online1,"Volume 36, Issue 2",June 2012,"Long waits online undermine users’ evaluations of Web sites and their providers, triggering abandonment behaviors. Yet e-business researchers and practitioners have not perfected mechanisms to respond to online wait issues. A filler interface that runs during the wait for search results may influence online users’ perceived waiting time (PWT); however, no scientific investigation has attempted to design effective filler interfaces for managing online waits. By adopting resource allocation theory, cognitive absorption theory, and human computer interaction (HCI) theories (competition for attention, visual search, and motion effect), we design diverse filler interfaces and investigate their effects on antecedents of PWT. The proposed research model considers cognitive absorption factors such as temporal dissociation, focused immersion, and heightened enjoyment as antecedents of PWT, which in turn triggers three outcomes: affective appraisals, cognitive appraisals, and Web site use intention. A multistage, multimethod approach is used to test the research hypotheses. In the first stage, we compare a filler interface condition with a no-filler interface condition, and find the superiority of a filler interface with respect to inducing focused immersion and temporal dissociation. In the second stage, we conduct two controlled experiments to examine whether filler interfaces with various designs (varying the presence and relevance of image, text, and image motion) distinctly influence antecedents of PWT and confirm their distinctive effects on focused immersion, temporal dissociation, and heightened enjoyment. In addition, by conducting a structural equation modeling analysis, we find that our research model explains 51 percent, 51 percent, 44 percent, and 45 percent of the variance in PWT, affective appraisals, cognitive appraisals, and Web site use intention respectively. Theoretical and practical implications of these findings are provided.",[],"Lee, Younghwa",N/A,"Department of Management, College of Business Administration, University of Northern Iowa, Cedar Falls, IA 50614 U.S.A."
https://misq.umn.edu/misq/article/36/2/365/582/Can-Online-Wait-Be-Managed-The-Effect-of-Filler,MIS Quarterly,Can Online Wait Be Managed? The Effect of Filler Interfaces and Presentation Modes on Perceived Waiting Time Online1,"Volume 36, Issue 2",June 2012,"Long waits online undermine users’ evaluations of Web sites and their providers, triggering abandonment behaviors. Yet e-business researchers and practitioners have not perfected mechanisms to respond to online wait issues. A filler interface that runs during the wait for search results may influence online users’ perceived waiting time (PWT); however, no scientific investigation has attempted to design effective filler interfaces for managing online waits. By adopting resource allocation theory, cognitive absorption theory, and human computer interaction (HCI) theories (competition for attention, visual search, and motion effect), we design diverse filler interfaces and investigate their effects on antecedents of PWT. The proposed research model considers cognitive absorption factors such as temporal dissociation, focused immersion, and heightened enjoyment as antecedents of PWT, which in turn triggers three outcomes: affective appraisals, cognitive appraisals, and Web site use intention. A multistage, multimethod approach is used to test the research hypotheses. In the first stage, we compare a filler interface condition with a no-filler interface condition, and find the superiority of a filler interface with respect to inducing focused immersion and temporal dissociation. In the second stage, we conduct two controlled experiments to examine whether filler interfaces with various designs (varying the presence and relevance of image, text, and image motion) distinctly influence antecedents of PWT and confirm their distinctive effects on focused immersion, temporal dissociation, and heightened enjoyment. In addition, by conducting a structural equation modeling analysis, we find that our research model explains 51 percent, 51 percent, 44 percent, and 45 percent of the variance in PWT, affective appraisals, cognitive appraisals, and Web site use intention respectively. Theoretical and practical implications of these findings are provided.",[],"Chen, Andrew N. K.",N/A,"Accounting and Information Systems, School of Business, University of Kansas, 1300 Sunnyside Avenue, Lawrence, KS 66045 U.S.A."
https://misq.umn.edu/misq/article/36/2/365/582/Can-Online-Wait-Be-Managed-The-Effect-of-Filler,MIS Quarterly,Can Online Wait Be Managed? The Effect of Filler Interfaces and Presentation Modes on Perceived Waiting Time Online1,"Volume 36, Issue 2",June 2012,"Long waits online undermine users’ evaluations of Web sites and their providers, triggering abandonment behaviors. Yet e-business researchers and practitioners have not perfected mechanisms to respond to online wait issues. A filler interface that runs during the wait for search results may influence online users’ perceived waiting time (PWT); however, no scientific investigation has attempted to design effective filler interfaces for managing online waits. By adopting resource allocation theory, cognitive absorption theory, and human computer interaction (HCI) theories (competition for attention, visual search, and motion effect), we design diverse filler interfaces and investigate their effects on antecedents of PWT. The proposed research model considers cognitive absorption factors such as temporal dissociation, focused immersion, and heightened enjoyment as antecedents of PWT, which in turn triggers three outcomes: affective appraisals, cognitive appraisals, and Web site use intention. A multistage, multimethod approach is used to test the research hypotheses. In the first stage, we compare a filler interface condition with a no-filler interface condition, and find the superiority of a filler interface with respect to inducing focused immersion and temporal dissociation. In the second stage, we conduct two controlled experiments to examine whether filler interfaces with various designs (varying the presence and relevance of image, text, and image motion) distinctly influence antecedents of PWT and confirm their distinctive effects on focused immersion, temporal dissociation, and heightened enjoyment. In addition, by conducting a structural equation modeling analysis, we find that our research model explains 51 percent, 51 percent, 44 percent, and 45 percent of the variance in PWT, affective appraisals, cognitive appraisals, and Web site use intention respectively. Theoretical and practical implications of these findings are provided.",[],"Ilie, Virginia",N/A,"School of Management, California Luthern University, 60 West Olsen Road, Thousand Oaks, CA 91360 U.S.A."
https://misq.umn.edu/misq/article/36/2/395/578/On-Product-Uncertainty-in-Online-Markets-Theory,MIS Quarterly,On Product Uncertainty in Online Markets: Theory and Evidence1,"Volume 36, Issue 2",June 2012,N/A,[],"Dimoka, Angelika",N/A,"Fox School of Business, Temple University, Philadelphia, PA 19122 U.S.A."
https://misq.umn.edu/misq/article/36/2/395/578/On-Product-Uncertainty-in-Online-Markets-Theory,MIS Quarterly,On Product Uncertainty in Online Markets: Theory and Evidence1,"Volume 36, Issue 2",June 2012,N/A,[],"Hong, Yili",N/A,"Fox School of Business, Temple University, Philadelphia, PA 19122 U.S.A."
https://misq.umn.edu/misq/article/36/2/395/578/On-Product-Uncertainty-in-Online-Markets-Theory,MIS Quarterly,On Product Uncertainty in Online Markets: Theory and Evidence1,"Volume 36, Issue 2",June 2012,N/A,[],"Pavlou, Paul A.",N/A,"Fox School of Business, Temple University, Philadelphia, PA 19122 U.S.A."
https://misq.umn.edu/misq/article/36/2/427/554/The-Career-Paths-Less-or-More-Traveled-A-Sequence,MIS Quarterly,"The Career Paths Less (or More) Traveled: A Sequence Analysis of IT Career Histories, Mobility Patterns, and Career Success1","Volume 36, Issue 2",June 2012,"This paper examines the objective career histories, mobility patterns, and career success of 500 individuals drawn from the National Longitudinal Survey of Youth (NLSY79), who had worked in the information technology workforce. Sequence analysis of career histories shows that careers of the IT workforce are more diverse than the traditional view of a dual IT career path (technical versus managerial). This study reveals a new career typology comprising three broad, distinct paths: IT careers; professional labor market (PLM) careers; and secondary labor market (SLM) careers. Of the 500 individuals in the IT workforce, 173 individuals pursued IT careers while the remaining 327 individuals left IT for other high-status non-IT professional jobs in PLM or lower-status, non-IT jobs in SLM careers. Findings from this study contribute to refining the concept of “boundaryless” careers. By tracing the diverse trajectories of career mobility, we enrich our understanding of how individuals construct boundaryless careers that span not only organizational but also occupational boundaries. Career success did not differ in terms of average pay for individuals in IT and PLM careers. By contrast, individuals in SLM careers attained the lowest pay. We conclude this study with implications for future research and for the management of IT professionals’ careers.",[],"Joseph, Damien",N/A,"Nanyang Business School, Nanyang Technological University, Nanyang Avenue, Singapore 639798"
https://misq.umn.edu/misq/article/36/2/427/554/The-Career-Paths-Less-or-More-Traveled-A-Sequence,MIS Quarterly,"The Career Paths Less (or More) Traveled: A Sequence Analysis of IT Career Histories, Mobility Patterns, and Career Success1","Volume 36, Issue 2",June 2012,"This paper examines the objective career histories, mobility patterns, and career success of 500 individuals drawn from the National Longitudinal Survey of Youth (NLSY79), who had worked in the information technology workforce. Sequence analysis of career histories shows that careers of the IT workforce are more diverse than the traditional view of a dual IT career path (technical versus managerial). This study reveals a new career typology comprising three broad, distinct paths: IT careers; professional labor market (PLM) careers; and secondary labor market (SLM) careers. Of the 500 individuals in the IT workforce, 173 individuals pursued IT careers while the remaining 327 individuals left IT for other high-status non-IT professional jobs in PLM or lower-status, non-IT jobs in SLM careers. Findings from this study contribute to refining the concept of “boundaryless” careers. By tracing the diverse trajectories of career mobility, we enrich our understanding of how individuals construct boundaryless careers that span not only organizational but also occupational boundaries. Career success did not differ in terms of average pay for individuals in IT and PLM careers. By contrast, individuals in SLM careers attained the lowest pay. We conclude this study with implications for future research and for the management of IT professionals’ careers.",[],"Boh, Wai Fong",N/A,"Nanyang Business School, Nanyang Technological University, Nanyang Avenue, Singapore 639798"
https://misq.umn.edu/misq/article/36/2/427/554/The-Career-Paths-Less-or-More-Traveled-A-Sequence,MIS Quarterly,"The Career Paths Less (or More) Traveled: A Sequence Analysis of IT Career Histories, Mobility Patterns, and Career Success1","Volume 36, Issue 2",June 2012,"This paper examines the objective career histories, mobility patterns, and career success of 500 individuals drawn from the National Longitudinal Survey of Youth (NLSY79), who had worked in the information technology workforce. Sequence analysis of career histories shows that careers of the IT workforce are more diverse than the traditional view of a dual IT career path (technical versus managerial). This study reveals a new career typology comprising three broad, distinct paths: IT careers; professional labor market (PLM) careers; and secondary labor market (SLM) careers. Of the 500 individuals in the IT workforce, 173 individuals pursued IT careers while the remaining 327 individuals left IT for other high-status non-IT professional jobs in PLM or lower-status, non-IT jobs in SLM careers. Findings from this study contribute to refining the concept of “boundaryless” careers. By tracing the diverse trajectories of career mobility, we enrich our understanding of how individuals construct boundaryless careers that span not only organizational but also occupational boundaries. Career success did not differ in terms of average pay for individuals in IT and PLM careers. By contrast, individuals in SLM careers attained the lowest pay. We conclude this study with implications for future research and for the management of IT professionals’ careers.",[],"Ang, Soon",N/A,"Nanyang Business School, Nanyang Technological University, Nanyang Avenue, Singapore 639798"
https://misq.umn.edu/misq/article/36/2/427/554/The-Career-Paths-Less-or-More-Traveled-A-Sequence,MIS Quarterly,"The Career Paths Less (or More) Traveled: A Sequence Analysis of IT Career Histories, Mobility Patterns, and Career Success1","Volume 36, Issue 2",June 2012,"This paper examines the objective career histories, mobility patterns, and career success of 500 individuals drawn from the National Longitudinal Survey of Youth (NLSY79), who had worked in the information technology workforce. Sequence analysis of career histories shows that careers of the IT workforce are more diverse than the traditional view of a dual IT career path (technical versus managerial). This study reveals a new career typology comprising three broad, distinct paths: IT careers; professional labor market (PLM) careers; and secondary labor market (SLM) careers. Of the 500 individuals in the IT workforce, 173 individuals pursued IT careers while the remaining 327 individuals left IT for other high-status non-IT professional jobs in PLM or lower-status, non-IT jobs in SLM careers. Findings from this study contribute to refining the concept of “boundaryless” careers. By tracing the diverse trajectories of career mobility, we enrich our understanding of how individuals construct boundaryless careers that span not only organizational but also occupational boundaries. Career success did not differ in terms of average pay for individuals in IT and PLM careers. By contrast, individuals in SLM careers attained the lowest pay. We conclude this study with implications for future research and for the management of IT professionals’ careers.",[],"Slaughter, Sandra A.",N/A,"College of Management, Georgia Institute of Technology, Atlanta, GA 30303-0520 U.S.A."
https://misq.umn.edu/misq/article/36/2/453/557/Understanding-User-Revisions-When-Using,MIS Quarterly,Understanding User Revisions When Using Information System Features: Adaptive System Use and Triggers1,"Volume 36, Issue 2",June 2012,"Post-adoptive system use is often characterized by cycles of adaptation, in which people actively revise how they use information systems. This paper investigates how and why individual users revise their system use at the feature level. A new concept, adaptive system use (ASU), is conceptualized as a user’s revisions of which and how system features are used. This research identifies four specific ASU behaviors that collectively describe how people revise their use of system features. A model of ASU is developed based on Louis and Sutton’s (1991) research on how people switch to active thinking from automatic thinking. The model specifies three antecedents of ASU (novel situations, discrepancies, and deliberate initiatives) and two moderators (personal innovativeness in IT and facilitating conditions). An empirical study of 253 Microsoft Office users largely supported the research model. The findings suggest that triggers―including novel situations, discrepancies, and deliberate initiatives―are a significant impetus to ASU. This research also confirms moderating effects of personal innovativeness in IT. The findings also show the relationships among triggers: in addition to their direct impact on ASU, novel situations and deliberate initiatives exert their influence on ASU indirectly by giving rise to discrepancies in system use. Moreover, a cluster analysis identifies three heterogeneous triggering conditions and reveals that people engage in different ASU behaviors under different triggering conditions.",[],"Sun, Heshan",N/A,"School of Information Resources and Library Science, University of Arizona, 1515 East First Street, Tucson, AZ 85719 U.S.A."
https://misq.umn.edu/misq/article/36/2/479/563/A-Cost-Based-Database-Request-Distribution,MIS Quarterly,A Cost-Based Database Request Distribution Technique for Online E-Commerce Applications1,"Volume 36, Issue 2",June 2012,"E-commerce is growing to represent an increasing share of overall sales revenue, and online sales are expected to continue growing for the foreseeable future. This growth translates into increased activity on the supporting infrastructure, leading to a corresponding need to scale the infrastructure. This is difficult in an era of shrinking budgets and increasing functional requirements. Increasingly, IT managers are turning to virtualized cloud providers, drawn by the pay-for-use business model. As cloud computing becomes more popular, it is important for data center managers to accomplish more with fewer dollars (i.e., to increase the utilization of existing resources). Advanced request distribution techniques can help ensure both high utilization and smart request distribution, where requests are sent to the service resources best able to handle them. While such request distribution techniques have been applied to the web and application layers of the traditional online application architecture, request distribution techniques for the data layer have focused primarily on online transaction processing scenarios. However, online applications often have a significant read-intensive workload, where read operations constitute a significant percentage of workloads (up to 95 percent or higher). In this paper, we propose a cost-based database request distribution (C-DBRD) strategy, a policy to distribute requests, across a cluster of commercial, off-the-shelf databases, and discuss its implementation. We first develop the intuition behind our approach, and describe a high-level architecture for database request distribution. We then develop a theoretical model for database load computation, which we use to design a method for database request distribution and build a software implementation. Finally, following a design science methodology, we evaluate our artifacts through experimental evaluation. Our experiments, in the lab and in production-scale systems, show significant improvement of database layer resource utilization, demonstrating up to a 45 percent improvement over existing request distribution techniques.",[],"VanderMeer, Debra",N/A,"College of Business, Florida International University, Miami, FL 33199 U.S.A."
https://misq.umn.edu/misq/article/36/2/479/563/A-Cost-Based-Database-Request-Distribution,MIS Quarterly,A Cost-Based Database Request Distribution Technique for Online E-Commerce Applications1,"Volume 36, Issue 2",June 2012,"E-commerce is growing to represent an increasing share of overall sales revenue, and online sales are expected to continue growing for the foreseeable future. This growth translates into increased activity on the supporting infrastructure, leading to a corresponding need to scale the infrastructure. This is difficult in an era of shrinking budgets and increasing functional requirements. Increasingly, IT managers are turning to virtualized cloud providers, drawn by the pay-for-use business model. As cloud computing becomes more popular, it is important for data center managers to accomplish more with fewer dollars (i.e., to increase the utilization of existing resources). Advanced request distribution techniques can help ensure both high utilization and smart request distribution, where requests are sent to the service resources best able to handle them. While such request distribution techniques have been applied to the web and application layers of the traditional online application architecture, request distribution techniques for the data layer have focused primarily on online transaction processing scenarios. However, online applications often have a significant read-intensive workload, where read operations constitute a significant percentage of workloads (up to 95 percent or higher). In this paper, we propose a cost-based database request distribution (C-DBRD) strategy, a policy to distribute requests, across a cluster of commercial, off-the-shelf databases, and discuss its implementation. We first develop the intuition behind our approach, and describe a high-level architecture for database request distribution. We then develop a theoretical model for database load computation, which we use to design a method for database request distribution and build a software implementation. Finally, following a design science methodology, we evaluate our artifacts through experimental evaluation. Our experiments, in the lab and in production-scale systems, show significant improvement of database layer resource utilization, demonstrating up to a 45 percent improvement over existing request distribution techniques.",[],"Dutta, Kaushik",N/A,"Department of Information Systems, National University of Singapore, Singapore"
https://misq.umn.edu/misq/article/36/2/479/563/A-Cost-Based-Database-Request-Distribution,MIS Quarterly,A Cost-Based Database Request Distribution Technique for Online E-Commerce Applications1,"Volume 36, Issue 2",June 2012,"E-commerce is growing to represent an increasing share of overall sales revenue, and online sales are expected to continue growing for the foreseeable future. This growth translates into increased activity on the supporting infrastructure, leading to a corresponding need to scale the infrastructure. This is difficult in an era of shrinking budgets and increasing functional requirements. Increasingly, IT managers are turning to virtualized cloud providers, drawn by the pay-for-use business model. As cloud computing becomes more popular, it is important for data center managers to accomplish more with fewer dollars (i.e., to increase the utilization of existing resources). Advanced request distribution techniques can help ensure both high utilization and smart request distribution, where requests are sent to the service resources best able to handle them. While such request distribution techniques have been applied to the web and application layers of the traditional online application architecture, request distribution techniques for the data layer have focused primarily on online transaction processing scenarios. However, online applications often have a significant read-intensive workload, where read operations constitute a significant percentage of workloads (up to 95 percent or higher). In this paper, we propose a cost-based database request distribution (C-DBRD) strategy, a policy to distribute requests, across a cluster of commercial, off-the-shelf databases, and discuss its implementation. We first develop the intuition behind our approach, and describe a high-level architecture for database request distribution. We then develop a theoretical model for database load computation, which we use to design a method for database request distribution and build a software implementation. Finally, following a design science methodology, we evaluate our artifacts through experimental evaluation. Our experiments, in the lab and in production-scale systems, show significant improvement of database layer resource utilization, demonstrating up to a 45 percent improvement over existing request distribution techniques.",[],"Datta, Anindya",N/A,"Department of Information Systems, National University of Singapore, Singapore"
https://misq.umn.edu/misq/article/36/2/509/567/Efficiency-or-Innovation-How-do-Industry,MIS Quarterly,Efficiency or Innovation: How do Industry Environments Moderate the Effects of Firms’ It Asset Portfolios?1,"Volume 36, Issue 2",June 2012,"Firms invest in a variety of information technologies and seek to align their IT asset portfolios with two key performance outcomes: efficiency and innovation. Existing research makes the universalistic assumption that both outcomes will always be realized through firms’ IT asset portfolios. There has been limited research on the conditions under which firms’ IT asset portfolios should be oriented more toward efficiency or innovation. Here, we argue that the nature of the industry where a firm competes will have a significant moderating effect on the link between firms’ IT asset portfolios and efficiency or innovation outcomes. Using panel data that covers a wide range of industry environments, we find that at lower levels of dynamism, munificence, and complexity, IT asset portfolios are associated with a greater increase in efficiency. In contrast, in environments with higher levels of complexity, IT asset portfolios are associated with a greater increase in innovation (i.e., development of new products and processes, and exploration of growth opportunities). These results provide insights about how firms could realize strategic alignment by tailoring their IT asset portfolios toward an efficiency or innovation focus.",[],"Xue, Ling",N/A,"Department of Operations and Information Management, University of Scranton, Scranton, PA 18503 U.S.A."
https://misq.umn.edu/misq/article/36/2/509/567/Efficiency-or-Innovation-How-do-Industry,MIS Quarterly,Efficiency or Innovation: How do Industry Environments Moderate the Effects of Firms’ It Asset Portfolios?1,"Volume 36, Issue 2",June 2012,"Firms invest in a variety of information technologies and seek to align their IT asset portfolios with two key performance outcomes: efficiency and innovation. Existing research makes the universalistic assumption that both outcomes will always be realized through firms’ IT asset portfolios. There has been limited research on the conditions under which firms’ IT asset portfolios should be oriented more toward efficiency or innovation. Here, we argue that the nature of the industry where a firm competes will have a significant moderating effect on the link between firms’ IT asset portfolios and efficiency or innovation outcomes. Using panel data that covers a wide range of industry environments, we find that at lower levels of dynamism, munificence, and complexity, IT asset portfolios are associated with a greater increase in efficiency. In contrast, in environments with higher levels of complexity, IT asset portfolios are associated with a greater increase in innovation (i.e., development of new products and processes, and exploration of growth opportunities). These results provide insights about how firms could realize strategic alignment by tailoring their IT asset portfolios toward an efficiency or innovation focus.",[],"Ray, Gautam",N/A,"Department of Information and Decision Sciences, Carlson School of Management, University of Minnesota, Minneapolis, MN 55455 U.S.A."
https://misq.umn.edu/misq/article/36/2/509/567/Efficiency-or-Innovation-How-do-Industry,MIS Quarterly,Efficiency or Innovation: How do Industry Environments Moderate the Effects of Firms’ It Asset Portfolios?1,"Volume 36, Issue 2",June 2012,"Firms invest in a variety of information technologies and seek to align their IT asset portfolios with two key performance outcomes: efficiency and innovation. Existing research makes the universalistic assumption that both outcomes will always be realized through firms’ IT asset portfolios. There has been limited research on the conditions under which firms’ IT asset portfolios should be oriented more toward efficiency or innovation. Here, we argue that the nature of the industry where a firm competes will have a significant moderating effect on the link between firms’ IT asset portfolios and efficiency or innovation outcomes. Using panel data that covers a wide range of industry environments, we find that at lower levels of dynamism, munificence, and complexity, IT asset portfolios are associated with a greater increase in efficiency. In contrast, in environments with higher levels of complexity, IT asset portfolios are associated with a greater increase in innovation (i.e., development of new products and processes, and exploration of growth opportunities). These results provide insights about how firms could realize strategic alignment by tailoring their IT asset portfolios toward an efficiency or innovation focus.",[],"Sambamurthy, Vallabh",N/A,"Accounting and Information Systems, Eli Broad College of Business, Michigan State University, East Lansing, MI 48824 U.S.A."
https://misq.umn.edu/misq/article/36/2/529/571/Toward-a-New-Theory-of-the-Contribution-of-the-IT,MIS Quarterly,Toward a New Theory of the Contribution of the IT Function in Organizations1,"Volume 36, Issue 2",June 2012,N/A,[],"Guillemette, Manon G.",N/A,"Faculty of Management, Université de Sherbrooke, 2500 University Boulevard, Sherbrooke, Quebec Canada J1K 2R1"
https://misq.umn.edu/misq/article/36/2/529/571/Toward-a-New-Theory-of-the-Contribution-of-the-IT,MIS Quarterly,Toward a New Theory of the Contribution of the IT Function in Organizations1,"Volume 36, Issue 2",June 2012,N/A,[],"Paré, Guy",N/A,"HEC Montréal, 3000 Côte-Ste-Catherine Road, Montréal, Quebec Canada H3T 2A7"
https://misq.umn.edu/misq/article/36/2/553/572/The-Asymmetric-Benefits-of-Relational-Flexibility,MIS Quarterly,The Asymmetric Benefits of Relational Flexibility: Evidence from Software Development Outsourcing1,"Volume 36, Issue 2",June 2012,"In this paper, the interacting effect of formal contracts and relational governance on vendor profitability and quality in the software outsourcing industry are examined. We focus on a critical manifestation of relational governance—the presence of relational flexibility in the exchange relationship—and argue that the enacted observation of relational flexibility is driven by perceptions of exchange hazards. In a departure from extant literature, however, we propose that the benefits accruing from it are asymmetric and depend on how the exchange risks are apportioned by the formal contract. Formally, we hypothesize that relational flexibility provides greater benefits to an exchange partner that faces the greater proportion of risk in a project, induced through the contract. In addition, we hypothesize that these benefits manifest on the performance dimensions that are of importance to the risk-exposed partner. We test our hypotheses on 105 software projects completed by a software outsourcing vendor for multiple clients. The results show that relational flexibility positively affects profitability in only fixed price contracts, where the vendor faces greater risk, while positively affecting quality only in time and materials contracts, where the client is at greater risk. We thus provide evidence for the asymmetric benefits from relational governance, thereby arguing for a more contingent and limited view of the value of relational governance, based on risk-exposure, rather than the more expansive view prevalent in the literature contending that relational governance provides benefits for all parties to an exchange. We conclude with a discussion of the research and managerial implications of our findings.",[],"Gopal, Anandasivam",N/A,"Robert H. Smith School of Business, University of Maryland, College Park, MD 20742 U.S.A."
https://misq.umn.edu/misq/article/36/2/553/572/The-Asymmetric-Benefits-of-Relational-Flexibility,MIS Quarterly,The Asymmetric Benefits of Relational Flexibility: Evidence from Software Development Outsourcing1,"Volume 36, Issue 2",June 2012,"In this paper, the interacting effect of formal contracts and relational governance on vendor profitability and quality in the software outsourcing industry are examined. We focus on a critical manifestation of relational governance—the presence of relational flexibility in the exchange relationship—and argue that the enacted observation of relational flexibility is driven by perceptions of exchange hazards. In a departure from extant literature, however, we propose that the benefits accruing from it are asymmetric and depend on how the exchange risks are apportioned by the formal contract. Formally, we hypothesize that relational flexibility provides greater benefits to an exchange partner that faces the greater proportion of risk in a project, induced through the contract. In addition, we hypothesize that these benefits manifest on the performance dimensions that are of importance to the risk-exposed partner. We test our hypotheses on 105 software projects completed by a software outsourcing vendor for multiple clients. The results show that relational flexibility positively affects profitability in only fixed price contracts, where the vendor faces greater risk, while positively affecting quality only in time and materials contracts, where the client is at greater risk. We thus provide evidence for the asymmetric benefits from relational governance, thereby arguing for a more contingent and limited view of the value of relational governance, based on risk-exposure, rather than the more expansive view prevalent in the literature contending that relational governance provides benefits for all parties to an exchange. We conclude with a discussion of the research and managerial implications of our findings.",[],"Koka, Balaji R.",N/A,"Jesse H. Jones School of Business, Rice University, Houston, TX 77055 U.S.A."
https://misq.umn.edu/misq/article/36/2/577/580/Enacting-Clan-Control-in-Complex-It-Projects-A,MIS Quarterly,Enacting Clan Control in Complex It Projects: A Social Capital Perspective1,"Volume 36, Issue 2",June 2012,"The information technology project control literature has documented that clan control is often essential in complex multistakeholder projects for project success. However, instituting clan control in such conditions is challenging as people come to a project with diverse skills and backgrounds. There is often insufficient time for clan control to develop naturally. This paper investigates the question, “How can clan control be enacted in complex IT projects?” Recognizing social capital as a resource, we conceptualize a clan as a group with strong social capital (i.e., where its members have developed their structural, cognitive, and relational ties to the point that they share common values and beliefs and are committed to a set of peer norms). We theorize that the enactment of clan control is a dual process of (1) building the clan by developing its social capital dimensions (structural, cognitive, and relational ties) or reappropriating social capital from elsewhere and (2) leveraging the clan by reinforcing project-facilitating shared values, beliefs, and norms, and inhibiting those that impede the achievement of project goals. We explore how clan control was enacted in a large IT project at a major logistics organization in which clan control was quickly instituted to avoid an impending project failure. Our research contributes to theory in three ways: (1) we reconcile the two differing views of clan control into a single framework, (2) we explain the role of controllers in enacting clan control, and (3) we clarify how formal control can be employed to develop clan control.",[],"Chua, Cecil Eng Huang",N/A,"Information Systems and Operations Management Department, University of Auckland Business School, Auckland, New Zealand"
https://misq.umn.edu/misq/article/36/2/577/580/Enacting-Clan-Control-in-Complex-It-Projects-A,MIS Quarterly,Enacting Clan Control in Complex It Projects: A Social Capital Perspective1,"Volume 36, Issue 2",June 2012,"The information technology project control literature has documented that clan control is often essential in complex multistakeholder projects for project success. However, instituting clan control in such conditions is challenging as people come to a project with diverse skills and backgrounds. There is often insufficient time for clan control to develop naturally. This paper investigates the question, “How can clan control be enacted in complex IT projects?” Recognizing social capital as a resource, we conceptualize a clan as a group with strong social capital (i.e., where its members have developed their structural, cognitive, and relational ties to the point that they share common values and beliefs and are committed to a set of peer norms). We theorize that the enactment of clan control is a dual process of (1) building the clan by developing its social capital dimensions (structural, cognitive, and relational ties) or reappropriating social capital from elsewhere and (2) leveraging the clan by reinforcing project-facilitating shared values, beliefs, and norms, and inhibiting those that impede the achievement of project goals. We explore how clan control was enacted in a large IT project at a major logistics organization in which clan control was quickly instituted to avoid an impending project failure. Our research contributes to theory in three ways: (1) we reconcile the two differing views of clan control into a single framework, (2) we explain the role of controllers in enacting clan control, and (3) we clarify how formal control can be employed to develop clan control.",[],"Lim, Wee-Kiat",N/A,"Department of Sociology, University of Colorado at Boulder, Boulder, CO 80309 U.S.A."
https://misq.umn.edu/misq/article/36/2/577/580/Enacting-Clan-Control-in-Complex-It-Projects-A,MIS Quarterly,Enacting Clan Control in Complex It Projects: A Social Capital Perspective1,"Volume 36, Issue 2",June 2012,"The information technology project control literature has documented that clan control is often essential in complex multistakeholder projects for project success. However, instituting clan control in such conditions is challenging as people come to a project with diverse skills and backgrounds. There is often insufficient time for clan control to develop naturally. This paper investigates the question, “How can clan control be enacted in complex IT projects?” Recognizing social capital as a resource, we conceptualize a clan as a group with strong social capital (i.e., where its members have developed their structural, cognitive, and relational ties to the point that they share common values and beliefs and are committed to a set of peer norms). We theorize that the enactment of clan control is a dual process of (1) building the clan by developing its social capital dimensions (structural, cognitive, and relational ties) or reappropriating social capital from elsewhere and (2) leveraging the clan by reinforcing project-facilitating shared values, beliefs, and norms, and inhibiting those that impede the achievement of project goals. We explore how clan control was enacted in a large IT project at a major logistics organization in which clan control was quickly instituted to avoid an impending project failure. Our research contributes to theory in three ways: (1) we reconcile the two differing views of clan control into a single framework, (2) we explain the role of controllers in enacting clan control, and (3) we clarify how formal control can be employed to develop clan control.",[],"Soh, Christina",N/A,"Information Technology and Operations Management Department, Nanyang Business School, Nanyang Technological University, Singapore 639798 Singapore"
https://misq.umn.edu/misq/article/36/2/577/580/Enacting-Clan-Control-in-Complex-It-Projects-A,MIS Quarterly,Enacting Clan Control in Complex It Projects: A Social Capital Perspective1,"Volume 36, Issue 2",June 2012,"The information technology project control literature has documented that clan control is often essential in complex multistakeholder projects for project success. However, instituting clan control in such conditions is challenging as people come to a project with diverse skills and backgrounds. There is often insufficient time for clan control to develop naturally. This paper investigates the question, “How can clan control be enacted in complex IT projects?” Recognizing social capital as a resource, we conceptualize a clan as a group with strong social capital (i.e., where its members have developed their structural, cognitive, and relational ties to the point that they share common values and beliefs and are committed to a set of peer norms). We theorize that the enactment of clan control is a dual process of (1) building the clan by developing its social capital dimensions (structural, cognitive, and relational ties) or reappropriating social capital from elsewhere and (2) leveraging the clan by reinforcing project-facilitating shared values, beliefs, and norms, and inhibiting those that impede the achievement of project goals. We explore how clan control was enacted in a large IT project at a major logistics organization in which clan control was quickly instituted to avoid an impending project failure. Our research contributes to theory in three ways: (1) we reconcile the two differing views of clan control into a single framework, (2) we explain the role of controllers in enacting clan control, and (3) we clarify how formal control can be employed to develop clan control.",[],"Sia, Siew Kien",N/A,"Information Technology and Operations Management Department, Nanyang Business School, Nanyang Technological University, Singapore 639798 Singapore"
https://misq.umn.edu/misq/article/36/2/601/565/IS-Employee-Attitudes-and-Perceptions-at-Varying,MIS Quarterly,IS Employee Attitudes and Perceptions at Varying Levels of Software Process Maturity1,"Volume 36, Issue 2",June 2012,"Taking a control theory view of software process innovation, we tested prevalent beliefs regarding software process maturity and Information Systems employee attitudes and perceptions by surveying 736 IS professionals in 10 organizations at varying levels of the CMM (capability maturity model). Although anecdotal reports and the scant empirical studies to date suggest job attitudes and perceptions are more positive for employees in organizations at higher levels of software process maturity, we found evidence of a more complex picture. While our data supported expectations that role conflict and perceived work overload were lower for IS professionals in organizations at a level of maturity where software process behavioral controls are implemented, other results were not fully in line with prevalent beliefs. Most notably, IS workers reported significantly lower professional efficacy and lower job satisfaction in organizations at CMM Level 3, where behavioral controls are the dominant form of formal control, than in organizations at Level 1, which is relatively free of formal controls. Some anticipated positive attitudes and perceptions surfaced in organizations at the highest rungs of software process maturity (CMM Levels 4/5), where the established behavioral controls are supplemented by substantial outcome controls, as IS professionals reported lower role ambiguity and higher job satisfaction than did their counterparts in organizations at CMM Level 3.",[],"Ply, Janet K.",N/A,"Pendére, Inc., 1805 S. 9th Street, Waco, TX 76706 U.S.A."
https://misq.umn.edu/misq/article/36/2/601/565/IS-Employee-Attitudes-and-Perceptions-at-Varying,MIS Quarterly,IS Employee Attitudes and Perceptions at Varying Levels of Software Process Maturity1,"Volume 36, Issue 2",June 2012,"Taking a control theory view of software process innovation, we tested prevalent beliefs regarding software process maturity and Information Systems employee attitudes and perceptions by surveying 736 IS professionals in 10 organizations at varying levels of the CMM (capability maturity model). Although anecdotal reports and the scant empirical studies to date suggest job attitudes and perceptions are more positive for employees in organizations at higher levels of software process maturity, we found evidence of a more complex picture. While our data supported expectations that role conflict and perceived work overload were lower for IS professionals in organizations at a level of maturity where software process behavioral controls are implemented, other results were not fully in line with prevalent beliefs. Most notably, IS workers reported significantly lower professional efficacy and lower job satisfaction in organizations at CMM Level 3, where behavioral controls are the dominant form of formal control, than in organizations at Level 1, which is relatively free of formal controls. Some anticipated positive attitudes and perceptions surfaced in organizations at the highest rungs of software process maturity (CMM Levels 4/5), where the established behavioral controls are supplemented by substantial outcome controls, as IS professionals reported lower role ambiguity and higher job satisfaction than did their counterparts in organizations at CMM Level 3.",[],"Moore, Jo Ellen",N/A,"Southern Illinois University Edwardsville, Campus Box 1106, Edwardsville, IL 62026-1106 U.S.A."
https://misq.umn.edu/misq/article/36/2/601/565/IS-Employee-Attitudes-and-Perceptions-at-Varying,MIS Quarterly,IS Employee Attitudes and Perceptions at Varying Levels of Software Process Maturity1,"Volume 36, Issue 2",June 2012,"Taking a control theory view of software process innovation, we tested prevalent beliefs regarding software process maturity and Information Systems employee attitudes and perceptions by surveying 736 IS professionals in 10 organizations at varying levels of the CMM (capability maturity model). Although anecdotal reports and the scant empirical studies to date suggest job attitudes and perceptions are more positive for employees in organizations at higher levels of software process maturity, we found evidence of a more complex picture. While our data supported expectations that role conflict and perceived work overload were lower for IS professionals in organizations at a level of maturity where software process behavioral controls are implemented, other results were not fully in line with prevalent beliefs. Most notably, IS workers reported significantly lower professional efficacy and lower job satisfaction in organizations at CMM Level 3, where behavioral controls are the dominant form of formal control, than in organizations at Level 1, which is relatively free of formal controls. Some anticipated positive attitudes and perceptions surfaced in organizations at the highest rungs of software process maturity (CMM Levels 4/5), where the established behavioral controls are supplemented by substantial outcome controls, as IS professionals reported lower role ambiguity and higher job satisfaction than did their counterparts in organizations at CMM Level 3.",[],"Williams, Clay K.",N/A,"Southern Illinois University Edwardsville, Campus Box 1106, Edwardsville, IL 62026-1106 U.S.A."
https://misq.umn.edu/misq/article/36/2/601/565/IS-Employee-Attitudes-and-Perceptions-at-Varying,MIS Quarterly,IS Employee Attitudes and Perceptions at Varying Levels of Software Process Maturity1,"Volume 36, Issue 2",June 2012,"Taking a control theory view of software process innovation, we tested prevalent beliefs regarding software process maturity and Information Systems employee attitudes and perceptions by surveying 736 IS professionals in 10 organizations at varying levels of the CMM (capability maturity model). Although anecdotal reports and the scant empirical studies to date suggest job attitudes and perceptions are more positive for employees in organizations at higher levels of software process maturity, we found evidence of a more complex picture. While our data supported expectations that role conflict and perceived work overload were lower for IS professionals in organizations at a level of maturity where software process behavioral controls are implemented, other results were not fully in line with prevalent beliefs. Most notably, IS workers reported significantly lower professional efficacy and lower job satisfaction in organizations at CMM Level 3, where behavioral controls are the dominant form of formal control, than in organizations at Level 1, which is relatively free of formal controls. Some anticipated positive attitudes and perceptions surfaced in organizations at the highest rungs of software process maturity (CMM Levels 4/5), where the established behavioral controls are supplemented by substantial outcome controls, as IS professionals reported lower role ambiguity and higher job satisfaction than did their counterparts in organizations at CMM Level 3.",[],"Thatcher, Jason Bennett",N/A,"Clemson University, 101 Sirrine Hall, Clemson, SC 29634 U.S.A."
https://misq.umn.edu/misq/article/36/2/625/568/Absorptive-Capacity-and-Information-Systems,MIS Quarterly,"Absorptive Capacity and Information Systems Research: Review, Synthesis, and Directions for Future Research1","Volume 36, Issue 2",June 2012,"Absorptive capacity is a firm’s ability to identify, assimilate, transform, and apply valuable external knowledge. It is considered an imperative for business success. Modern information technologies perform a critical role in the development and maintenance of a firm’s absorptive capacity. We provide an assessment of absorptive capacity in the information systems literature. IS scholars have used the absorptive capacity construct in diverse and often contradictory ways. Confusion surrounds how absorptive capacity should be conceptualized, its appropriate level of analysis, and how it can be measured. Our aim in reviewing this construct is to reduce such confusion by improving our understanding of absorptive capacity and guiding its effective use in IS research. We trace the evolution of the absorptive capacity construct in the broader organizational literature and pay special attention to its conceptualization, assumptions, and relationship to organizational learning. Following this, we investigate how absorptive capacity has been conceptualized, measured, and used in IS research. We also examine how absorptive capacity fits into distinct IS themes and facilitates understanding of various IS phenomena. Based on our analysis, we provide a framework through which IS researchers can more fully leverage the rich aspects of absorptive capacity when investigating the role of information technology in organizations.",[],"Roberts, Nicholas",N/A,"Johnson College of Business and Economics, University of South Carolina Upstate, 160 East St. John Street, Spartanburg, SC 29306 U.S.A."
https://misq.umn.edu/misq/article/36/2/625/568/Absorptive-Capacity-and-Information-Systems,MIS Quarterly,"Absorptive Capacity and Information Systems Research: Review, Synthesis, and Directions for Future Research1","Volume 36, Issue 2",June 2012,"Absorptive capacity is a firm’s ability to identify, assimilate, transform, and apply valuable external knowledge. It is considered an imperative for business success. Modern information technologies perform a critical role in the development and maintenance of a firm’s absorptive capacity. We provide an assessment of absorptive capacity in the information systems literature. IS scholars have used the absorptive capacity construct in diverse and often contradictory ways. Confusion surrounds how absorptive capacity should be conceptualized, its appropriate level of analysis, and how it can be measured. Our aim in reviewing this construct is to reduce such confusion by improving our understanding of absorptive capacity and guiding its effective use in IS research. We trace the evolution of the absorptive capacity construct in the broader organizational literature and pay special attention to its conceptualization, assumptions, and relationship to organizational learning. Following this, we investigate how absorptive capacity has been conceptualized, measured, and used in IS research. We also examine how absorptive capacity fits into distinct IS themes and facilitates understanding of various IS phenomena. Based on our analysis, we provide a framework through which IS researchers can more fully leverage the rich aspects of absorptive capacity when investigating the role of information technology in organizations.",[],"Galluch, Pamela S.",N/A,"Department of Business Administration and Economics, Roanoke College, 211 College Lane, Salem, VA 24153 U.S.A."
https://misq.umn.edu/misq/article/36/2/625/568/Absorptive-Capacity-and-Information-Systems,MIS Quarterly,"Absorptive Capacity and Information Systems Research: Review, Synthesis, and Directions for Future Research1","Volume 36, Issue 2",June 2012,"Absorptive capacity is a firm’s ability to identify, assimilate, transform, and apply valuable external knowledge. It is considered an imperative for business success. Modern information technologies perform a critical role in the development and maintenance of a firm’s absorptive capacity. We provide an assessment of absorptive capacity in the information systems literature. IS scholars have used the absorptive capacity construct in diverse and often contradictory ways. Confusion surrounds how absorptive capacity should be conceptualized, its appropriate level of analysis, and how it can be measured. Our aim in reviewing this construct is to reduce such confusion by improving our understanding of absorptive capacity and guiding its effective use in IS research. We trace the evolution of the absorptive capacity construct in the broader organizational literature and pay special attention to its conceptualization, assumptions, and relationship to organizational learning. Following this, we investigate how absorptive capacity has been conceptualized, measured, and used in IS research. We also examine how absorptive capacity fits into distinct IS themes and facilitates understanding of various IS phenomena. Based on our analysis, we provide a framework through which IS researchers can more fully leverage the rich aspects of absorptive capacity when investigating the role of information technology in organizations.",[],"Dinger, Michael",N/A,"School of Business, Henderson State University, Box 7801, Arkadelphia, AR 71999-0111 U.S.A."
https://misq.umn.edu/misq/article/36/2/625/568/Absorptive-Capacity-and-Information-Systems,MIS Quarterly,"Absorptive Capacity and Information Systems Research: Review, Synthesis, and Directions for Future Research1","Volume 36, Issue 2",June 2012,"Absorptive capacity is a firm’s ability to identify, assimilate, transform, and apply valuable external knowledge. It is considered an imperative for business success. Modern information technologies perform a critical role in the development and maintenance of a firm’s absorptive capacity. We provide an assessment of absorptive capacity in the information systems literature. IS scholars have used the absorptive capacity construct in diverse and often contradictory ways. Confusion surrounds how absorptive capacity should be conceptualized, its appropriate level of analysis, and how it can be measured. Our aim in reviewing this construct is to reduce such confusion by improving our understanding of absorptive capacity and guiding its effective use in IS research. We trace the evolution of the absorptive capacity construct in the broader organizational literature and pay special attention to its conceptualization, assumptions, and relationship to organizational learning. Following this, we investigate how absorptive capacity has been conceptualized, measured, and used in IS research. We also examine how absorptive capacity fits into distinct IS themes and facilitates understanding of various IS phenomena. Based on our analysis, we provide a framework through which IS researchers can more fully leverage the rich aspects of absorptive capacity when investigating the role of information technology in organizations.",[],"Grover, Varun",N/A,"Department of Management, Clemson University, 101 Sirrine Hall, Box 341305, Clemson, SC 29634 U.S.A."
https://misq.umn.edu/misq/article/36/2/649/574/Carrots-and-Rainbows-Motivation-and-Social,MIS Quarterly,Carrots and Rainbows: Motivation and Social Practice in Open Source Software Development1,"Volume 36, Issue 2",June 2012,"Open source software (OSS) is a social and economic phenomenon that raises fundamental questions about the motivations of contributors to information systems development. Some developers are unpaid volunteers who seek to solve their own technical problems, while others create OSS as part of their employment contract. For the past 10 years, a substantial amount of academic work has theorized about and empirically examined developer motivations. We review this work and suggest considering motivation in terms of the values of the social practice in which developers participate. Based on the social philosophy of Alasdair MacIntyre, we construct a theoretical framework that expands our assumptions about individual motivation to include the idea of a long-term, value-informed quest beyond short-term rewards. This motivation–practice framework depicts how the social practice and its supporting institutions mediate between individual motivation and outcome. The framework contains three theoretical conjectures that seek to explain how collectively elaborated standards of excellence prompt developers to produce high-quality software, change institutions, and sustain OSS development. From the framework, we derive six concrete propositions and suggest a new research agenda on motivation in OSS.",[],"von Krogh, Georg",N/A,"ETH Zurich, Switzerland"
https://misq.umn.edu/misq/article/36/2/649/574/Carrots-and-Rainbows-Motivation-and-Social,MIS Quarterly,Carrots and Rainbows: Motivation and Social Practice in Open Source Software Development1,"Volume 36, Issue 2",June 2012,"Open source software (OSS) is a social and economic phenomenon that raises fundamental questions about the motivations of contributors to information systems development. Some developers are unpaid volunteers who seek to solve their own technical problems, while others create OSS as part of their employment contract. For the past 10 years, a substantial amount of academic work has theorized about and empirically examined developer motivations. We review this work and suggest considering motivation in terms of the values of the social practice in which developers participate. Based on the social philosophy of Alasdair MacIntyre, we construct a theoretical framework that expands our assumptions about individual motivation to include the idea of a long-term, value-informed quest beyond short-term rewards. This motivation–practice framework depicts how the social practice and its supporting institutions mediate between individual motivation and outcome. The framework contains three theoretical conjectures that seek to explain how collectively elaborated standards of excellence prompt developers to produce high-quality software, change institutions, and sustain OSS development. From the framework, we derive six concrete propositions and suggest a new research agenda on motivation in OSS.",[],"Haefliger, Stefan",N/A,"ETH Zurich, Switzerland"
https://misq.umn.edu/misq/article/36/2/649/574/Carrots-and-Rainbows-Motivation-and-Social,MIS Quarterly,Carrots and Rainbows: Motivation and Social Practice in Open Source Software Development1,"Volume 36, Issue 2",June 2012,"Open source software (OSS) is a social and economic phenomenon that raises fundamental questions about the motivations of contributors to information systems development. Some developers are unpaid volunteers who seek to solve their own technical problems, while others create OSS as part of their employment contract. For the past 10 years, a substantial amount of academic work has theorized about and empirically examined developer motivations. We review this work and suggest considering motivation in terms of the values of the social practice in which developers participate. Based on the social philosophy of Alasdair MacIntyre, we construct a theoretical framework that expands our assumptions about individual motivation to include the idea of a long-term, value-informed quest beyond short-term rewards. This motivation–practice framework depicts how the social practice and its supporting institutions mediate between individual motivation and outcome. The framework contains three theoretical conjectures that seek to explain how collectively elaborated standards of excellence prompt developers to produce high-quality software, change institutions, and sustain OSS development. From the framework, we derive six concrete propositions and suggest a new research agenda on motivation in OSS.",[],"Spaeth, Sebastian",N/A,"ETH Zurich, Switzerland"
https://misq.umn.edu/misq/article/36/2/649/574/Carrots-and-Rainbows-Motivation-and-Social,MIS Quarterly,Carrots and Rainbows: Motivation and Social Practice in Open Source Software Development1,"Volume 36, Issue 2",June 2012,"Open source software (OSS) is a social and economic phenomenon that raises fundamental questions about the motivations of contributors to information systems development. Some developers are unpaid volunteers who seek to solve their own technical problems, while others create OSS as part of their employment contract. For the past 10 years, a substantial amount of academic work has theorized about and empirically examined developer motivations. We review this work and suggest considering motivation in terms of the values of the social practice in which developers participate. Based on the social philosophy of Alasdair MacIntyre, we construct a theoretical framework that expands our assumptions about individual motivation to include the idea of a long-term, value-informed quest beyond short-term rewards. This motivation–practice framework depicts how the social practice and its supporting institutions mediate between individual motivation and outcome. The framework contains three theoretical conjectures that seek to explain how collectively elaborated standards of excellence prompt developers to produce high-quality software, change institutions, and sustain OSS development. From the framework, we derive six concrete propositions and suggest a new research agenda on motivation in OSS.",[],"Wallin, Martin W.",N/A,"ETH Zurich, Switzerland"
https://misq.umn.edu/misq/article/36/2/iii/560/Editor-s-CommentsDoes-MIS-Have-Native-Theories,MIS Quarterly,Editor’s CommentsDoes MIS Have Native Theories?,"Volume 36, Issue 2",June 2012,N/A,[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Dimoka, Angelika",N/A,Temple University
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Banker, Rajiv D.",N/A,Temple University
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Benbasat, Izak",N/A,University of British Columbia
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Davis, Fred D.",N/A,University of Arkansas & Sogang University
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Dennis, Alan R.",N/A,Indiana University
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Gefen, David",N/A,Drexel University
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Gupta, Alok",N/A,University of Minnesota
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Ischebeck, Anja",N/A,University of Graz
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Kenning, Peter H.",N/A,Zeppelin University
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Pavlou, Paul A.",N/A,Temple University
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Müller-Putz, Gernot",N/A,Graz University of Technology
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Riedl, René",N/A,University of Linz
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Brocke, Jan vom",N/A,University of Liechtenstein
https://misq.umn.edu/misq/article/36/3/679/547/On-The-Use-of-Neurophysiological-Tools-in-Is,MIS Quarterly,On The Use of Neurophysiological Tools in Is Research: Developing A Research Agenda for Neurois1,"Volume 36, Issue 3",September 2012,"This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.",[],"Weber, Bernd",N/A,University of Bonn
https://misq.umn.edu/misq/article/36/3/703/531/Comparing-PLS-to-Regression-and-LISREL-A-Response,MIS Quarterly,"Comparing PLS to Regression and LISREL: A Response to Marcoulides, Chin, and Saunders1","Volume 36, Issue 3",September 2012,"In the Foreword to an MIS Quarterly Special Issue on PLS, the senior editors for the special issue noted that they rejected a number of papers because the authors attempted comparisons between results from PLS, multiple regression, and structural equation modeling (Marcoulides et al. 2009). They raised several issues they argued had to be taken into account to have legitimate comparison studies, supporting their position primarily by citing three authors: Dijkstra (1983), McDonald (1996), and Schneeweiss (1993). As researchers interested in conducting comparison studies, we read the Foreword carefully, but found it did not provide clear guidance on how to conduct “legitimate” comparisons. Nor did our reading of Dijksta, McDonald, and Schneeweiss raise any red flags about dangers in this kind of comparison research. We were concerned that instead of helping researchers to successfully engage in comparison research, the Foreword might end up discouraging that type of work, and might even be used incorrectly to reject legitimate comparison studies. This Issues and Opinions piece addresses the question of why one might conduct comparison studies, and gives an overview of the process of comparison research with a focus on what is required to make those comparisons legitimate. In addition, we explicitly address the issues raised by Marcoulides et al., to explore where they might (or might not) come into play when conducting or evaluating this type of study.",[],"Goodhue, Dale L.",N/A,"Terry College of Business, MIS Department, University of Georgia, Athens, GA 30606 U.S.A."
https://misq.umn.edu/misq/article/36/3/703/531/Comparing-PLS-to-Regression-and-LISREL-A-Response,MIS Quarterly,"Comparing PLS to Regression and LISREL: A Response to Marcoulides, Chin, and Saunders1","Volume 36, Issue 3",September 2012,"In the Foreword to an MIS Quarterly Special Issue on PLS, the senior editors for the special issue noted that they rejected a number of papers because the authors attempted comparisons between results from PLS, multiple regression, and structural equation modeling (Marcoulides et al. 2009). They raised several issues they argued had to be taken into account to have legitimate comparison studies, supporting their position primarily by citing three authors: Dijkstra (1983), McDonald (1996), and Schneeweiss (1993). As researchers interested in conducting comparison studies, we read the Foreword carefully, but found it did not provide clear guidance on how to conduct “legitimate” comparisons. Nor did our reading of Dijksta, McDonald, and Schneeweiss raise any red flags about dangers in this kind of comparison research. We were concerned that instead of helping researchers to successfully engage in comparison research, the Foreword might end up discouraging that type of work, and might even be used incorrectly to reject legitimate comparison studies. This Issues and Opinions piece addresses the question of why one might conduct comparison studies, and gives an overview of the process of comparison research with a focus on what is required to make those comparisons legitimate. In addition, we explicitly address the issues raised by Marcoulides et al., to explore where they might (or might not) come into play when conducting or evaluating this type of study.",[],"Lewis, William",N/A,"Schools of Business, Wake Forest University, Winston-Salem, NC 27109 U.S.A."
https://misq.umn.edu/misq/article/36/3/703/531/Comparing-PLS-to-Regression-and-LISREL-A-Response,MIS Quarterly,"Comparing PLS to Regression and LISREL: A Response to Marcoulides, Chin, and Saunders1","Volume 36, Issue 3",September 2012,"In the Foreword to an MIS Quarterly Special Issue on PLS, the senior editors for the special issue noted that they rejected a number of papers because the authors attempted comparisons between results from PLS, multiple regression, and structural equation modeling (Marcoulides et al. 2009). They raised several issues they argued had to be taken into account to have legitimate comparison studies, supporting their position primarily by citing three authors: Dijkstra (1983), McDonald (1996), and Schneeweiss (1993). As researchers interested in conducting comparison studies, we read the Foreword carefully, but found it did not provide clear guidance on how to conduct “legitimate” comparisons. Nor did our reading of Dijksta, McDonald, and Schneeweiss raise any red flags about dangers in this kind of comparison research. We were concerned that instead of helping researchers to successfully engage in comparison research, the Foreword might end up discouraging that type of work, and might even be used incorrectly to reject legitimate comparison studies. This Issues and Opinions piece addresses the question of why one might conduct comparison studies, and gives an overview of the process of comparison research with a focus on what is required to make those comparisons legitimate. In addition, we explicitly address the issues raised by Marcoulides et al., to explore where they might (or might not) come into play when conducting or evaluating this type of study.",[],"Thompson, Ron",N/A,N/A
https://misq.umn.edu/misq/article/36/3/717/551/When-Imprecise-Statistical-Statements-Become,MIS Quarterly,"When Imprecise Statistical Statements Become Problematic: A Response to Goodhue, Lewis, and Thompson1","Volume 36, Issue 3",September 2012,N/A,[],"Marcoulides, George A.",N/A,"Graduate School of Education, University of California, Riverside, Sproul Hall 1207, Riverside, CA 92521 U.S.A."
https://misq.umn.edu/misq/article/36/3/717/551/When-Imprecise-Statistical-Statements-Become,MIS Quarterly,"When Imprecise Statistical Statements Become Problematic: A Response to Goodhue, Lewis, and Thompson1","Volume 36, Issue 3",September 2012,N/A,[],"Chin, Wynne W.",N/A,"Department of Decision and Information Systems, C. T. Bauer College of Business, University of Houston, Houston TX 77204-6021 U.S.A."
https://misq.umn.edu/misq/article/36/3/717/551/When-Imprecise-Statistical-Statements-Become,MIS Quarterly,"When Imprecise Statistical Statements Become Problematic: A Response to Goodhue, Lewis, and Thompson1","Volume 36, Issue 3",September 2012,N/A,[],"Saunders, Carol",N/A,"Sogang University, 2 Shinsu-dong, Mapo-gu, Seoul, 121-742 Korea"
https://misq.umn.edu/misq/article/36/3/729/534/Generalization-and-Induction-Misconceptions,MIS Quarterly,"Generalization and Induction: Misconceptions, Clarifications, and a Classification of Induction1","Volume 36, Issue 3",September 2012,"In “Generalizing Generalizability in Information Systems Research,” Lee and Baskerville (2003) try to clarify generalization and classify it into four types. Unfortunately, their account is problematic. We propose repairs. Central among these is our balance-of-evidence argument that we should adopt the view that Hume’s problem of induction has a solution, even if we do not know what it is. We build upon this by proposing an alternative classification of induction. There are five types of generalization: (1) theoretical, (2) within-population, (3) cross-population, (4) contextual, and (5) temporal, with theoretical generalization being across the empirical and theoretical levels and the rest within the empirical level. Our classification also includes two kinds of inductive reasoning that do not belong to the domain of generalization. We then discuss the implications of our classification for information systems research.",[],"Tsang, Eric W. K.",N/A,"School of Management, University of Texas at Dallas, 800 W. Campbell Road, Richardson, TX 75083-0688 U.S.A."
https://misq.umn.edu/misq/article/36/3/729/534/Generalization-and-Induction-Misconceptions,MIS Quarterly,"Generalization and Induction: Misconceptions, Clarifications, and a Classification of Induction1","Volume 36, Issue 3",September 2012,"In “Generalizing Generalizability in Information Systems Research,” Lee and Baskerville (2003) try to clarify generalization and classify it into four types. Unfortunately, their account is problematic. We propose repairs. Central among these is our balance-of-evidence argument that we should adopt the view that Hume’s problem of induction has a solution, even if we do not know what it is. We build upon this by proposing an alternative classification of induction. There are five types of generalization: (1) theoretical, (2) within-population, (3) cross-population, (4) contextual, and (5) temporal, with theoretical generalization being across the empirical and theoretical levels and the rest within the empirical level. Our classification also includes two kinds of inductive reasoning that do not belong to the domain of generalization. We then discuss the implications of our classification for information systems research.",[],"Williams, John N.",N/A,"School of Social Sciences, Singapore Management University, 90 Stamford Road, Singapore 178903 Singapore"
https://misq.umn.edu/misq/article/36/3/749/550/Conceptualizing-Generalizability-New-Contributions,MIS Quarterly,Conceptualizing Generalizability: New Contributions and a Reply1,"Volume 36, Issue 3",September 2012,"Tsang and Williams offer some good and provocative ideas in their critique of our earlier article on generalizing and generalizability. In this essay we will advance some new ideas by building on those collected in both Tsang and Williams and our original article (Lee and Baskerville 2003). Because IS is a pluralist scientific discipline, one in which both qualitative and quantitative (and both interpretive and positivist) research approaches are valued, “generalize” is unlikely to be a viable term or concept if only one IS research paradigm may lay claim to it and excludes others from using it. Both papers agree on this point, but approach the problem differently. Where we originally generalized generalizability by offering new language, Tsang and Williams conceptualize generalizability by framing it more closely to its older, more statistically oriented form. We agree about the importance of induction and about the classification or taxonomy of different types of induction. We build further in this essay, advancing the ethical questions raised by generalization: A formulation of judgment calls that need to be made when generalizing a theory to a new setting. We further demonstrate how the process of generalizing may actually proceed, based on the common ground between Tsang and Williams and our original article.",[],"Lee, Allen S.",N/A,"Virginia Commonwealth University, 301 West Main Street, Richmond, VA 23284-4000 U.S.A."
https://misq.umn.edu/misq/article/36/3/749/550/Conceptualizing-Generalizability-New-Contributions,MIS Quarterly,Conceptualizing Generalizability: New Contributions and a Reply1,"Volume 36, Issue 3",September 2012,"Tsang and Williams offer some good and provocative ideas in their critique of our earlier article on generalizing and generalizability. In this essay we will advance some new ideas by building on those collected in both Tsang and Williams and our original article (Lee and Baskerville 2003). Because IS is a pluralist scientific discipline, one in which both qualitative and quantitative (and both interpretive and positivist) research approaches are valued, “generalize” is unlikely to be a viable term or concept if only one IS research paradigm may lay claim to it and excludes others from using it. Both papers agree on this point, but approach the problem differently. Where we originally generalized generalizability by offering new language, Tsang and Williams conceptualize generalizability by framing it more closely to its older, more statistically oriented form. We agree about the importance of induction and about the classification or taxonomy of different types of induction. We build further in this essay, advancing the ethical questions raised by generalization: A formulation of judgment calls that need to be made when generalizing a theory to a new setting. We further demonstrate how the process of generalizing may actually proceed, based on the common ground between Tsang and Williams and our original article.",[],"Baskerville, Richard L.",N/A,"Georgia State University, POB 4015, Atlanta, GA 30302-4015 U.S.A."
https://misq.umn.edu/misq/article/36/3/763/537/The-Roles-of-Theory-in-Canonical-Action-Research1,MIS Quarterly,The Roles of Theory in Canonical Action Research1,"Volume 36, Issue 3",September 2012,"Canonical action research (CAR) aims to address real-world problems and improve organizational performance by combining scholarly observations with practical interventions. However, efforts to conduct CAR have revealed challenges that reflect a significant research–practice gap. We examine these challenges by revisiting the process, principles, and criteria of CAR developed earlier. The specific roles of two different types of theory in the cyclical action research process are considered. A project undertaken in two public relations firms illustrates how our methodological revision improves the rigor and quality of CAR. This article contributes both a significantly enhanced action research method, with detailed guidelines and suggestions that emphasize the roles of focal and instrumental theories, and an emerging theory of knowledge sharing that incorporates key elements of Chinese management and culture.",[],"Davison, Robert M.",N/A,"Department of Information Systems, City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong"
https://misq.umn.edu/misq/article/36/3/763/537/The-Roles-of-Theory-in-Canonical-Action-Research1,MIS Quarterly,The Roles of Theory in Canonical Action Research1,"Volume 36, Issue 3",September 2012,"Canonical action research (CAR) aims to address real-world problems and improve organizational performance by combining scholarly observations with practical interventions. However, efforts to conduct CAR have revealed challenges that reflect a significant research–practice gap. We examine these challenges by revisiting the process, principles, and criteria of CAR developed earlier. The specific roles of two different types of theory in the cyclical action research process are considered. A project undertaken in two public relations firms illustrates how our methodological revision improves the rigor and quality of CAR. This article contributes both a significantly enhanced action research method, with detailed guidelines and suggestions that emphasize the roles of focal and instrumental theories, and an emerging theory of knowledge sharing that incorporates key elements of Chinese management and culture.",[],"Martinsons, Maris G.",N/A,"Department of Management, City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong"
https://misq.umn.edu/misq/article/36/3/763/537/The-Roles-of-Theory-in-Canonical-Action-Research1,MIS Quarterly,The Roles of Theory in Canonical Action Research1,"Volume 36, Issue 3",September 2012,"Canonical action research (CAR) aims to address real-world problems and improve organizational performance by combining scholarly observations with practical interventions. However, efforts to conduct CAR have revealed challenges that reflect a significant research–practice gap. We examine these challenges by revisiting the process, principles, and criteria of CAR developed earlier. The specific roles of two different types of theory in the cyclical action research process are considered. A project undertaken in two public relations firms illustrates how our methodological revision improves the rigor and quality of CAR. This article contributes both a significantly enhanced action research method, with detailed guidelines and suggestions that emphasize the roles of focal and instrumental theories, and an emerging theory of knowledge sharing that incorporates key elements of Chinese management and culture.",[],"Ou, Carol X. J.",N/A,"Department of Information Management, Tilburg University, Warandelaan 2, 5037 AB Tilburg, The Netherlands"
https://misq.umn.edu/misq/article/36/3/787/539/Principles-for-Conducting-Critical-Realist-Case,MIS Quarterly,Principles for Conducting Critical Realist Case Study Research in Information Systems1,"Volume 36, Issue 3",September 2012,"Critical realism is emerging as a viable philosophical paradigm for conducting social science research, and has been proposed as an alternative to the more prevalent paradigms of positivism and interpretivism. Few papers, however, have offered clear guidance for applying this philosophy to actual research methodologies. Under critical realism, a causal explanation for a given phenomenon is inferred by explicitly identifying the means by which structural entities and contextual conditions interact to generate a given set of events. Consistent with this view of causality, we propose a set of methodological principles for conducting and evaluating critical realism-based explanatory case study research within the information systems field. The principles are derived directly from the ontological and epistemological assumptions of critical realism. We demonstrate the utility of each of the principles through examples drawn from existing critical realist case studies. The article concludes by discussing the implications of critical realism based research for IS research and practice.",[],"Wynn, Donald",N/A,"School of Business Administration, University of Dayton, Dayton, OH 45469-2130 U.S.A."
https://misq.umn.edu/misq/article/36/3/787/539/Principles-for-Conducting-Critical-Realist-Case,MIS Quarterly,Principles for Conducting Critical Realist Case Study Research in Information Systems1,"Volume 36, Issue 3",September 2012,"Critical realism is emerging as a viable philosophical paradigm for conducting social science research, and has been proposed as an alternative to the more prevalent paradigms of positivism and interpretivism. Few papers, however, have offered clear guidance for applying this philosophy to actual research methodologies. Under critical realism, a causal explanation for a given phenomenon is inferred by explicitly identifying the means by which structural entities and contextual conditions interact to generate a given set of events. Consistent with this view of causality, we propose a set of methodological principles for conducting and evaluating critical realism-based explanatory case study research within the information systems field. The principles are derived directly from the ontological and epistemological assumptions of critical realism. We demonstrate the utility of each of the principles through examples drawn from existing critical realist case studies. The article concludes by discussing the implications of critical realism based research for IS research and practice.",[],"Williams, Clay K.",N/A,"Southern Illinois University Edwardsville, Campus Box 1106, Edwardsville, IL 62026 U.S.A."
https://misq.umn.edu/misq/article/36/3/811/541/How-to-Conduct-a-Functional-Magnetic-Resonance,MIS Quarterly,How to Conduct a Functional Magnetic Resonance (fMRI) Study in Social Science Research1,"Volume 36, Issue 3",September 2012,"This research essay outlines a set of guidelines for conducting functional Magnetic Resonance Imaging (fMRI) studies in social science research in general and also, accordingly, in Information Systems research. Given the increased interest in using neuroimaging tools across the social sciences, this study aims at specifying the key steps needed to conduct an fMRI study while ensuring that enough detail is provided to evaluate the methods and results. The outline of an fMRI study consists of four key steps: (1) formulating the research question, (2) designing the fMRI protocol, (3) analyzing fMRI data, and (4) interpreting and reporting fMRI results. These steps are described with an illustrative example of a published fMRI study on trust and distrust in this journal (Dimoka 2010). The paper contributes to the methodological literature by (1) providing a set of guidelines for designing and conducting fMRI studies, (2) specifying methodological details that should be included in fMRI studies in academic venues, and (3) illustrating these practices with an exemplar fMRI study. Future directions for conducting high-quality fMRI studies in the social sciences are discussed.",[],"Dimoka, Angelika",N/A,"Fox School of Business, Temple University, Philadelphia, PA 19122 U.S.A."
https://misq.umn.edu/misq/article/36/3/841/544/Building-Member-Attachment-in-Online-Communities,MIS Quarterly,Building Member Attachment in Online Communities: Applying Theories of Group Identity and Interpersonal Bonds1,"Volume 36, Issue 3",September 2012,"Online communities are increasingly important to organizations and the general public, but there is little theoretically based research on what makes some online communities more successful than others. In this article, we apply theory from the field of social psychology to understand how online communities develop member attachment, an important dimension of community success. We implemented and empirically tested two sets of community features for building member attachment by strengthening either group identity or interpersonal bonds. To increase identity-based attachment, we gave members information about group activities and intergroup competition, and tools for group-level communication. To increase bond-based attachment, we gave members information about the activities of individual members and interpersonal similarity, and tools for interpersonal communication. Results from a six-month field experiment show that participants’ visit frequency and self-reported attachment increased in both conditions. Community features intended to foster identity-based attachment had stronger effects than features intended to foster bond-based attachment. Participants in the identity condition with access to group profiles and repeated exposure to their group’s activities visited their community twice as frequently as participants in other conditions. The new features also had stronger effects on newcomers than on old-timers. This research illustrates how theory from the social science literature can be applied to gain a more systematic understanding of online communities and how theory-inspired features can improve their success.",[],"Ren, Yuqing",N/A,"Carlson School of Management, University of Minnesota, Minneapolis, MN 55455 U.S.A"
https://misq.umn.edu/misq/article/36/3/841/544/Building-Member-Attachment-in-Online-Communities,MIS Quarterly,Building Member Attachment in Online Communities: Applying Theories of Group Identity and Interpersonal Bonds1,"Volume 36, Issue 3",September 2012,"Online communities are increasingly important to organizations and the general public, but there is little theoretically based research on what makes some online communities more successful than others. In this article, we apply theory from the field of social psychology to understand how online communities develop member attachment, an important dimension of community success. We implemented and empirically tested two sets of community features for building member attachment by strengthening either group identity or interpersonal bonds. To increase identity-based attachment, we gave members information about group activities and intergroup competition, and tools for group-level communication. To increase bond-based attachment, we gave members information about the activities of individual members and interpersonal similarity, and tools for interpersonal communication. Results from a six-month field experiment show that participants’ visit frequency and self-reported attachment increased in both conditions. Community features intended to foster identity-based attachment had stronger effects than features intended to foster bond-based attachment. Participants in the identity condition with access to group profiles and repeated exposure to their group’s activities visited their community twice as frequently as participants in other conditions. The new features also had stronger effects on newcomers than on old-timers. This research illustrates how theory from the social science literature can be applied to gain a more systematic understanding of online communities and how theory-inspired features can improve their success.",[],"Harper, F. Maxwell",N/A,"Department of Computer Science, University of Minnesota, Minneapolis, MN 55455 U.S.A."
https://misq.umn.edu/misq/article/36/3/841/544/Building-Member-Attachment-in-Online-Communities,MIS Quarterly,Building Member Attachment in Online Communities: Applying Theories of Group Identity and Interpersonal Bonds1,"Volume 36, Issue 3",September 2012,"Online communities are increasingly important to organizations and the general public, but there is little theoretically based research on what makes some online communities more successful than others. In this article, we apply theory from the field of social psychology to understand how online communities develop member attachment, an important dimension of community success. We implemented and empirically tested two sets of community features for building member attachment by strengthening either group identity or interpersonal bonds. To increase identity-based attachment, we gave members information about group activities and intergroup competition, and tools for group-level communication. To increase bond-based attachment, we gave members information about the activities of individual members and interpersonal similarity, and tools for interpersonal communication. Results from a six-month field experiment show that participants’ visit frequency and self-reported attachment increased in both conditions. Community features intended to foster identity-based attachment had stronger effects than features intended to foster bond-based attachment. Participants in the identity condition with access to group profiles and repeated exposure to their group’s activities visited their community twice as frequently as participants in other conditions. The new features also had stronger effects on newcomers than on old-timers. This research illustrates how theory from the social science literature can be applied to gain a more systematic understanding of online communities and how theory-inspired features can improve their success.",[],"Drenner, Sara",N/A,"Department of Computer Science, University of Minnesota, Minneapolis, MN 55455 U.S.A."
https://misq.umn.edu/misq/article/36/3/841/544/Building-Member-Attachment-in-Online-Communities,MIS Quarterly,Building Member Attachment in Online Communities: Applying Theories of Group Identity and Interpersonal Bonds1,"Volume 36, Issue 3",September 2012,"Online communities are increasingly important to organizations and the general public, but there is little theoretically based research on what makes some online communities more successful than others. In this article, we apply theory from the field of social psychology to understand how online communities develop member attachment, an important dimension of community success. We implemented and empirically tested two sets of community features for building member attachment by strengthening either group identity or interpersonal bonds. To increase identity-based attachment, we gave members information about group activities and intergroup competition, and tools for group-level communication. To increase bond-based attachment, we gave members information about the activities of individual members and interpersonal similarity, and tools for interpersonal communication. Results from a six-month field experiment show that participants’ visit frequency and self-reported attachment increased in both conditions. Community features intended to foster identity-based attachment had stronger effects than features intended to foster bond-based attachment. Participants in the identity condition with access to group profiles and repeated exposure to their group’s activities visited their community twice as frequently as participants in other conditions. The new features also had stronger effects on newcomers than on old-timers. This research illustrates how theory from the social science literature can be applied to gain a more systematic understanding of online communities and how theory-inspired features can improve their success.",[],"Terveen, Loren",N/A,"Department of Computer Science, University of Minnesota, Minneapolis, MN 55455 U.S.A."
https://misq.umn.edu/misq/article/36/3/841/544/Building-Member-Attachment-in-Online-Communities,MIS Quarterly,Building Member Attachment in Online Communities: Applying Theories of Group Identity and Interpersonal Bonds1,"Volume 36, Issue 3",September 2012,"Online communities are increasingly important to organizations and the general public, but there is little theoretically based research on what makes some online communities more successful than others. In this article, we apply theory from the field of social psychology to understand how online communities develop member attachment, an important dimension of community success. We implemented and empirically tested two sets of community features for building member attachment by strengthening either group identity or interpersonal bonds. To increase identity-based attachment, we gave members information about group activities and intergroup competition, and tools for group-level communication. To increase bond-based attachment, we gave members information about the activities of individual members and interpersonal similarity, and tools for interpersonal communication. Results from a six-month field experiment show that participants’ visit frequency and self-reported attachment increased in both conditions. Community features intended to foster identity-based attachment had stronger effects than features intended to foster bond-based attachment. Participants in the identity condition with access to group profiles and repeated exposure to their group’s activities visited their community twice as frequently as participants in other conditions. The new features also had stronger effects on newcomers than on old-timers. This research illustrates how theory from the social science literature can be applied to gain a more systematic understanding of online communities and how theory-inspired features can improve their success.",[],"Kiesler, Sara",N/A,"Human–Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA 15213 U.S.A."
https://misq.umn.edu/misq/article/36/3/841/544/Building-Member-Attachment-in-Online-Communities,MIS Quarterly,Building Member Attachment in Online Communities: Applying Theories of Group Identity and Interpersonal Bonds1,"Volume 36, Issue 3",September 2012,"Online communities are increasingly important to organizations and the general public, but there is little theoretically based research on what makes some online communities more successful than others. In this article, we apply theory from the field of social psychology to understand how online communities develop member attachment, an important dimension of community success. We implemented and empirically tested two sets of community features for building member attachment by strengthening either group identity or interpersonal bonds. To increase identity-based attachment, we gave members information about group activities and intergroup competition, and tools for group-level communication. To increase bond-based attachment, we gave members information about the activities of individual members and interpersonal similarity, and tools for interpersonal communication. Results from a six-month field experiment show that participants’ visit frequency and self-reported attachment increased in both conditions. Community features intended to foster identity-based attachment had stronger effects than features intended to foster bond-based attachment. Participants in the identity condition with access to group profiles and repeated exposure to their group’s activities visited their community twice as frequently as participants in other conditions. The new features also had stronger effects on newcomers than on old-timers. This research illustrates how theory from the social science literature can be applied to gain a more systematic understanding of online communities and how theory-inspired features can improve their success.",[],"Riedl, John",N/A,"Department of Computer Science, University of Minnesota, Minneapolis, MN 55455 U.S.A."
https://misq.umn.edu/misq/article/36/3/841/544/Building-Member-Attachment-in-Online-Communities,MIS Quarterly,Building Member Attachment in Online Communities: Applying Theories of Group Identity and Interpersonal Bonds1,"Volume 36, Issue 3",September 2012,"Online communities are increasingly important to organizations and the general public, but there is little theoretically based research on what makes some online communities more successful than others. In this article, we apply theory from the field of social psychology to understand how online communities develop member attachment, an important dimension of community success. We implemented and empirically tested two sets of community features for building member attachment by strengthening either group identity or interpersonal bonds. To increase identity-based attachment, we gave members information about group activities and intergroup competition, and tools for group-level communication. To increase bond-based attachment, we gave members information about the activities of individual members and interpersonal similarity, and tools for interpersonal communication. Results from a six-month field experiment show that participants’ visit frequency and self-reported attachment increased in both conditions. Community features intended to foster identity-based attachment had stronger effects than features intended to foster bond-based attachment. Participants in the identity condition with access to group profiles and repeated exposure to their group’s activities visited their community twice as frequently as participants in other conditions. The new features also had stronger effects on newcomers than on old-timers. This research illustrates how theory from the social science literature can be applied to gain a more systematic understanding of online communities and how theory-inspired features can improve their success.",[],"Kraut, Robert E.",N/A,"Human–Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA 15213 U.S.A."
https://misq.umn.edu/misq/article/36/3/865/552/A-Knowledge-Based-Model-of-Radical-Innovation-in,MIS Quarterly,A Knowledge-Based Model of Radical Innovation in Small Software Firms1,"Volume 36, Issue 3",September 2012,"In this paper, we adopt the lens of absorptive capacity (ACAP), defined by two dimensions—the knowledge base (consisting of knowledge diversity, depth, and linkages) and routines (consisting of sensing and experimentation)—to explain how a software firm’s knowledge endowments influence its level of radical information technology innovation during a technological breakthrough. We distinguish three types of IT innovations—base, processes, and service innovation—that form an innovation ecology. We posit that (1) ACAP is a relational construct where the impact of the knowledge base is mediated by routines; (2) IT innovations are either externally adopted or internally generated; and (3) knowledge antecedents associated with different types of innovations differ. We hypothesize a three-step, mediated path (knowledge base → sensing → experimentation → innovation) for external innovation adoption, and a two-step path (knowledge diversity/depth → experimentation → innovation) for internal innovation creation to explain the software firm’s level of radical innovation across three IT innovation types. We validate the model through a cross-sector study that examined how 121 small software firms innovated with Internet computing. We confirm the mediated nature of ACAP for external base innovations, which are driven by all three knowledge-based factors as follows: (1) knowledge depth (direct positive effect); (2) knowledge diversity (mediated three-step path), (3) knowledge linkages (mediated three step path). Process innovations are externally driven by a three-step mediated path for knowledge linkages, as well as being directly affected by knowledge diversity, but negatively and directly impeded by knowledge depth. Service innovations are not driven by any mediated influence of ACAP, but driven directly by knowledge diversity. At the same time, both service and process innovations are strongly influenced by prior IT innovations: base and/or service. Several directions for future studies of radical IT innovation are proposed.",[],"Carlo, Jessica Luo",N/A,"Department of Advertising, Public Relations, and Retailing, Michigan State University, East Lansing, MI 48824-1212 U.S.A."
https://misq.umn.edu/misq/article/36/3/865/552/A-Knowledge-Based-Model-of-Radical-Innovation-in,MIS Quarterly,A Knowledge-Based Model of Radical Innovation in Small Software Firms1,"Volume 36, Issue 3",September 2012,"In this paper, we adopt the lens of absorptive capacity (ACAP), defined by two dimensions—the knowledge base (consisting of knowledge diversity, depth, and linkages) and routines (consisting of sensing and experimentation)—to explain how a software firm’s knowledge endowments influence its level of radical information technology innovation during a technological breakthrough. We distinguish three types of IT innovations—base, processes, and service innovation—that form an innovation ecology. We posit that (1) ACAP is a relational construct where the impact of the knowledge base is mediated by routines; (2) IT innovations are either externally adopted or internally generated; and (3) knowledge antecedents associated with different types of innovations differ. We hypothesize a three-step, mediated path (knowledge base → sensing → experimentation → innovation) for external innovation adoption, and a two-step path (knowledge diversity/depth → experimentation → innovation) for internal innovation creation to explain the software firm’s level of radical innovation across three IT innovation types. We validate the model through a cross-sector study that examined how 121 small software firms innovated with Internet computing. We confirm the mediated nature of ACAP for external base innovations, which are driven by all three knowledge-based factors as follows: (1) knowledge depth (direct positive effect); (2) knowledge diversity (mediated three-step path), (3) knowledge linkages (mediated three step path). Process innovations are externally driven by a three-step mediated path for knowledge linkages, as well as being directly affected by knowledge diversity, but negatively and directly impeded by knowledge depth. Service innovations are not driven by any mediated influence of ACAP, but driven directly by knowledge diversity. At the same time, both service and process innovations are strongly influenced by prior IT innovations: base and/or service. Several directions for future studies of radical IT innovation are proposed.",[],"Lyytinen, Kalle",N/A,"Department of Information Systems, Weatherhead School of Management, Case Western Reserve University, Cleveland, OH 44106 U.S.A."
https://misq.umn.edu/misq/article/36/3/865/552/A-Knowledge-Based-Model-of-Radical-Innovation-in,MIS Quarterly,A Knowledge-Based Model of Radical Innovation in Small Software Firms1,"Volume 36, Issue 3",September 2012,"In this paper, we adopt the lens of absorptive capacity (ACAP), defined by two dimensions—the knowledge base (consisting of knowledge diversity, depth, and linkages) and routines (consisting of sensing and experimentation)—to explain how a software firm’s knowledge endowments influence its level of radical information technology innovation during a technological breakthrough. We distinguish three types of IT innovations—base, processes, and service innovation—that form an innovation ecology. We posit that (1) ACAP is a relational construct where the impact of the knowledge base is mediated by routines; (2) IT innovations are either externally adopted or internally generated; and (3) knowledge antecedents associated with different types of innovations differ. We hypothesize a three-step, mediated path (knowledge base → sensing → experimentation → innovation) for external innovation adoption, and a two-step path (knowledge diversity/depth → experimentation → innovation) for internal innovation creation to explain the software firm’s level of radical innovation across three IT innovation types. We validate the model through a cross-sector study that examined how 121 small software firms innovated with Internet computing. We confirm the mediated nature of ACAP for external base innovations, which are driven by all three knowledge-based factors as follows: (1) knowledge depth (direct positive effect); (2) knowledge diversity (mediated three-step path), (3) knowledge linkages (mediated three step path). Process innovations are externally driven by a three-step mediated path for knowledge linkages, as well as being directly affected by knowledge diversity, but negatively and directly impeded by knowledge depth. Service innovations are not driven by any mediated influence of ACAP, but driven directly by knowledge diversity. At the same time, both service and process innovations are strongly influenced by prior IT innovations: base and/or service. Several directions for future studies of radical IT innovation are proposed.",[],"Rose, Gregory M.",N/A,"College of Business, Washington State University, 14204 NE Salmon Creek Avenue, Vancouver, WA 98686 U.S.A."
https://misq.umn.edu/misq/article/36/3/897/543/Information-Technology-Implementers-Responses-to,MIS Quarterly,Information Technology Implementers’ Responses to User Resistance: Nature And Effects1,"Volume 36, Issue 3",September 2012,"User resistance has long been acknowledged as a critical issue during information technology implementation. Resistance can be functional when it signals the existence of problems with the IT or with its effects; it will be dysfunctional when it leads to organizational disruption. Notwithstanding the nature of resistance, the implementers—business managers, functional managers, or IT professionals—have to address it. Although the literature recognizes the importance of user resistance, it has paid little attention to implementers’ responses—and their effect—when resistance occurs. Our study focuses on this phenomenon, and addresses two questions: What are implementers’ responses to user resistance? What are the effects of these responses on user resistance? To answer these questions, we conducted a case survey, which combines the richness of case studies with the benefits of analyzing large quantities of data. Our case database includes 89 cases with a total of 137 episodes of resistance. In response to our first research question, we propose a taxonomy that includes four categories of implementers’ responses to user resistance: inaction, acknowledgment, rectification, and dissuasion. To answer our second question, we adopted a set-theoretic analysis approach, which we enriched with content analysis of the cases. Based on these analyses, we offer a theoretical explanation of how implementers’ responses may affect the antecedents that earlier research found to be associated with user resistance behaviors.",[],"Rivard, Suzanne",N/A,"HEC Montréal, 3000 Chemin-de-la-Côte-Ste-Catherine, Montréal H3T 2V7 Canada"
https://misq.umn.edu/misq/article/36/3/897/543/Information-Technology-Implementers-Responses-to,MIS Quarterly,Information Technology Implementers’ Responses to User Resistance: Nature And Effects1,"Volume 36, Issue 3",September 2012,"User resistance has long been acknowledged as a critical issue during information technology implementation. Resistance can be functional when it signals the existence of problems with the IT or with its effects; it will be dysfunctional when it leads to organizational disruption. Notwithstanding the nature of resistance, the implementers—business managers, functional managers, or IT professionals—have to address it. Although the literature recognizes the importance of user resistance, it has paid little attention to implementers’ responses—and their effect—when resistance occurs. Our study focuses on this phenomenon, and addresses two questions: What are implementers’ responses to user resistance? What are the effects of these responses on user resistance? To answer these questions, we conducted a case survey, which combines the richness of case studies with the benefits of analyzing large quantities of data. Our case database includes 89 cases with a total of 137 episodes of resistance. In response to our first research question, we propose a taxonomy that includes four categories of implementers’ responses to user resistance: inaction, acknowledgment, rectification, and dissuasion. To answer our second question, we adopted a set-theoretic analysis approach, which we enriched with content analysis of the cases. Based on these analyses, we offer a theoretical explanation of how implementers’ responses may affect the antecedents that earlier research found to be associated with user resistance behaviors.",[],"Lapointe, Liette",N/A,"Desautels Faculty of Management, McGill University, 1001 Sherbrooke Street West, Montréal H3A 1G5 Canada"
https://misq.umn.edu/misq/article/36/3/921/540/Standards-Competition-in-the-Presence-of-Digital,MIS Quarterly,Standards Competition in the Presence of Digital Conversion Technology: An Empirical Analysis of the Flash Memory Card Market1,"Volume 36, Issue 3",September 2012,"Both theoretical and empirical evidence suggest that, in many markets with standards competition, network effects make the strong grow stronger and can “tip” the market toward a single, winner-take-all standard. We hypothesize, however, that low cost digital conversion technologies, which facilitate easy compatibility across competing standards, may reduce the strength of these network effects. We empirically test our hypotheses in the context of the digital flash memory card market.We first test for the presence of network effects in this market and find that network effects, as measured here, are associated with a significant positive price premium for leading flash memory card formats. We then find that the availability of digital converters reduces the price premium of the leading flash card formats and reduces the overall concentration in the flash memory market. Thus, our results suggest that, in the presence of low cost conversion technologies and digital content, the probability of market dominance can be lessened to the point where multiple, otherwise incompatible, standards are viable.Our conclusion that the presence of converters weakens network effects implies that producers of non-dominant digital goods standards benefit from the provision of conversion technology. Our analysis thus aids managers seeking to understand the impact of converters on market outcomes, and contributes to the existing literature on network effects by providing new insights into how conversion technologies can affect pricing strategies in these increasingly important digital settings.",[],"Liu, Charles Zhechao",N/A,"University of Texas at San Antonio, San Antonio, TX 78249 U.S.A."
https://misq.umn.edu/misq/article/36/3/921/540/Standards-Competition-in-the-Presence-of-Digital,MIS Quarterly,Standards Competition in the Presence of Digital Conversion Technology: An Empirical Analysis of the Flash Memory Card Market1,"Volume 36, Issue 3",September 2012,"Both theoretical and empirical evidence suggest that, in many markets with standards competition, network effects make the strong grow stronger and can “tip” the market toward a single, winner-take-all standard. We hypothesize, however, that low cost digital conversion technologies, which facilitate easy compatibility across competing standards, may reduce the strength of these network effects. We empirically test our hypotheses in the context of the digital flash memory card market.We first test for the presence of network effects in this market and find that network effects, as measured here, are associated with a significant positive price premium for leading flash memory card formats. We then find that the availability of digital converters reduces the price premium of the leading flash card formats and reduces the overall concentration in the flash memory market. Thus, our results suggest that, in the presence of low cost conversion technologies and digital content, the probability of market dominance can be lessened to the point where multiple, otherwise incompatible, standards are viable.Our conclusion that the presence of converters weakens network effects implies that producers of non-dominant digital goods standards benefit from the provision of conversion technology. Our analysis thus aids managers seeking to understand the impact of converters on market outcomes, and contributes to the existing literature on network effects by providing new insights into how conversion technologies can affect pricing strategies in these increasingly important digital settings.",[],"Kemerer, Chris F.",N/A,"University of Pittsburgh, Pittsburgh, PA 15260 U.S.A. and King Abdul Aziz University, Jeddah, Saudi Arabia"
https://misq.umn.edu/misq/article/36/3/921/540/Standards-Competition-in-the-Presence-of-Digital,MIS Quarterly,Standards Competition in the Presence of Digital Conversion Technology: An Empirical Analysis of the Flash Memory Card Market1,"Volume 36, Issue 3",September 2012,"Both theoretical and empirical evidence suggest that, in many markets with standards competition, network effects make the strong grow stronger and can “tip” the market toward a single, winner-take-all standard. We hypothesize, however, that low cost digital conversion technologies, which facilitate easy compatibility across competing standards, may reduce the strength of these network effects. We empirically test our hypotheses in the context of the digital flash memory card market.We first test for the presence of network effects in this market and find that network effects, as measured here, are associated with a significant positive price premium for leading flash memory card formats. We then find that the availability of digital converters reduces the price premium of the leading flash card formats and reduces the overall concentration in the flash memory market. Thus, our results suggest that, in the presence of low cost conversion technologies and digital content, the probability of market dominance can be lessened to the point where multiple, otherwise incompatible, standards are viable.Our conclusion that the presence of converters weakens network effects implies that producers of non-dominant digital goods standards benefit from the provision of conversion technology. Our analysis thus aids managers seeking to understand the impact of converters on market outcomes, and contributes to the existing literature on network effects by providing new insights into how conversion technologies can affect pricing strategies in these increasingly important digital settings.",[],"Slaughter, Sandra A.",N/A,"Georgia Institute of Technology, Atlanta, GA 30308 U.S.A."
https://misq.umn.edu/misq/article/36/3/921/540/Standards-Competition-in-the-Presence-of-Digital,MIS Quarterly,Standards Competition in the Presence of Digital Conversion Technology: An Empirical Analysis of the Flash Memory Card Market1,"Volume 36, Issue 3",September 2012,"Both theoretical and empirical evidence suggest that, in many markets with standards competition, network effects make the strong grow stronger and can “tip” the market toward a single, winner-take-all standard. We hypothesize, however, that low cost digital conversion technologies, which facilitate easy compatibility across competing standards, may reduce the strength of these network effects. We empirically test our hypotheses in the context of the digital flash memory card market.We first test for the presence of network effects in this market and find that network effects, as measured here, are associated with a significant positive price premium for leading flash memory card formats. We then find that the availability of digital converters reduces the price premium of the leading flash card formats and reduces the overall concentration in the flash memory market. Thus, our results suggest that, in the presence of low cost conversion technologies and digital content, the probability of market dominance can be lessened to the point where multiple, otherwise incompatible, standards are viable.Our conclusion that the presence of converters weakens network effects implies that producers of non-dominant digital goods standards benefit from the provision of conversion technology. Our analysis thus aids managers seeking to understand the impact of converters on market outcomes, and contributes to the existing literature on network effects by providing new insights into how conversion technologies can affect pricing strategies in these increasingly important digital settings.",[],"Smith, Michael D.",N/A,"Carnegie Mellon University, Pittsburgh, PA 15213 U.S.A."
https://misq.umn.edu/misq/article/36/3/943/533/Senior-Editor-Note,MIS Quarterly,Senior Editor Note,"Volume 36, Issue 3",September 2012,N/A,[],"Goodhue, Dale L.",N/A,"Terry College of Business, MIS Department, University of Georgia, Athens, GA 30606 U.S.A."
https://misq.umn.edu/misq/article/36/3/iii/532/Editor-s-CommentsRiding-the-Wave-Past-Trends-and,MIS Quarterly,Editor’s CommentsRiding the Wave: Past Trends and Future Directions for Health IT Research,"Volume 36, Issue 3",September 2012,N/A,[],"Romanow, Darryl",N/A,Georgia State University
https://misq.umn.edu/misq/article/36/3/iii/532/Editor-s-CommentsRiding-the-Wave-Past-Trends-and,MIS Quarterly,Editor’s CommentsRiding the Wave: Past Trends and Future Directions for Health IT Research,"Volume 36, Issue 3",September 2012,N/A,[],"Cho, Sunyoung",N/A,Georgia Gwinnett College
https://misq.umn.edu/misq/article/36/3/iii/532/Editor-s-CommentsRiding-the-Wave-Past-Trends-and,MIS Quarterly,Editor’s CommentsRiding the Wave: Past Trends and Future Directions for Health IT Research,"Volume 36, Issue 3",September 2012,N/A,[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/36/3/945/546/A-Research-Note-on-Representing-Part-Whole,MIS Quarterly,A Research Note on Representing Part–Whole Relations in Conceptual Modeling1,"Volume 36, Issue 3",September 2012,"Empirical research is an important methodology for the study of conceptual modeling practices. The recently published article “Representing Part–Whole Relations in Conceptual Modeling: An Empirical Evaluation” (Shanks et al. 2008) uses the lens of ontology to study a relatively sophisticated aspect of conceptual modeling practice, the representation of aggregation and composition. It contends that some analysts argue that a composite should be represented as a relationship while others argue that a composite should be represented as an entity. We find no evidence of such a dispute in the data modeling literature. We observe that composites are objects. By definition, all object-types should be represented as entities. Therefore, using the relationship construct to represent composites should not be seen as a viable alternative. Additionally, we found significant conceptual and methodological issues within the study that call its conclusions into question. As a way to offer insight into the requisite methodological procedures for research in this area, we conducted two experiments that both explicate and address the issues raised. Our results call into question the utility of using ontology as a foundation for conceptual modeling practice. Furthermore, they suggest a contrary but at least equally plausible explanation for the results reported by Shanks et al. In conducting this work we hope to encourage dialogue that will be beneficial for future endeavors aimed at identifying, developing, and evaluating appropriate foundations for the discipline of conceptual modeling.",[],"Allen, Gove N.",N/A,"Marriot School of Management, Brigham Young University, 783 Tanner Building, Provo, UT 84602 U.S.A."
https://misq.umn.edu/misq/article/36/3/945/546/A-Research-Note-on-Representing-Part-Whole,MIS Quarterly,A Research Note on Representing Part–Whole Relations in Conceptual Modeling1,"Volume 36, Issue 3",September 2012,"Empirical research is an important methodology for the study of conceptual modeling practices. The recently published article “Representing Part–Whole Relations in Conceptual Modeling: An Empirical Evaluation” (Shanks et al. 2008) uses the lens of ontology to study a relatively sophisticated aspect of conceptual modeling practice, the representation of aggregation and composition. It contends that some analysts argue that a composite should be represented as a relationship while others argue that a composite should be represented as an entity. We find no evidence of such a dispute in the data modeling literature. We observe that composites are objects. By definition, all object-types should be represented as entities. Therefore, using the relationship construct to represent composites should not be seen as a viable alternative. Additionally, we found significant conceptual and methodological issues within the study that call its conclusions into question. As a way to offer insight into the requisite methodological procedures for research in this area, we conducted two experiments that both explicate and address the issues raised. Our results call into question the utility of using ontology as a foundation for conceptual modeling practice. Furthermore, they suggest a contrary but at least equally plausible explanation for the results reported by Shanks et al. In conducting this work we hope to encourage dialogue that will be beneficial for future endeavors aimed at identifying, developing, and evaluating appropriate foundations for the discipline of conceptual modeling.",[],"March, Salvatore T.",N/A,"Owen Graduate School of Management, Vanderbilt University, Nashville, TN 37203 U.S.A."
https://misq.umn.edu/misq/article/36/3/965/535/The-Hole-in-the-Whole-A-Response-to-Allen-and,MIS Quarterly,The Hole in the Whole: A Response to Allen and March1,"Volume 36, Issue 3",September 2012,"Allen and March provide a critique of one of our papers in which we argue composites should be represented as entities/objects in a conceptual model rather than relationships/associations (Shanks et al. 2008). They contend we have addressed a non-issue. Furthermore, they argue our theoretical rationale and empirical evidence have flaws. In this paper, we provide a response to their arguments. We show that the issue we address is substantive. We show, also, that our theoretical analysis and empirical results are robust. We find, instead, that Allen and March’s theoretical arguments and empirical evidence have flaws.",[],"Shanks, Graeme",N/A,"Department of Computing and Information Systems, University of Melbourne, Parkville, Victoria, Australia 3052"
https://misq.umn.edu/misq/article/36/3/965/535/The-Hole-in-the-Whole-A-Response-to-Allen-and,MIS Quarterly,The Hole in the Whole: A Response to Allen and March1,"Volume 36, Issue 3",September 2012,"Allen and March provide a critique of one of our papers in which we argue composites should be represented as entities/objects in a conceptual model rather than relationships/associations (Shanks et al. 2008). They contend we have addressed a non-issue. Furthermore, they argue our theoretical rationale and empirical evidence have flaws. In this paper, we provide a response to their arguments. We show that the issue we address is substantive. We show, also, that our theoretical analysis and empirical results are robust. We find, instead, that Allen and March’s theoretical arguments and empirical evidence have flaws.",[],"Weber, Ron",N/A,"Faculty of Information Technology, Monash University, Caulfield East, Victoria, Australia 3145"
https://misq.umn.edu/misq/article/36/3/981/538/Does-PLS-Have-Advantages-for-Small-Sample-Size-or,MIS Quarterly,Does PLS Have Advantages for Small Sample Size or Non-Normal Data?1,"Volume 36, Issue 3",September 2012,"There is a pervasive belief in the MIS research community that PLS has advantages over other techniques when analyzing small sample sizes or data with non-normal distributions. Based on these beliefs, major MIS journals have published studies using PLS with sample sizes that would be deemed unacceptably small if used with other statistical techniques. We used Monte Carlo simulation more extensively than previous research to evaluate PLS, multiple regression, and LISREL in terms of accuracy and statistical power under varying conditions of sample size, normality of the data, number of indicators per construct, reliability of the indicators, and complexity of the research model. We found that PLS performed as effectively as the other techniques in detecting actual paths, and not falsely detecting non-existent paths. However, because PLS (like regression) apparently does not compensate for measurement error, PLS and regression were consistently less accurate than LISREL. When used with small sample sizes, PLS, like the other techniques, suffers from increased standard deviations, decreased statistical power,and reduced accuracy. All three techniques were remarkably robust against moderate departures from normality, and equally so. In total, we found that the similarities in results across the three techniques were much stronger than the differences.",[],"Goodhue, Dale L.",N/A,"Terry College of Business, MIS Department, University of Georgia, Athens, GA 30606 U.S.A"
https://misq.umn.edu/misq/article/36/3/981/538/Does-PLS-Have-Advantages-for-Small-Sample-Size-or,MIS Quarterly,Does PLS Have Advantages for Small Sample Size or Non-Normal Data?1,"Volume 36, Issue 3",September 2012,"There is a pervasive belief in the MIS research community that PLS has advantages over other techniques when analyzing small sample sizes or data with non-normal distributions. Based on these beliefs, major MIS journals have published studies using PLS with sample sizes that would be deemed unacceptably small if used with other statistical techniques. We used Monte Carlo simulation more extensively than previous research to evaluate PLS, multiple regression, and LISREL in terms of accuracy and statistical power under varying conditions of sample size, normality of the data, number of indicators per construct, reliability of the indicators, and complexity of the research model. We found that PLS performed as effectively as the other techniques in detecting actual paths, and not falsely detecting non-existent paths. However, because PLS (like regression) apparently does not compensate for measurement error, PLS and regression were consistently less accurate than LISREL. When used with small sample sizes, PLS, like the other techniques, suffers from increased standard deviations, decreased statistical power,and reduced accuracy. All three techniques were remarkably robust against moderate departures from normality, and equally so. In total, we found that the similarities in results across the three techniques were much stronger than the differences.",[],"Lewis, William",N/A,"Schools of Business, Wake Forest University, Winston-Salem, NC 27109 U.S.A."
https://misq.umn.edu/misq/article/36/3/981/538/Does-PLS-Have-Advantages-for-Small-Sample-Size-or,MIS Quarterly,Does PLS Have Advantages for Small Sample Size or Non-Normal Data?1,"Volume 36, Issue 3",September 2012,"There is a pervasive belief in the MIS research community that PLS has advantages over other techniques when analyzing small sample sizes or data with non-normal distributions. Based on these beliefs, major MIS journals have published studies using PLS with sample sizes that would be deemed unacceptably small if used with other statistical techniques. We used Monte Carlo simulation more extensively than previous research to evaluate PLS, multiple regression, and LISREL in terms of accuracy and statistical power under varying conditions of sample size, normality of the data, number of indicators per construct, reliability of the indicators, and complexity of the research model. We found that PLS performed as effectively as the other techniques in detecting actual paths, and not falsely detecting non-existent paths. However, because PLS (like regression) apparently does not compensate for measurement error, PLS and regression were consistently less accurate than LISREL. When used with small sample sizes, PLS, like the other techniques, suffers from increased standard deviations, decreased statistical power,and reduced accuracy. All three techniques were remarkably robust against moderate departures from normality, and equally so. In total, we found that the similarities in results across the three techniques were much stronger than the differences.",[],"Thompson, Ron",N/A,N/A
https://misq.umn.edu/misq/article/36/3/1003/536/Assessing-Common-Method-Bias-Problems-with-the,MIS Quarterly,Assessing Common Method Bias: Problems with the ULMC Technique1,"Volume 36, Issue 3",September 2012,"Recent work, in journals such as MIS Quarterly and Management Science, has highlighted the importance of evaluating the influence of common method bias (CMB) on the results of statistical analysis. In this research note, we assess the utility of the unmeasured latent method construct (ULMC) approach in partial least squares (PLS), introduced by Liang et al. (2007). Such an assessment of the ULMC approach is important, because it has been employed in 76 studies since it appeared in MIS Quarterly in early 2007. Using data generated via Monte Carlo simulations, we use PLS structural equation modeling (SEM) to demonstrate that the ULMC approach of Liang et al. is neither able to detect, nor control for, common method bias. Method estimates using this approach resulted in negligible estimates, regardless of whether there were some, large, or no method bias introduced in the simulated data. Our study contributes to the IS and research methods literature by illustrating that, and explaining why the ULMC approach does not accurately detect common method bias in PLS. Further, our results build on prior work done using covariance-based SEM questioning the usefulness of the ULMC technique for detecting CMB.",[],"Chin, Wynne W.",N/A,"C. T. Bauer College of Business, University of Houston, Houston, TX 77204, U.S.A.Department of Service Systems Management and Engineering, Sogang University, Seoul, Korea"
https://misq.umn.edu/misq/article/36/3/1003/536/Assessing-Common-Method-Bias-Problems-with-the,MIS Quarterly,Assessing Common Method Bias: Problems with the ULMC Technique1,"Volume 36, Issue 3",September 2012,"Recent work, in journals such as MIS Quarterly and Management Science, has highlighted the importance of evaluating the influence of common method bias (CMB) on the results of statistical analysis. In this research note, we assess the utility of the unmeasured latent method construct (ULMC) approach in partial least squares (PLS), introduced by Liang et al. (2007). Such an assessment of the ULMC approach is important, because it has been employed in 76 studies since it appeared in MIS Quarterly in early 2007. Using data generated via Monte Carlo simulations, we use PLS structural equation modeling (SEM) to demonstrate that the ULMC approach of Liang et al. is neither able to detect, nor control for, common method bias. Method estimates using this approach resulted in negligible estimates, regardless of whether there were some, large, or no method bias introduced in the simulated data. Our study contributes to the IS and research methods literature by illustrating that, and explaining why the ULMC approach does not accurately detect common method bias in PLS. Further, our results build on prior work done using covariance-based SEM questioning the usefulness of the ULMC technique for detecting CMB.",[],"Thatcher, Jason Bennett",N/A,"College of Business and Behavioral Science, Clemson University, Clemson, SC 29634-0701 U.S.A."
https://misq.umn.edu/misq/article/36/3/1003/536/Assessing-Common-Method-Bias-Problems-with-the,MIS Quarterly,Assessing Common Method Bias: Problems with the ULMC Technique1,"Volume 36, Issue 3",September 2012,"Recent work, in journals such as MIS Quarterly and Management Science, has highlighted the importance of evaluating the influence of common method bias (CMB) on the results of statistical analysis. In this research note, we assess the utility of the unmeasured latent method construct (ULMC) approach in partial least squares (PLS), introduced by Liang et al. (2007). Such an assessment of the ULMC approach is important, because it has been employed in 76 studies since it appeared in MIS Quarterly in early 2007. Using data generated via Monte Carlo simulations, we use PLS structural equation modeling (SEM) to demonstrate that the ULMC approach of Liang et al. is neither able to detect, nor control for, common method bias. Method estimates using this approach resulted in negligible estimates, regardless of whether there were some, large, or no method bias introduced in the simulated data. Our study contributes to the IS and research methods literature by illustrating that, and explaining why the ULMC approach does not accurately detect common method bias in PLS. Further, our results build on prior work done using covariance-based SEM questioning the usefulness of the ULMC technique for detecting CMB.",[],"Wright, Ryan T.",N/A,"Isenberg School of Management, University of Massachusetts, Amherst, Amherst, MA 01003 CA 94117 U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/36/4/1021/1494/Bidding-Behavior-Evolution-in-Sequential-Auctions,MIS Quarterly,Bidding Behavior Evolution in Sequential Auctions: Characterization and Analysis1,"Volume 36, Issue 4",December 2012,"Retailers are increasingly exploiting sequential online auctions as an effective and low cost distribution channel for disposing large quantities of inventory. In such auction environments, bidders have the opportunity of participating in many auctions to learn and choose the bidding strategy that best fits their preferences. Previous studies have mostly focused on identifying bidding strategies in single, isolated online auctions. Using a large data set collected from sequential online auctions, we first characterize bidding strategies in this interesting online environment and then develop an empirical model to explain bidders’ adoption of different strategies. We also examine how bidders change their strategies over time. Our findings challenge the general belief that bidders employ their strategies regardless of experience or their specific demand. We find that bidders’ demand, participation experience, and auction design parameters affect their choice of bidding strategies. Bidders with unit demand are likely to choose early bidding strategies, while those with multiple unit demand adopt late bidding strategies. Auction design parameters that affect bidders’ perception of demand and supply trends affect bidders’ choice of bidding strategies. As bidders gain experience within a sequence of auctions, they start choosing late bidding strategies. Our findings help auctioneers to design auction sequences that maximize their objectives.",[],"Goes, Paulo B.",N/A,"Management Information Systems, Eller College of Management, University of Arizona, 1130 E. Helen Street, Tucson, AZ 85719 U.S.A."
https://misq.umn.edu/misq/article/36/4/1021/1494/Bidding-Behavior-Evolution-in-Sequential-Auctions,MIS Quarterly,Bidding Behavior Evolution in Sequential Auctions: Characterization and Analysis1,"Volume 36, Issue 4",December 2012,"Retailers are increasingly exploiting sequential online auctions as an effective and low cost distribution channel for disposing large quantities of inventory. In such auction environments, bidders have the opportunity of participating in many auctions to learn and choose the bidding strategy that best fits their preferences. Previous studies have mostly focused on identifying bidding strategies in single, isolated online auctions. Using a large data set collected from sequential online auctions, we first characterize bidding strategies in this interesting online environment and then develop an empirical model to explain bidders’ adoption of different strategies. We also examine how bidders change their strategies over time. Our findings challenge the general belief that bidders employ their strategies regardless of experience or their specific demand. We find that bidders’ demand, participation experience, and auction design parameters affect their choice of bidding strategies. Bidders with unit demand are likely to choose early bidding strategies, while those with multiple unit demand adopt late bidding strategies. Auction design parameters that affect bidders’ perception of demand and supply trends affect bidders’ choice of bidding strategies. As bidders gain experience within a sequence of auctions, they start choosing late bidding strategies. Our findings help auctioneers to design auction sequences that maximize their objectives.",[],"Karuga, Gilbert G.",N/A,"School of Business, University of Kansas, 1300 Sunnyside Avenue, Lawrence, KS 66049 U.S.A."
https://misq.umn.edu/misq/article/36/4/1021/1494/Bidding-Behavior-Evolution-in-Sequential-Auctions,MIS Quarterly,Bidding Behavior Evolution in Sequential Auctions: Characterization and Analysis1,"Volume 36, Issue 4",December 2012,"Retailers are increasingly exploiting sequential online auctions as an effective and low cost distribution channel for disposing large quantities of inventory. In such auction environments, bidders have the opportunity of participating in many auctions to learn and choose the bidding strategy that best fits their preferences. Previous studies have mostly focused on identifying bidding strategies in single, isolated online auctions. Using a large data set collected from sequential online auctions, we first characterize bidding strategies in this interesting online environment and then develop an empirical model to explain bidders’ adoption of different strategies. We also examine how bidders change their strategies over time. Our findings challenge the general belief that bidders employ their strategies regardless of experience or their specific demand. We find that bidders’ demand, participation experience, and auction design parameters affect their choice of bidding strategies. Bidders with unit demand are likely to choose early bidding strategies, while those with multiple unit demand adopt late bidding strategies. Auction design parameters that affect bidders’ perception of demand and supply trends affect bidders’ choice of bidding strategies. As bidders gain experience within a sequence of auctions, they start choosing late bidding strategies. Our findings help auctioneers to design auction sequences that maximize their objectives.",[],"Tripathi, Arvind K.",N/A,"Information Systems and Operations Management, University of Auckland Business School, Auckland, New Zealand 1142"
https://misq.umn.edu/misq/article/36/4/1043/1482/Information-Technology-Outsourcing-Knowledge,MIS Quarterly,"Information Technology Outsourcing, Knowledge Transfer, and Firm Productivity: An Empirical Analysis1","Volume 36, Issue 4",December 2012,"Firms are increasingly sourcing internal information systems functions from external service providers. However, there is limited empirical evidence of the economic impact of this delivery option and, more specifically, of the productivity gains accruing to firms that have outsourced. Moreover, there is little evidence of the role and contributions of the individual mechanisms by which service providers create value for client firms. We are particularly interested in whether client firms benefit from the accumulated knowledge held by information technology (IT) service firms. In this paper, we examine the impact of IT outsourcing on the productivity of firms that choose this mode of services delivery focusing, on the role of IT-related knowledge. Since firms self-select into their optimal sourcing mode, we use a variety of econometric techniques including propensity score-based matching and switching regression to control for potential bias arising from endogenously determined sourcing modes. We demonstrate that IT outsourcing does lead to productivity gains for firms that select this mode of service delivery. Our results also suggest that IT-related knowledge held by IT services vendors enables these productivity gains, the magnitude of which is moderated by a firm’s IT intensity. Moreover, the value of outsourcing to a client firm increases with its propensity for outsourcing, which in turn depends on firm-specific attributes including efficiency level, financial leverage, and variability in business conditions. Our analyses also show that firms that outsource have been able to achieve additional productivity gains from contracting out compared with their counterfactuals.",[],"Chang, Young Bong",N/A,"School of Technology Management, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea 698-798"
https://misq.umn.edu/misq/article/36/4/1043/1482/Information-Technology-Outsourcing-Knowledge,MIS Quarterly,"Information Technology Outsourcing, Knowledge Transfer, and Firm Productivity: An Empirical Analysis1","Volume 36, Issue 4",December 2012,"Firms are increasingly sourcing internal information systems functions from external service providers. However, there is limited empirical evidence of the economic impact of this delivery option and, more specifically, of the productivity gains accruing to firms that have outsourced. Moreover, there is little evidence of the role and contributions of the individual mechanisms by which service providers create value for client firms. We are particularly interested in whether client firms benefit from the accumulated knowledge held by information technology (IT) service firms. In this paper, we examine the impact of IT outsourcing on the productivity of firms that choose this mode of services delivery focusing, on the role of IT-related knowledge. Since firms self-select into their optimal sourcing mode, we use a variety of econometric techniques including propensity score-based matching and switching regression to control for potential bias arising from endogenously determined sourcing modes. We demonstrate that IT outsourcing does lead to productivity gains for firms that select this mode of service delivery. Our results also suggest that IT-related knowledge held by IT services vendors enables these productivity gains, the magnitude of which is moderated by a firm’s IT intensity. Moreover, the value of outsourcing to a client firm increases with its propensity for outsourcing, which in turn depends on firm-specific attributes including efficiency level, financial leverage, and variability in business conditions. Our analyses also show that firms that outsource have been able to achieve additional productivity gains from contracting out compared with their counterfactuals.",[],"Gurbaxani, Vijay",N/A,"Center for Digital Transformation, The Paul Merage School of Business, University of California, Irvine, CA 92697-3125"
https://misq.umn.edu/misq/article/36/4/1065/1493/Impact-of-User-Satisfaction-with-Mandated-CRM-Use,MIS Quarterly,Impact of User Satisfaction with Mandated CRM Use on Employee Service Quality1,"Volume 36, Issue 4",December 2012,"An increasing number of organizations are now implementing customer relationship management (CRM) systems to support front-line employees’ service tasks. With the belief that CRM can enhance employees’ service quality, management often mandates employees to use the implemented CRM. However, challenges emerge if/when employees are dissatisfied with using the system. To understand the role of front-line employee users’ satisfaction with their mandated use of CRM in determining their service quality, we conducted a field study in one of the largest telecommunications service organizations in China and gathered time-lagged data from self-reported employee surveys, as well as from the firm’s archival data sources. Our results suggest that employees’ overall user satisfaction (UserSat) with their mandated use of CRM has a positive impact on employee service quality (ESQ) above and beyond the expected positive impacts that job dedication (JD) and embodied service knowledge (ESK) have on ESQ. Interestingly, the positive effect of UserSat on ESQ is comparable to the positive effects of JD and ESK, respectively, on ESQ. Importantly, UserSat and ESK have a substitutive effect on ESQ, suggesting that the impact of UserSat on ESQ is stronger/weaker for employees with lower/higher levels of ESK. Finally, ESQ predicts customer satisfaction with customer service employees (CSWCSE); ESQ also fully mediates the impacts of UserSat and ESK, and partially mediates the impact of JD, on CSWCSE. The results of this study emphasize the importance of user satisfaction in determining employees’ task outcomes when use of an information system is mandated.",[],"Po-An Hsieh, J. J.",N/A,"Department of Management and Marketing, Faculty of Business, Hong Kong Polytechnic University, Hong Kong"
https://misq.umn.edu/misq/article/36/4/1065/1493/Impact-of-User-Satisfaction-with-Mandated-CRM-Use,MIS Quarterly,Impact of User Satisfaction with Mandated CRM Use on Employee Service Quality1,"Volume 36, Issue 4",December 2012,"An increasing number of organizations are now implementing customer relationship management (CRM) systems to support front-line employees’ service tasks. With the belief that CRM can enhance employees’ service quality, management often mandates employees to use the implemented CRM. However, challenges emerge if/when employees are dissatisfied with using the system. To understand the role of front-line employee users’ satisfaction with their mandated use of CRM in determining their service quality, we conducted a field study in one of the largest telecommunications service organizations in China and gathered time-lagged data from self-reported employee surveys, as well as from the firm’s archival data sources. Our results suggest that employees’ overall user satisfaction (UserSat) with their mandated use of CRM has a positive impact on employee service quality (ESQ) above and beyond the expected positive impacts that job dedication (JD) and embodied service knowledge (ESK) have on ESQ. Interestingly, the positive effect of UserSat on ESQ is comparable to the positive effects of JD and ESK, respectively, on ESQ. Importantly, UserSat and ESK have a substitutive effect on ESQ, suggesting that the impact of UserSat on ESQ is stronger/weaker for employees with lower/higher levels of ESK. Finally, ESQ predicts customer satisfaction with customer service employees (CSWCSE); ESQ also fully mediates the impacts of UserSat and ESK, and partially mediates the impact of JD, on CSWCSE. The results of this study emphasize the importance of user satisfaction in determining employees’ task outcomes when use of an information system is mandated.",[],"Rai, Arun",N/A,"Center for Process Innovation and Department of Computer Information Systems, Robinson College of Business, Georgia State University, Atlanta, GA 30303 U.S.A."
https://misq.umn.edu/misq/article/36/4/1065/1493/Impact-of-User-Satisfaction-with-Mandated-CRM-Use,MIS Quarterly,Impact of User Satisfaction with Mandated CRM Use on Employee Service Quality1,"Volume 36, Issue 4",December 2012,"An increasing number of organizations are now implementing customer relationship management (CRM) systems to support front-line employees’ service tasks. With the belief that CRM can enhance employees’ service quality, management often mandates employees to use the implemented CRM. However, challenges emerge if/when employees are dissatisfied with using the system. To understand the role of front-line employee users’ satisfaction with their mandated use of CRM in determining their service quality, we conducted a field study in one of the largest telecommunications service organizations in China and gathered time-lagged data from self-reported employee surveys, as well as from the firm’s archival data sources. Our results suggest that employees’ overall user satisfaction (UserSat) with their mandated use of CRM has a positive impact on employee service quality (ESQ) above and beyond the expected positive impacts that job dedication (JD) and embodied service knowledge (ESK) have on ESQ. Interestingly, the positive effect of UserSat on ESQ is comparable to the positive effects of JD and ESK, respectively, on ESQ. Importantly, UserSat and ESK have a substitutive effect on ESQ, suggesting that the impact of UserSat on ESQ is stronger/weaker for employees with lower/higher levels of ESK. Finally, ESQ predicts customer satisfaction with customer service employees (CSWCSE); ESQ also fully mediates the impacts of UserSat and ESK, and partially mediates the impact of JD, on CSWCSE. The results of this study emphasize the importance of user satisfaction in determining employees’ task outcomes when use of an information system is mandated.",[],"Petter, Stacie",N/A,"Information Systems and Quantitative Analysis, College of Information Science and Technology, University of Nebraska, Omaha, NE 68182-0392 U.S.A."
https://misq.umn.edu/misq/article/36/4/1065/1493/Impact-of-User-Satisfaction-with-Mandated-CRM-Use,MIS Quarterly,Impact of User Satisfaction with Mandated CRM Use on Employee Service Quality1,"Volume 36, Issue 4",December 2012,"An increasing number of organizations are now implementing customer relationship management (CRM) systems to support front-line employees’ service tasks. With the belief that CRM can enhance employees’ service quality, management often mandates employees to use the implemented CRM. However, challenges emerge if/when employees are dissatisfied with using the system. To understand the role of front-line employee users’ satisfaction with their mandated use of CRM in determining their service quality, we conducted a field study in one of the largest telecommunications service organizations in China and gathered time-lagged data from self-reported employee surveys, as well as from the firm’s archival data sources. Our results suggest that employees’ overall user satisfaction (UserSat) with their mandated use of CRM has a positive impact on employee service quality (ESQ) above and beyond the expected positive impacts that job dedication (JD) and embodied service knowledge (ESK) have on ESQ. Interestingly, the positive effect of UserSat on ESQ is comparable to the positive effects of JD and ESK, respectively, on ESQ. Importantly, UserSat and ESK have a substitutive effect on ESQ, suggesting that the impact of UserSat on ESQ is stronger/weaker for employees with lower/higher levels of ESK. Finally, ESQ predicts customer satisfaction with customer service employees (CSWCSE); ESQ also fully mediates the impacts of UserSat and ESK, and partially mediates the impact of JD, on CSWCSE. The results of this study emphasize the importance of user satisfaction in determining employees’ task outcomes when use of an information system is mandated.",[],"Zhang, Ting",N/A,"School of Management, Xi'an Jiaotong University, Xi'an, Shaanxi 710049 China"
https://misq.umn.edu/misq/article/36/4/1081/1498/Dialectics-of-Collective-Minding-Contradictory,MIS Quarterly,Dialectics of Collective Minding: Contradictory Appropriations of Information Technology in a High-Risk Project1,"Volume 36, Issue 4",December 2012,"In unpredictable and unforgiving environments, organizations need to act with care and reliability, often referred to as collective mindfulness. We present a theory-generating, interpretative field study of a highly complex and successful building project by architect Frank O. Gehry. We argue that what has been labeled collective mindfulness is only possible through a dialectic process of collective minding, in which organizational actors simultaneously exhibit elements of being mindful and mindless. Our analysis reveals that collective minding emerges from struggling with contradictions in the five elements of mindfulness. We argue that when actors struggle with these dialectic tensions, the same information technology capabilities are enacted as multiple, contradictory technologies-in-practice. Implications for the further study of collective minding and the appropriation of IT capabilities are discussed.",[],"Carlo, Jessica Luo",N/A,"Department of Advertising, Public Relations, and Retailing, Michigan State University, East Lansing, MI 48824-1212 U.S.A."
https://misq.umn.edu/misq/article/36/4/1081/1498/Dialectics-of-Collective-Minding-Contradictory,MIS Quarterly,Dialectics of Collective Minding: Contradictory Appropriations of Information Technology in a High-Risk Project1,"Volume 36, Issue 4",December 2012,"In unpredictable and unforgiving environments, organizations need to act with care and reliability, often referred to as collective mindfulness. We present a theory-generating, interpretative field study of a highly complex and successful building project by architect Frank O. Gehry. We argue that what has been labeled collective mindfulness is only possible through a dialectic process of collective minding, in which organizational actors simultaneously exhibit elements of being mindful and mindless. Our analysis reveals that collective minding emerges from struggling with contradictions in the five elements of mindfulness. We argue that when actors struggle with these dialectic tensions, the same information technology capabilities are enacted as multiple, contradictory technologies-in-practice. Implications for the further study of collective minding and the appropriation of IT capabilities are discussed.",[],"Lyytinen, Kalle",N/A,"Department of Information Systems, Weatherhead School of Management, Case Western Reserve University, Cleveland, OH 44106 U.S.A."
https://misq.umn.edu/misq/article/36/4/1081/1498/Dialectics-of-Collective-Minding-Contradictory,MIS Quarterly,Dialectics of Collective Minding: Contradictory Appropriations of Information Technology in a High-Risk Project1,"Volume 36, Issue 4",December 2012,"In unpredictable and unforgiving environments, organizations need to act with care and reliability, often referred to as collective mindfulness. We present a theory-generating, interpretative field study of a highly complex and successful building project by architect Frank O. Gehry. We argue that what has been labeled collective mindfulness is only possible through a dialectic process of collective minding, in which organizational actors simultaneously exhibit elements of being mindful and mindless. Our analysis reveals that collective minding emerges from struggling with contradictions in the five elements of mindfulness. We argue that when actors struggle with these dialectic tensions, the same information technology capabilities are enacted as multiple, contradictory technologies-in-practice. Implications for the further study of collective minding and the appropriation of IT capabilities are discussed.",[],"Boland, Richard J.",N/A,"Department of Information Systems, Weatherhead School of Management, Case Western Reserve University, Cleveland, OH 44106 U.S.A."
https://misq.umn.edu/misq/article/36/4/1109/1474/Growth-and-Sustainability-of-Managed-Security,MIS Quarterly,Growth and Sustainability of Managed Security Services Networks: An Economic Perspective1,"Volume 36, Issue 4",December 2012,"Managed security service provider (MSSP) networks are a form of collaboration where several firms share resources such as diagnostics, prevention tools, and policies to provide security for their computer networks. While the decision to outsource the security operations of an organization may seem counterintuitive, there are potential benefits from joining an MSSP network that include pooling of risk and access to more security-enabling resources and expertise. We examine structural results explaining the reasons firms join an MSSP network, and characterize the growth of MSSP network size under different forms of ownership (monopoly versus consortium). We find that the need for an initial investment in MSSP networks (which is necessary to overcome the stalling effect) only affects the optimal network size for a consortium but has no impact on the optimal network size for a profit-maximizing monopolist. Our results provide an explanation why the majority of the MSSPs are for-profit entities and consortium-based MSSPs are less common. Such a market structure can be attributed to the potential for larger size by the for-profit MSSP owner combined with beneficial pricing structure and a lack of growth uncertainty for the early clients.",[],"Gupta, Alok",N/A,"Department of Information and Decision Sciences, Carlson School of Management, University of Minnesota, Minneapolis, MN 55455 U.S.A."
https://misq.umn.edu/misq/article/36/4/1109/1474/Growth-and-Sustainability-of-Managed-Security,MIS Quarterly,Growth and Sustainability of Managed Security Services Networks: An Economic Perspective1,"Volume 36, Issue 4",December 2012,"Managed security service provider (MSSP) networks are a form of collaboration where several firms share resources such as diagnostics, prevention tools, and policies to provide security for their computer networks. While the decision to outsource the security operations of an organization may seem counterintuitive, there are potential benefits from joining an MSSP network that include pooling of risk and access to more security-enabling resources and expertise. We examine structural results explaining the reasons firms join an MSSP network, and characterize the growth of MSSP network size under different forms of ownership (monopoly versus consortium). We find that the need for an initial investment in MSSP networks (which is necessary to overcome the stalling effect) only affects the optimal network size for a consortium but has no impact on the optimal network size for a profit-maximizing monopolist. Our results provide an explanation why the majority of the MSSPs are for-profit entities and consortium-based MSSPs are less common. Such a market structure can be attributed to the potential for larger size by the for-profit MSSP owner combined with beneficial pricing structure and a lack of growth uncertainty for the early clients.",[],"Zhdanov, Dmitry",N/A,"Department of Operations & Information Management, School of Business, University of Connecticut, Storrs, CT 06269 U.S.A."
https://misq.umn.edu/misq/article/36/4/1131/1476/The-Effectiveness-of-Online-Shopping,MIS Quarterly,The Effectiveness of Online Shopping Characteristics and Well-Designed Websites on Satisfaction1,"Volume 36, Issue 4",December 2012,"Electronic commerce has grown rapidly in recent years. However, surveys of online customers continue to indicate that many remain unsatisfied with their online purchase experiences. Clearly, more research is needed to better understand what affects customers’ evaluations of their online experiences. Through a large dataset gathered from two online websites, this study investigates the importance of product uncertainty and retailer visibility in customers’ online purchase decisions, as well as the mitigating effects of retailer characteristics. We find that high product uncertainty and low retailer visibility have a negative impact on customer satisfaction. However, a retailer’s service quality, website design, and pricing play important roles in mitigating the negative impact of high product uncertainty and low retailer visibility. Specifically, service quality can mitigate the negative impacts of low retailer visibility and high product uncertainty in online markets. Website design, on the other hand, helps to reduce the impact of product uncertainty when experience goods are involved.",[],"Luo, Jifeng",N/A,"Antai College of Economics & Management, Shanghai Jiao Tong University, Shanghai 200052, China"
https://misq.umn.edu/misq/article/36/4/1131/1476/The-Effectiveness-of-Online-Shopping,MIS Quarterly,The Effectiveness of Online Shopping Characteristics and Well-Designed Websites on Satisfaction1,"Volume 36, Issue 4",December 2012,"Electronic commerce has grown rapidly in recent years. However, surveys of online customers continue to indicate that many remain unsatisfied with their online purchase experiences. Clearly, more research is needed to better understand what affects customers’ evaluations of their online experiences. Through a large dataset gathered from two online websites, this study investigates the importance of product uncertainty and retailer visibility in customers’ online purchase decisions, as well as the mitigating effects of retailer characteristics. We find that high product uncertainty and low retailer visibility have a negative impact on customer satisfaction. However, a retailer’s service quality, website design, and pricing play important roles in mitigating the negative impact of high product uncertainty and low retailer visibility. Specifically, service quality can mitigate the negative impacts of low retailer visibility and high product uncertainty in online markets. Website design, on the other hand, helps to reduce the impact of product uncertainty when experience goods are involved.",[],"Ba, Sulin",N/A,"Department of Operations and Information Management, School of Business, University of Connecticut, 2100 Hillside Drive, Storrs, CT 06269-1041 U.S.A."
https://misq.umn.edu/misq/article/36/4/1131/1476/The-Effectiveness-of-Online-Shopping,MIS Quarterly,The Effectiveness of Online Shopping Characteristics and Well-Designed Websites on Satisfaction1,"Volume 36, Issue 4",December 2012,"Electronic commerce has grown rapidly in recent years. However, surveys of online customers continue to indicate that many remain unsatisfied with their online purchase experiences. Clearly, more research is needed to better understand what affects customers’ evaluations of their online experiences. Through a large dataset gathered from two online websites, this study investigates the importance of product uncertainty and retailer visibility in customers’ online purchase decisions, as well as the mitigating effects of retailer characteristics. We find that high product uncertainty and low retailer visibility have a negative impact on customer satisfaction. However, a retailer’s service quality, website design, and pricing play important roles in mitigating the negative impact of high product uncertainty and low retailer visibility. Specifically, service quality can mitigate the negative impacts of low retailer visibility and high product uncertainty in online markets. Website design, on the other hand, helps to reduce the impact of product uncertainty when experience goods are involved.",[],"Zhang, Han",N/A,"Scheller College of Business, Georgia Institute of Technology, 800 West Peachtree Street NW, Atlanta, GA 30308-0520 U.S.A."
https://misq.umn.edu/misq/article/36/4/1145/1479/Does-Information-Technology-Investment-Influence-a,MIS Quarterly,Does Information Technology Investment Influence a Firm’s Market Value? A Case of Non-Publicly Traded Healthcare Firms1,"Volume 36, Issue 4",December 2012,"Managers make informed information technology investment decisions when they are able to quantify how IT contributes to firm performance. While financial accounting measures inform IT’s influence on retrospective firm performance, senior managers expect evidence of how IT influences prospective measures such as the firm’s market value. We examine the efficacy of IT’s influence on firm value combined with measures of financial performance for non-publicly traded (NPT) hospitals that lack conventional market-based measures. We gathered actual sale transactions for NPT hospitals in the United States to derive the q ratio, a measure of market value. Our findings indicate that the influence of IT investment on the firm is more pronounced and statistically significant on firm value than exclusively on the accounting performance measures. Specifically, we find that the impact of IT investment is not significant on return on assets (ROA) and operating income for the same set of hospitals. This research note contributes to research and practice by demonstrating that the overall impact of IT is better understood when accounting measures are complemented with the firm’s market value. Such market valuation is also critical in merger and acquisition decisions, an activity that is likely to accelerate in the healthcare industry. Our findings provide hospitals, as well as other NPT firms, with insights into the impact of IT investment and a pragmatic approach to demonstrating IT’s contribution to firm value.",[],"Kohli, Rajiv",N/A,"Mason School of Business, The College of William & Mary, P.O. Box 8795, Williamsburg, VA 23187-8795 U.S.A."
https://misq.umn.edu/misq/article/36/4/1145/1479/Does-Information-Technology-Investment-Influence-a,MIS Quarterly,Does Information Technology Investment Influence a Firm’s Market Value? A Case of Non-Publicly Traded Healthcare Firms1,"Volume 36, Issue 4",December 2012,"Managers make informed information technology investment decisions when they are able to quantify how IT contributes to firm performance. While financial accounting measures inform IT’s influence on retrospective firm performance, senior managers expect evidence of how IT influences prospective measures such as the firm’s market value. We examine the efficacy of IT’s influence on firm value combined with measures of financial performance for non-publicly traded (NPT) hospitals that lack conventional market-based measures. We gathered actual sale transactions for NPT hospitals in the United States to derive the q ratio, a measure of market value. Our findings indicate that the influence of IT investment on the firm is more pronounced and statistically significant on firm value than exclusively on the accounting performance measures. Specifically, we find that the impact of IT investment is not significant on return on assets (ROA) and operating income for the same set of hospitals. This research note contributes to research and practice by demonstrating that the overall impact of IT is better understood when accounting measures are complemented with the firm’s market value. Such market valuation is also critical in merger and acquisition decisions, an activity that is likely to accelerate in the healthcare industry. Our findings provide hospitals, as well as other NPT firms, with insights into the impact of IT investment and a pragmatic approach to demonstrating IT’s contribution to firm value.",[],"Devaraj, Sarv",N/A,"Management Department, Mendoza College of Business, University of Notre Dame, Notre Dame, IN 46556 U.S.A."
https://misq.umn.edu/misq/article/36/4/1145/1479/Does-Information-Technology-Investment-Influence-a,MIS Quarterly,Does Information Technology Investment Influence a Firm’s Market Value? A Case of Non-Publicly Traded Healthcare Firms1,"Volume 36, Issue 4",December 2012,"Managers make informed information technology investment decisions when they are able to quantify how IT contributes to firm performance. While financial accounting measures inform IT’s influence on retrospective firm performance, senior managers expect evidence of how IT influences prospective measures such as the firm’s market value. We examine the efficacy of IT’s influence on firm value combined with measures of financial performance for non-publicly traded (NPT) hospitals that lack conventional market-based measures. We gathered actual sale transactions for NPT hospitals in the United States to derive the q ratio, a measure of market value. Our findings indicate that the influence of IT investment on the firm is more pronounced and statistically significant on firm value than exclusively on the accounting performance measures. Specifically, we find that the impact of IT investment is not significant on return on assets (ROA) and operating income for the same set of hospitals. This research note contributes to research and practice by demonstrating that the overall impact of IT is better understood when accounting measures are complemented with the firm’s market value. Such market valuation is also critical in merger and acquisition decisions, an activity that is likely to accelerate in the healthcare industry. Our findings provide hospitals, as well as other NPT firms, with insights into the impact of IT investment and a pragmatic approach to demonstrating IT’s contribution to firm value.",[],"Ow, Terence T.",N/A,"Department of Management, College of Business Administration, Marquette University, Milwaukee, WI 53201-1881 U.S.A."https://misq.umn.edu/misq/article/36/4/iii/1475/Editor-s-CommentsUse1,MIS Quarterly,Editor’s CommentsUse1,"Volume 36, Issue 4",December 2012,N/A,[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/36/4/1165/1483/Business-Intelligence-and-Analytics-From-Big-Data,MIS Quarterly,Business Intelligence and Analytics: From Big Data to Big Impact,"Volume 36, Issue 4",December 2012,"Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework.",[],"Chen, Hsinchun",N/A,"Eller College of Management, University of Arizona, Tucson, AZ 85721 U.S.A."
https://misq.umn.edu/misq/article/36/4/1165/1483/Business-Intelligence-and-Analytics-From-Big-Data,MIS Quarterly,Business Intelligence and Analytics: From Big Data to Big Impact,"Volume 36, Issue 4",December 2012,"Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework.",[],"Chiang, Roger H. L.",N/A,"Carl H. Lindner College of Business, University of Cincinnati, Cincinnati, OH 45221-0211 U.S.A."
https://misq.umn.edu/misq/article/36/4/1165/1483/Business-Intelligence-and-Analytics-From-Big-Data,MIS Quarterly,Business Intelligence and Analytics: From Big Data to Big Impact,"Volume 36, Issue 4",December 2012,"Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework.",[],"Storey, Veda C.",N/A,"J. Mack Robinson College of Business, Georgia State University, Atlanta, GA 30302-4015 U.S.A."
https://misq.umn.edu/misq/article/36/4/iii/1475/Editor-s-CommentsUse1,MIS Quarterly,Editor’s CommentsUse1,"Volume 36, Issue 4",December 2012,N/A,[],"Straub, Detmar",N/A,Georgia State University
https://misq.umn.edu/misq/article/36/4/iii/1475/Editor-s-CommentsUse1,MIS Quarterly,Editor’s CommentsUse1,"Volume 36, Issue 4",December 2012,N/A,[],"Giudice, Manlio del",N/A,"Faculty of Science Studies, Second University of Naples"
https://misq.umn.edu/misq/article/36/4/1189/1488/Business-Intelligence-in-BLOGS-Understanding,MIS Quarterly,Business Intelligence in BLOGS: Understanding Consumer Interactions and Communities1,"Volume 36, Issue 4",December 2012,"The increasing popularity of Web 2.0 has led to exponential growth of user-generated content in both volume and significance. One important type of user-generated content is the blog. Blogs encompass useful information (e.g., insightful product reviews and information-rich consumer communities) that could potentially be a gold mine for business intelligence, bringing great opportunities for both academic research and business applications. However, performing business intelligence on blogs is quite challenging because of the vast amount of information and the lack of commonly adopted methodology for effectively collecting and analyzing such information. In this paper, we propose a framework for gathering business intelligence from blogs by automatically collecting and analyzing blog contents and bloggers’ interaction networks. Through a system developed using the framework, we conducted two case studies with one case focusing on a consumer product and the other on a company. Our case studies demonstrate how to use the framework and appropriate techniques to effectively collect, extract, and analyze blogs related to the topics of interest, reveal novel patterns in the blogger interactions and communities, and answer important business intelligence questions in the domains. The framework is sufficiently generic and can be applied to any topics of interest, organizations, and products. Future academic research and business applications related to the topics examined in the two cases can also be built using the findings of this study.",[],"Chau, Michael",N/A,"School of Business, The University of Hong Kong, Pokfulam, Hong Kong"
https://misq.umn.edu/misq/article/36/4/1189/1488/Business-Intelligence-in-BLOGS-Understanding,MIS Quarterly,Business Intelligence in BLOGS: Understanding Consumer Interactions and Communities1,"Volume 36, Issue 4",December 2012,"The increasing popularity of Web 2.0 has led to exponential growth of user-generated content in both volume and significance. One important type of user-generated content is the blog. Blogs encompass useful information (e.g., insightful product reviews and information-rich consumer communities) that could potentially be a gold mine for business intelligence, bringing great opportunities for both academic research and business applications. However, performing business intelligence on blogs is quite challenging because of the vast amount of information and the lack of commonly adopted methodology for effectively collecting and analyzing such information. In this paper, we propose a framework for gathering business intelligence from blogs by automatically collecting and analyzing blog contents and bloggers’ interaction networks. Through a system developed using the framework, we conducted two case studies with one case focusing on a consumer product and the other on a company. Our case studies demonstrate how to use the framework and appropriate techniques to effectively collect, extract, and analyze blogs related to the topics of interest, reveal novel patterns in the blogger interactions and communities, and answer important business intelligence questions in the domains. The framework is sufficiently generic and can be applied to any topics of interest, organizations, and products. Future academic research and business applications related to the topics examined in the two cases can also be built using the findings of this study.",[],"Xu, Jennifer",N/A,"Computer Information Systems, Bentley University, Waltham, MA 02452 U.S.A."
https://misq.umn.edu/misq/article/36/4/1217/1496/A-Social-Network-Based-Inference-Model-for,MIS Quarterly,A Social Network-Based Inference Model for Validating Customer Profile Data1,"Volume 36, Issue 4",December 2012,"Drawing from the social and relational perspectives, this study offers an innovative conceptualization and operational approach regarding the validation of self-reported customer demographic data, which has become an essential corporate asset for harnessing business intelligence. Specifically, based on social network and homophily paradigms in which individuals have a natural tendency to associate and interact frequently with others with similar characteristics, we constructed a relational inference model to determine the accuracy of self-administered consumer profiles. In addition, to further enhance the reliability of our model’s prediction capability, we employed the entropy mechanism that minimizes potential biases that may arise from a simple probabilistic approach. To empirically validate the accuracy of our inference framework, we obtained and analyzed over 20 million actual call transactions supplied by one of the largest global telecommunication service providers. The results suggest that our social network-based inference model consistently outperforms other competing mechanisms (e.g., weighted average and simple relational classifier) regardless of the criteria choice (e.g., number of call receivers, call duration, and call frequency), with an accuracy rate of approximately 93 percent. Finally, to confirm the generalizability of our findings, we conducted simulation experiments to validate the robustness of the results in response to variations in parameter values and increases in potential noise in the data. We discuss several implications related to business intelligence for both research and practice, and offer new directions for future studies.",[],"Park, Sung-Hyuk",N/A,"College of Business, Korea Advanced Institute of Science and Technology, 207-43 Chongryangri-dong, Dongdaemoon-gu, Seoul 130-722 Korea"
https://misq.umn.edu/misq/article/36/4/1217/1496/A-Social-Network-Based-Inference-Model-for,MIS Quarterly,A Social Network-Based Inference Model for Validating Customer Profile Data1,"Volume 36, Issue 4",December 2012,"Drawing from the social and relational perspectives, this study offers an innovative conceptualization and operational approach regarding the validation of self-reported customer demographic data, which has become an essential corporate asset for harnessing business intelligence. Specifically, based on social network and homophily paradigms in which individuals have a natural tendency to associate and interact frequently with others with similar characteristics, we constructed a relational inference model to determine the accuracy of self-administered consumer profiles. In addition, to further enhance the reliability of our model’s prediction capability, we employed the entropy mechanism that minimizes potential biases that may arise from a simple probabilistic approach. To empirically validate the accuracy of our inference framework, we obtained and analyzed over 20 million actual call transactions supplied by one of the largest global telecommunication service providers. The results suggest that our social network-based inference model consistently outperforms other competing mechanisms (e.g., weighted average and simple relational classifier) regardless of the criteria choice (e.g., number of call receivers, call duration, and call frequency), with an accuracy rate of approximately 93 percent. Finally, to confirm the generalizability of our findings, we conducted simulation experiments to validate the robustness of the results in response to variations in parameter values and increases in potential noise in the data. We discuss several implications related to business intelligence for both research and practice, and offer new directions for future studies.",[],"Huh, Soon-Young",N/A,"College of Business, Korea Advanced Institute of Science and Technology, 207-43 Chongryangri-dong, Dongdaemoon-gu, Seoul 130-722 Korea"
https://misq.umn.edu/misq/article/36/4/1217/1496/A-Social-Network-Based-Inference-Model-for,MIS Quarterly,A Social Network-Based Inference Model for Validating Customer Profile Data1,"Volume 36, Issue 4",December 2012,"Drawing from the social and relational perspectives, this study offers an innovative conceptualization and operational approach regarding the validation of self-reported customer demographic data, which has become an essential corporate asset for harnessing business intelligence. Specifically, based on social network and homophily paradigms in which individuals have a natural tendency to associate and interact frequently with others with similar characteristics, we constructed a relational inference model to determine the accuracy of self-administered consumer profiles. In addition, to further enhance the reliability of our model’s prediction capability, we employed the entropy mechanism that minimizes potential biases that may arise from a simple probabilistic approach. To empirically validate the accuracy of our inference framework, we obtained and analyzed over 20 million actual call transactions supplied by one of the largest global telecommunication service providers. The results suggest that our social network-based inference model consistently outperforms other competing mechanisms (e.g., weighted average and simple relational classifier) regardless of the criteria choice (e.g., number of call receivers, call duration, and call frequency), with an accuracy rate of approximately 93 percent. Finally, to confirm the generalizability of our findings, we conducted simulation experiments to validate the robustness of the results in response to variations in parameter values and increases in potential noise in the data. We discuss several implications related to business intelligence for both research and practice, and offer new directions for future studies.",[],"Oh, Wonseok",N/A,"School of Business, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul 120-749 Korea"
https://misq.umn.edu/misq/article/36/4/1217/1496/A-Social-Network-Based-Inference-Model-for,MIS Quarterly,A Social Network-Based Inference Model for Validating Customer Profile Data1,"Volume 36, Issue 4",December 2012,"Drawing from the social and relational perspectives, this study offers an innovative conceptualization and operational approach regarding the validation of self-reported customer demographic data, which has become an essential corporate asset for harnessing business intelligence. Specifically, based on social network and homophily paradigms in which individuals have a natural tendency to associate and interact frequently with others with similar characteristics, we constructed a relational inference model to determine the accuracy of self-administered consumer profiles. In addition, to further enhance the reliability of our model’s prediction capability, we employed the entropy mechanism that minimizes potential biases that may arise from a simple probabilistic approach. To empirically validate the accuracy of our inference framework, we obtained and analyzed over 20 million actual call transactions supplied by one of the largest global telecommunication service providers. The results suggest that our social network-based inference model consistently outperforms other competing mechanisms (e.g., weighted average and simple relational classifier) regardless of the criteria choice (e.g., number of call receivers, call duration, and call frequency), with an accuracy rate of approximately 93 percent. Finally, to confirm the generalizability of our findings, we conducted simulation experiments to validate the robustness of the results in response to variations in parameter values and increases in potential noise in the data. We discuss several implications related to business intelligence for both research and practice, and offer new directions for future studies.",[],"Han, Sang Pil",N/A,"College of Business, City University of Hong Kong, Suite P7913, Information Systems, 83 Tat Chee Avenue, Kowloon Tong, Hong Kong"
https://misq.umn.edu/misq/article/36/4/1239/1486/Web-2-0-Environmental-Scanning-and-Adaptive,MIS Quarterly,Web 2.0 Environmental Scanning and Adaptive Decision Support for Business Mergers and Acquisitions1,"Volume 36, Issue 4",December 2012,"Globalization has triggered a rapid increase in cross-border mergers and acquisitions (M&As). However, research shows that only 17 percent of cross-border M&As create shareholder value. One of the main reasons for this poor track record is top management’s lack of attention to nonfinancial aspects (e.g., sociocultural aspects) of M&As. With the rapid growth of Web 2.0 applications, online environmental scanning provides top executives with unprecedented opportunities to tap into collective web intelligence to develop better insights about the sociocultural and political–economic factors that cross-border M&As face. Grounded in Porter’s five forces model, one major contribution of our research is the design of a novel due diligence scorecard model that leverages collective web intelligence to enhance M&A decision making. Another important contribution of our work is the design and development of an adaptive business intelligence (BI) 2.0 system underpinned by an evolutionary learning approach, domain-specific sentiment analysis, and business relation mining to operationalize the aforementioned scorecard model for adaptive M&A decision support. With Chinese companies’ cross-border M&As as the business context, our experimental results confirm that the proposed adaptive BI 2.0 system can significantly aid decision makers under different M&A scenarios. The managerial implication of our findings is that firms can apply the proposed BI 2.0 technology to enhance their strategic decision making, particularly when making cross-border investments in targeted markets for which private information may not be readily available.",[],"Lau, Raymond Y. K.",N/A,"Department of Information Systems, City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong SAR, China"
https://misq.umn.edu/misq/article/36/4/1239/1486/Web-2-0-Environmental-Scanning-and-Adaptive,MIS Quarterly,Web 2.0 Environmental Scanning and Adaptive Decision Support for Business Mergers and Acquisitions1,"Volume 36, Issue 4",December 2012,"Globalization has triggered a rapid increase in cross-border mergers and acquisitions (M&As). However, research shows that only 17 percent of cross-border M&As create shareholder value. One of the main reasons for this poor track record is top management’s lack of attention to nonfinancial aspects (e.g., sociocultural aspects) of M&As. With the rapid growth of Web 2.0 applications, online environmental scanning provides top executives with unprecedented opportunities to tap into collective web intelligence to develop better insights about the sociocultural and political–economic factors that cross-border M&As face. Grounded in Porter’s five forces model, one major contribution of our research is the design of a novel due diligence scorecard model that leverages collective web intelligence to enhance M&A decision making. Another important contribution of our work is the design and development of an adaptive business intelligence (BI) 2.0 system underpinned by an evolutionary learning approach, domain-specific sentiment analysis, and business relation mining to operationalize the aforementioned scorecard model for adaptive M&A decision support. With Chinese companies’ cross-border M&As as the business context, our experimental results confirm that the proposed adaptive BI 2.0 system can significantly aid decision makers under different M&A scenarios. The managerial implication of our findings is that firms can apply the proposed BI 2.0 technology to enhance their strategic decision making, particularly when making cross-border investments in targeted markets for which private information may not be readily available.",[],"Liao, Stephen S. Y.",N/A,"Department of Information Systems, City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong SAR, China"
https://misq.umn.edu/misq/article/36/4/1239/1486/Web-2-0-Environmental-Scanning-and-Adaptive,MIS Quarterly,Web 2.0 Environmental Scanning and Adaptive Decision Support for Business Mergers and Acquisitions1,"Volume 36, Issue 4",December 2012,"Globalization has triggered a rapid increase in cross-border mergers and acquisitions (M&As). However, research shows that only 17 percent of cross-border M&As create shareholder value. One of the main reasons for this poor track record is top management’s lack of attention to nonfinancial aspects (e.g., sociocultural aspects) of M&As. With the rapid growth of Web 2.0 applications, online environmental scanning provides top executives with unprecedented opportunities to tap into collective web intelligence to develop better insights about the sociocultural and political–economic factors that cross-border M&As face. Grounded in Porter’s five forces model, one major contribution of our research is the design of a novel due diligence scorecard model that leverages collective web intelligence to enhance M&A decision making. Another important contribution of our work is the design and development of an adaptive business intelligence (BI) 2.0 system underpinned by an evolutionary learning approach, domain-specific sentiment analysis, and business relation mining to operationalize the aforementioned scorecard model for adaptive M&A decision support. With Chinese companies’ cross-border M&As as the business context, our experimental results confirm that the proposed adaptive BI 2.0 system can significantly aid decision makers under different M&A scenarios. The managerial implication of our findings is that firms can apply the proposed BI 2.0 technology to enhance their strategic decision making, particularly when making cross-border investments in targeted markets for which private information may not be readily available.",[],"Wong, K. F.",N/A,"Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China"
https://misq.umn.edu/misq/article/36/4/1239/1486/Web-2-0-Environmental-Scanning-and-Adaptive,MIS Quarterly,Web 2.0 Environmental Scanning and Adaptive Decision Support for Business Mergers and Acquisitions1,"Volume 36, Issue 4",December 2012,"Globalization has triggered a rapid increase in cross-border mergers and acquisitions (M&As). However, research shows that only 17 percent of cross-border M&As create shareholder value. One of the main reasons for this poor track record is top management’s lack of attention to nonfinancial aspects (e.g., sociocultural aspects) of M&As. With the rapid growth of Web 2.0 applications, online environmental scanning provides top executives with unprecedented opportunities to tap into collective web intelligence to develop better insights about the sociocultural and political–economic factors that cross-border M&As face. Grounded in Porter’s five forces model, one major contribution of our research is the design of a novel due diligence scorecard model that leverages collective web intelligence to enhance M&A decision making. Another important contribution of our work is the design and development of an adaptive business intelligence (BI) 2.0 system underpinned by an evolutionary learning approach, domain-specific sentiment analysis, and business relation mining to operationalize the aforementioned scorecard model for adaptive M&A decision support. With Chinese companies’ cross-border M&As as the business context, our experimental results confirm that the proposed adaptive BI 2.0 system can significantly aid decision makers under different M&A scenarios. The managerial implication of our findings is that firms can apply the proposed BI 2.0 technology to enhance their strategic decision making, particularly when making cross-border investments in targeted markets for which private information may not be readily available.",[],"Chiu, Dickson K. W.",N/A,"Department of Information Systems, City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong SAR, China"
https://misq.umn.edu/misq/article/36/4/1269/1491/Network-Based-Modeling-and-Analysis-of-Systemic,MIS Quarterly,Network-Based Modeling and Analysis of Systemic Risk in Banking Systems1,"Volume 36, Issue 4",December 2012,"In the wake of the 2008 financial tsunami, existing methods and tools for managing financial risk have been criticized for weaknesses in monitoring and alleviating risks at the systemic level. A 2009 article in Nature suggested new approaches to modeling economic meltdowns are needed to prevent future financial crises. However, existing studies have not focused on analysis of systemic risk at the individual bank level in a banking network, which is essential for monitoring and mitigating contagious bank failures. To this end, we develop a network approach to risk management (NARM) for modeling and analyzing systemic risk in banking systems. NARM views banks as a network linked through financial relationships. It incorporates network and financial principles into a business intelligence (BI) algorithm to analyze systemic risk attributed to each individual bank via simulations based on real-world data from the Federal Deposit Insurance Corporation. Our research demonstrates the feasibility of modeling and analyzing systemic risk at the individual bank level in a banking network using a BI-based approach. In terms of business impact, NARM offers a new means for predicting contagious bank failures and determining capital injection priorities in the wake of financial crises. Our simulation study shows that under significant market shocks, the interbank payment relationship becomes more influential than the correlated bank portfolio relationship in determining an individual bank’s survival. These insights should help financial regulators devise more effective policies and mechanisms to prevent the collapse of a banking system. Further, NARM and the simulation procedure driven by real-world data proposed in this study have instructional value to similar research areas such as bank stress testing, where time series data and business networks may be studied.",[],"Hu, Daning",N/A,"Department of Informatics, University of Zurich, Zurich, Switzerland"
https://misq.umn.edu/misq/article/36/4/1269/1491/Network-Based-Modeling-and-Analysis-of-Systemic,MIS Quarterly,Network-Based Modeling and Analysis of Systemic Risk in Banking Systems1,"Volume 36, Issue 4",December 2012,"In the wake of the 2008 financial tsunami, existing methods and tools for managing financial risk have been criticized for weaknesses in monitoring and alleviating risks at the systemic level. A 2009 article in Nature suggested new approaches to modeling economic meltdowns are needed to prevent future financial crises. However, existing studies have not focused on analysis of systemic risk at the individual bank level in a banking network, which is essential for monitoring and mitigating contagious bank failures. To this end, we develop a network approach to risk management (NARM) for modeling and analyzing systemic risk in banking systems. NARM views banks as a network linked through financial relationships. It incorporates network and financial principles into a business intelligence (BI) algorithm to analyze systemic risk attributed to each individual bank via simulations based on real-world data from the Federal Deposit Insurance Corporation. Our research demonstrates the feasibility of modeling and analyzing systemic risk at the individual bank level in a banking network using a BI-based approach. In terms of business impact, NARM offers a new means for predicting contagious bank failures and determining capital injection priorities in the wake of financial crises. Our simulation study shows that under significant market shocks, the interbank payment relationship becomes more influential than the correlated bank portfolio relationship in determining an individual bank’s survival. These insights should help financial regulators devise more effective policies and mechanisms to prevent the collapse of a banking system. Further, NARM and the simulation procedure driven by real-world data proposed in this study have instructional value to similar research areas such as bank stress testing, where time series data and business networks may be studied.",[],"Zhao, J. Leon",N/A,"Department of Information Systems, City University of Hong Kong, Hong Kong SAR, China"
https://misq.umn.edu/misq/article/36/4/1269/1491/Network-Based-Modeling-and-Analysis-of-Systemic,MIS Quarterly,Network-Based Modeling and Analysis of Systemic Risk in Banking Systems1,"Volume 36, Issue 4",December 2012,"In the wake of the 2008 financial tsunami, existing methods and tools for managing financial risk have been criticized for weaknesses in monitoring and alleviating risks at the systemic level. A 2009 article in Nature suggested new approaches to modeling economic meltdowns are needed to prevent future financial crises. However, existing studies have not focused on analysis of systemic risk at the individual bank level in a banking network, which is essential for monitoring and mitigating contagious bank failures. To this end, we develop a network approach to risk management (NARM) for modeling and analyzing systemic risk in banking systems. NARM views banks as a network linked through financial relationships. It incorporates network and financial principles into a business intelligence (BI) algorithm to analyze systemic risk attributed to each individual bank via simulations based on real-world data from the Federal Deposit Insurance Corporation. Our research demonstrates the feasibility of modeling and analyzing systemic risk at the individual bank level in a banking network using a BI-based approach. In terms of business impact, NARM offers a new means for predicting contagious bank failures and determining capital injection priorities in the wake of financial crises. Our simulation study shows that under significant market shocks, the interbank payment relationship becomes more influential than the correlated bank portfolio relationship in determining an individual bank’s survival. These insights should help financial regulators devise more effective policies and mechanisms to prevent the collapse of a banking system. Further, NARM and the simulation procedure driven by real-world data proposed in this study have instructional value to similar research areas such as bank stress testing, where time series data and business networks may be studied.",[],"Hua, Zhimin",N/A,"Department of Information Systems, City University of Hong Kong, Hong Kong SAR, China"
https://misq.umn.edu/misq/article/36/4/1269/1491/Network-Based-Modeling-and-Analysis-of-Systemic,MIS Quarterly,Network-Based Modeling and Analysis of Systemic Risk in Banking Systems1,"Volume 36, Issue 4",December 2012,"In the wake of the 2008 financial tsunami, existing methods and tools for managing financial risk have been criticized for weaknesses in monitoring and alleviating risks at the systemic level. A 2009 article in Nature suggested new approaches to modeling economic meltdowns are needed to prevent future financial crises. However, existing studies have not focused on analysis of systemic risk at the individual bank level in a banking network, which is essential for monitoring and mitigating contagious bank failures. To this end, we develop a network approach to risk management (NARM) for modeling and analyzing systemic risk in banking systems. NARM views banks as a network linked through financial relationships. It incorporates network and financial principles into a business intelligence (BI) algorithm to analyze systemic risk attributed to each individual bank via simulations based on real-world data from the Federal Deposit Insurance Corporation. Our research demonstrates the feasibility of modeling and analyzing systemic risk at the individual bank level in a banking network using a BI-based approach. In terms of business impact, NARM offers a new means for predicting contagious bank failures and determining capital injection priorities in the wake of financial crises. Our simulation study shows that under significant market shocks, the interbank payment relationship becomes more influential than the correlated bank portfolio relationship in determining an individual bank’s survival. These insights should help financial regulators devise more effective policies and mechanisms to prevent the collapse of a banking system. Further, NARM and the simulation procedure driven by real-world data proposed in this study have instructional value to similar research areas such as bank stress testing, where time series data and business networks may be studied.",[],"Wong, Michael C. S.",N/A,"Department of Economics and Finance, City University of Hong Kong, Hong Kong SAR, China"
https://misq.umn.edu/misq/article/36/4/1293/1489/MetaFraud-A-Meta-Learning-Framework-for-Detecting,MIS Quarterly,MetaFraud: A Meta-Learning Framework for Detecting Financial Fraud1,"Volume 36, Issue 4",December 2012,"Financial fraud can have serious ramifications for the long-term sustainability of an organization, as well as adverse effects on its employees and investors, and on the economy as a whole. Several of the largest bankruptcies in U.S. history involved firms that engaged in major fraud. Accordingly, there has been considerable emphasis on the development of automated approaches for detecting financial fraud. However, most methods have yielded performance results that are less than ideal. In consequence, financial fraud detection continues as an important challenge for business intelligence technologies.In light of the need for more robust identification methods, we use a design science approach to develop MetaFraud, a novel meta-learning framework for enhanced financial fraud detection. To evaluate the proposed framework, a series of experiments are conducted on a test bed encompassing thousands of legitimate and fraudulent firms. The results reveal that each component of the framework significantly contributes to its overall effectiveness. Additional experiments demonstrate the effectiveness of the meta-learning framework over state-of-the-art financial fraud detection methods. Moreover, the MetaFraud framework generates confidence scores associated with each prediction that can facilitate unprecedented financial fraud detection performance and serve as a useful decision-making aid. The results have important implications for several stakeholder groups, including compliance officers, investors, audit firms, and regulators.",[],"Abbasi, Ahmed",N/A,"McIntire School of Commerce, University of Virginia, Charlottesville, VA 22908 U.S.A."
https://misq.umn.edu/misq/article/36/4/1293/1489/MetaFraud-A-Meta-Learning-Framework-for-Detecting,MIS Quarterly,MetaFraud: A Meta-Learning Framework for Detecting Financial Fraud1,"Volume 36, Issue 4",December 2012,"Financial fraud can have serious ramifications for the long-term sustainability of an organization, as well as adverse effects on its employees and investors, and on the economy as a whole. Several of the largest bankruptcies in U.S. history involved firms that engaged in major fraud. Accordingly, there has been considerable emphasis on the development of automated approaches for detecting financial fraud. However, most methods have yielded performance results that are less than ideal. In consequence, financial fraud detection continues as an important challenge for business intelligence technologies.In light of the need for more robust identification methods, we use a design science approach to develop MetaFraud, a novel meta-learning framework for enhanced financial fraud detection. To evaluate the proposed framework, a series of experiments are conducted on a test bed encompassing thousands of legitimate and fraudulent firms. The results reveal that each component of the framework significantly contributes to its overall effectiveness. Additional experiments demonstrate the effectiveness of the meta-learning framework over state-of-the-art financial fraud detection methods. Moreover, the MetaFraud framework generates confidence scores associated with each prediction that can facilitate unprecedented financial fraud detection performance and serve as a useful decision-making aid. The results have important implications for several stakeholder groups, including compliance officers, investors, audit firms, and regulators.",[],"Albrecht, Conan",N/A,"Information Systems Department, Marriott School of Management, Brigham Young University, Provo, UT 84606 U.S.A."
https://misq.umn.edu/misq/article/36/4/1293/1489/MetaFraud-A-Meta-Learning-Framework-for-Detecting,MIS Quarterly,MetaFraud: A Meta-Learning Framework for Detecting Financial Fraud1,"Volume 36, Issue 4",December 2012,"Financial fraud can have serious ramifications for the long-term sustainability of an organization, as well as adverse effects on its employees and investors, and on the economy as a whole. Several of the largest bankruptcies in U.S. history involved firms that engaged in major fraud. Accordingly, there has been considerable emphasis on the development of automated approaches for detecting financial fraud. However, most methods have yielded performance results that are less than ideal. In consequence, financial fraud detection continues as an important challenge for business intelligence technologies.In light of the need for more robust identification methods, we use a design science approach to develop MetaFraud, a novel meta-learning framework for enhanced financial fraud detection. To evaluate the proposed framework, a series of experiments are conducted on a test bed encompassing thousands of legitimate and fraudulent firms. The results reveal that each component of the framework significantly contributes to its overall effectiveness. Additional experiments demonstrate the effectiveness of the meta-learning framework over state-of-the-art financial fraud detection methods. Moreover, the MetaFraud framework generates confidence scores associated with each prediction that can facilitate unprecedented financial fraud detection performance and serve as a useful decision-making aid. The results have important implications for several stakeholder groups, including compliance officers, investors, audit firms, and regulators.",[],"Vance, Anthony",N/A,"Information Systems Department, Marriott School of Management, Brigham Young University, Provo, UT 84606 U.S.A."
https://misq.umn.edu/misq/article/36/4/1293/1489/MetaFraud-A-Meta-Learning-Framework-for-Detecting,MIS Quarterly,MetaFraud: A Meta-Learning Framework for Detecting Financial Fraud1,"Volume 36, Issue 4",December 2012,"Financial fraud can have serious ramifications for the long-term sustainability of an organization, as well as adverse effects on its employees and investors, and on the economy as a whole. Several of the largest bankruptcies in U.S. history involved firms that engaged in major fraud. Accordingly, there has been considerable emphasis on the development of automated approaches for detecting financial fraud. However, most methods have yielded performance results that are less than ideal. In consequence, financial fraud detection continues as an important challenge for business intelligence technologies.In light of the need for more robust identification methods, we use a design science approach to develop MetaFraud, a novel meta-learning framework for enhanced financial fraud detection. To evaluate the proposed framework, a series of experiments are conducted on a test bed encompassing thousands of legitimate and fraudulent firms. The results reveal that each component of the framework significantly contributes to its overall effectiveness. Additional experiments demonstrate the effectiveness of the meta-learning framework over state-of-the-art financial fraud detection methods. Moreover, the MetaFraud framework generates confidence scores associated with each prediction that can facilitate unprecedented financial fraud detection performance and serve as a useful decision-making aid. The results have important implications for several stakeholder groups, including compliance officers, investors, audit firms, and regulators.",[],"Hansen, James",N/A,"Information Systems Department, Marriott School of Management, Brigham Young University, Provo, UT 84606 U.S.A."
https://misq.umn.edu/misq/article/36/4/1329/1478/A-Hidden-Markov-Model-for-Collaborative-Filtering1,MIS Quarterly,A Hidden Markov Model for Collaborative Filtering1,"Volume 36, Issue 4",December 2012,"In this paper, we present a method to make personalized recommendations when user preferences change over time. Most of the works in the recommender systems literature have been developed under the assumption that user preference has a static pattern. However, this is a strong assumption especially when the user is observed over a long period of time. With the help of a data set on employees’ blog reading behavior, we show that users’ product selection behaviors change over time. We propose a hidden Markov model to correctly interpret the users’ product selection behaviors and make personalized recommendations. The user preference is modeled as a hidden Markov sequence. A variable number of product selections of different types by each user in each time period requires a novel observation model. We propose a negative binomial mixture of multi-nomial to model such observations. This allows us to identify stable global preferences of users and to track individual users through these preferences. We evaluate our model using three real-world data sets with different characteristics. They include data on employee blog reading behavior inside a firm, users’ movie rating behavior at Netflix, and users’ music listening behavior collected through last.fm. We compare the recommendation performance of the proposed model with that of a number of collaborative filtering algorithms and a recently proposed temporal link prediction algorithm. We find that the proposed HMM-based collaborative filter performs as well as the best among the alternative algorithms when the data is sparse or static. However, it outperforms the existing algorithms when the data is less sparse and the user preference is changing. We further examine the performances of the algorithms using simulated data with different characteristics and highlight the scenarios where it is beneficial to use a dynamic model to generate product recommendation.",[],"Sahoo, Nachiketa",N/A,"School of Management, Boston University, 595 Commonwealth Avenue, Boston, MA 02215 U.S.A. and iLab, Heinz College, Carnegie Mellon University, Pittsburgh, PA 15213 U.S.A."
https://misq.umn.edu/misq/article/36/4/1329/1478/A-Hidden-Markov-Model-for-Collaborative-Filtering1,MIS Quarterly,A Hidden Markov Model for Collaborative Filtering1,"Volume 36, Issue 4",December 2012,"In this paper, we present a method to make personalized recommendations when user preferences change over time. Most of the works in the recommender systems literature have been developed under the assumption that user preference has a static pattern. However, this is a strong assumption especially when the user is observed over a long period of time. With the help of a data set on employees’ blog reading behavior, we show that users’ product selection behaviors change over time. We propose a hidden Markov model to correctly interpret the users’ product selection behaviors and make personalized recommendations. The user preference is modeled as a hidden Markov sequence. A variable number of product selections of different types by each user in each time period requires a novel observation model. We propose a negative binomial mixture of multi-nomial to model such observations. This allows us to identify stable global preferences of users and to track individual users through these preferences. We evaluate our model using three real-world data sets with different characteristics. They include data on employee blog reading behavior inside a firm, users’ movie rating behavior at Netflix, and users’ music listening behavior collected through last.fm. We compare the recommendation performance of the proposed model with that of a number of collaborative filtering algorithms and a recently proposed temporal link prediction algorithm. We find that the proposed HMM-based collaborative filter performs as well as the best among the alternative algorithms when the data is sparse or static. However, it outperforms the existing algorithms when the data is less sparse and the user preference is changing. We further examine the performances of the algorithms using simulated data with different characteristics and highlight the scenarios where it is beneficial to use a dynamic model to generate product recommendation.",[],"Singh, Param Vir",N/A,"David A. Tepper School of Business and iLab, Heinz College, Carnegie Mellon University, Pittsburgh, PA 15213 U.S.A."
https://misq.umn.edu/misq/article/36/4/1329/1478/A-Hidden-Markov-Model-for-Collaborative-Filtering1,MIS Quarterly,A Hidden Markov Model for Collaborative Filtering1,"Volume 36, Issue 4",December 2012,"In this paper, we present a method to make personalized recommendations when user preferences change over time. Most of the works in the recommender systems literature have been developed under the assumption that user preference has a static pattern. However, this is a strong assumption especially when the user is observed over a long period of time. With the help of a data set on employees’ blog reading behavior, we show that users’ product selection behaviors change over time. We propose a hidden Markov model to correctly interpret the users’ product selection behaviors and make personalized recommendations. The user preference is modeled as a hidden Markov sequence. A variable number of product selections of different types by each user in each time period requires a novel observation model. We propose a negative binomial mixture of multi-nomial to model such observations. This allows us to identify stable global preferences of users and to track individual users through these preferences. We evaluate our model using three real-world data sets with different characteristics. They include data on employee blog reading behavior inside a firm, users’ movie rating behavior at Netflix, and users’ music listening behavior collected through last.fm. We compare the recommendation performance of the proposed model with that of a number of collaborative filtering algorithms and a recently proposed temporal link prediction algorithm. We find that the proposed HMM-based collaborative filter performs as well as the best among the alternative algorithms when the data is sparse or static. However, it outperforms the existing algorithms when the data is less sparse and the user preference is changing. We further examine the performances of the algorithms using simulated data with different characteristics and highlight the scenarios where it is beneficial to use a dynamic model to generate product recommendation.",[],"Mukhopadhyay, Tridas",N/A,"David A. Tepper School of Business and iLab, Heinz College, Carnegie Mellon University, Pittsburgh, PA 15213 U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/37/1/1/1523/Beyond-Deterrence-An-Expanded-View-of-Employee,MIS Quarterly,Beyond Deterrence: An Expanded View of Employee Computer Abuse1,"Volume 37, Issue 1",March 2013,"Recent academic investigations of computer security policy violations have largely focused on non-malicious noncompliance due to poor training, low employee motivation, weak affective commitment, or individual oversight. Established theoretical foundations applied to this domain have related to protection motivation, deterrence, planned behavior, self-efficacy, individual adoption factors, organizational commitment, and other individual cognitive factors. But another class of violation demands greater research emphasis: the intentional commission of computer security policy violation, or insider computer abuse. Whether motivated by greed, disgruntlement, or other psychological processes, this act has the greatest potential for loss and damage to the employer. We argue the focus must include not only the act and its immediate antecedents of intention (to commit computer abuse) and deterrence (of the crime), but also phenomena which temporally precede these areas. Specifically, we assert the need to consider the thought processes of the potential offender and how these are influenced by the organizational context, prior to deterrence. We believe the interplay between thought processes and this context may significantly impact the efficacy of IS security controls, specifically deterrence safeguards. Through this focus, we extend the Straub and Welke (1998) security action cycle framework and propose three areas worthy of empirical investigation—techniques of neutralization (rationalization), expressive/instrumental criminal motivations, and disgruntlement as a result of perceptions of organizational injustice—and propose questions for future research in these areas.",[],"Willison, Robert",N/A,"Newcastle University Business School, Newcastle University, 5 Barrack Road, Newcastle Upon Tyne NE1 4SE United Kingdom"
https://misq.umn.edu/misq/article/37/1/1/1523/Beyond-Deterrence-An-Expanded-View-of-Employee,MIS Quarterly,Beyond Deterrence: An Expanded View of Employee Computer Abuse1,"Volume 37, Issue 1",March 2013,"Recent academic investigations of computer security policy violations have largely focused on non-malicious noncompliance due to poor training, low employee motivation, weak affective commitment, or individual oversight. Established theoretical foundations applied to this domain have related to protection motivation, deterrence, planned behavior, self-efficacy, individual adoption factors, organizational commitment, and other individual cognitive factors. But another class of violation demands greater research emphasis: the intentional commission of computer security policy violation, or insider computer abuse. Whether motivated by greed, disgruntlement, or other psychological processes, this act has the greatest potential for loss and damage to the employer. We argue the focus must include not only the act and its immediate antecedents of intention (to commit computer abuse) and deterrence (of the crime), but also phenomena which temporally precede these areas. Specifically, we assert the need to consider the thought processes of the potential offender and how these are influenced by the organizational context, prior to deterrence. We believe the interplay between thought processes and this context may significantly impact the efficacy of IS security controls, specifically deterrence safeguards. Through this focus, we extend the Straub and Welke (1998) security action cycle framework and propose three areas worthy of empirical investigation—techniques of neutralization (rationalization), expressive/instrumental criminal motivations, and disgruntlement as a result of perceptions of organizational injustice—and propose questions for future research in these areas.",[],"Warkentin, Merrill",N/A,"Department of Management and Information Systems, College of Business, Mississippi State University, P.O. Box 9581, Mississippi State, MS 39762-9581 U.S.A."
https://misq.umn.edu/misq/article/37/1/21/1516/Bridging-the-Qualitative-Quantitative-Divide,MIS Quarterly,Bridging the Qualitative–Quantitative Divide: Guidelines for Conducting Mixed Methods Research in Information Systems1,"Volume 37, Issue 1",March 2013,"Mixed methods research is an approach that combines quantitative and qualitative research methods in the same research inquiry. Such work can help develop rich insights into various phenomena of interest that cannot be fully understood using only a quantitative or a qualitative method. Notwithstanding the benefits and repeated calls for such work, there is a dearth of mixed methods research in information systems. Building on the literature on recent methodological advances in mixed methods research, we develop a set of guidelines for conducting mixed methods research in IS. We particularly elaborate on three important aspects of conducting mixed methods research: (1) appropriateness of a mixed methods approach; (2) development of meta-inferences (i.e., substantive theory) from mixed methods research; and (3) assessment of the quality of meta-inferences (i.e., validation of mixed methods research). The applicability of these guidelines is illustrated using two published IS papers that used mixed methods.",[],"Venkatesh, Viswanath",N/A,"Department of Information Systems, Walton College of Business, University of Arkansas, Fayetteville, AR 72701 U.S.A."
https://misq.umn.edu/misq/article/37/1/21/1516/Bridging-the-Qualitative-Quantitative-Divide,MIS Quarterly,Bridging the Qualitative–Quantitative Divide: Guidelines for Conducting Mixed Methods Research in Information Systems1,"Volume 37, Issue 1",March 2013,"Mixed methods research is an approach that combines quantitative and qualitative research methods in the same research inquiry. Such work can help develop rich insights into various phenomena of interest that cannot be fully understood using only a quantitative or a qualitative method. Notwithstanding the benefits and repeated calls for such work, there is a dearth of mixed methods research in information systems. Building on the literature on recent methodological advances in mixed methods research, we develop a set of guidelines for conducting mixed methods research in IS. We particularly elaborate on three important aspects of conducting mixed methods research: (1) appropriateness of a mixed methods approach; (2) development of meta-inferences (i.e., substantive theory) from mixed methods research; and (3) assessment of the quality of meta-inferences (i.e., validation of mixed methods research). The applicability of these guidelines is illustrated using two published IS papers that used mixed methods.",[],"Brown, Susan A.",N/A,"Management Information Systems Department, Eller College of Management, University of Arizona, Tucson, AZ 85721 U.S.A."
https://misq.umn.edu/misq/article/37/1/21/1516/Bridging-the-Qualitative-Quantitative-Divide,MIS Quarterly,Bridging the Qualitative–Quantitative Divide: Guidelines for Conducting Mixed Methods Research in Information Systems1,"Volume 37, Issue 1",March 2013,"Mixed methods research is an approach that combines quantitative and qualitative research methods in the same research inquiry. Such work can help develop rich insights into various phenomena of interest that cannot be fully understood using only a quantitative or a qualitative method. Notwithstanding the benefits and repeated calls for such work, there is a dearth of mixed methods research in information systems. Building on the literature on recent methodological advances in mixed methods research, we develop a set of guidelines for conducting mixed methods research in IS. We particularly elaborate on three important aspects of conducting mixed methods research: (1) appropriateness of a mixed methods approach; (2) development of meta-inferences (i.e., substantive theory) from mixed methods research; and (3) assessment of the quality of meta-inferences (i.e., validation of mixed methods research). The applicability of these guidelines is illustrated using two published IS papers that used mixed methods.",[],"Bala, Hillol",N/A,"Operations and Decision Technologies, Kelley School of Business, Indiana University, Bloomington, IN 47405 U.S.A."
https://misq.umn.edu/misq/article/37/1/55/1495/Impact-of-Information-Feedback-in-Continuous,MIS Quarterly,Impact of Information Feedback in Continuous Combinatorial Auctions: An Experimental Study of Economic Performance1,"Volume 37, Issue 1",March 2013,"Advancements in information technology offer opportunities for designing and deploying innovative market mechanisms that can improve the allocation and procurement processes of businesses. For example, combinatorial auctions—in which bidders can bid on combinations of goods—have been shown to increase the economic efficiency of a trade when goods have complementarities. However, the lack of real-time decision support tools for bidders has prevented this mechanism from reaching its full potential. With the objective of facilitating bidder participation in combinatorial auctions, this study, using recent research in real-time bidder support metrics, discusses several novel feedback schemes that can aid bidders in formulating combinatorial bids in real-time. The feedback schemes allow us to conduct continuous combinatorial auctions, where bidders can submit bids at any time. Using laboratory experiments with two different setups, we compare the economic performance of the continuous mechanism under three progressively advanced levels of feedback. Our findings indicate that information feedback plays a major role in influencing the economic outcomes of combinatorial auctions. We compare several important bid characteristics to explain the observed differences in aggregate measures. This study advances the ongoing research on combinatorial auctions by developing continuous auctions that differentiate themselves from earlier combinatorial auction mechanisms by facilitating free-flowing participation of bidders and providing exact prices of bundles on demand in real time. For practitioners, the study provides insights on how the nature of feedback can influence the economic outcomes of a complex trading mechanism.",[],"Adomavicius, Gediminas",N/A,"Carlson School of Management, University of Minnesota, Minneapolis, MN 55455 U.S.A."
https://misq.umn.edu/misq/article/37/1/55/1495/Impact-of-Information-Feedback-in-Continuous,MIS Quarterly,Impact of Information Feedback in Continuous Combinatorial Auctions: An Experimental Study of Economic Performance1,"Volume 37, Issue 1",March 2013,"Advancements in information technology offer opportunities for designing and deploying innovative market mechanisms that can improve the allocation and procurement processes of businesses. For example, combinatorial auctions—in which bidders can bid on combinations of goods—have been shown to increase the economic efficiency of a trade when goods have complementarities. However, the lack of real-time decision support tools for bidders has prevented this mechanism from reaching its full potential. With the objective of facilitating bidder participation in combinatorial auctions, this study, using recent research in real-time bidder support metrics, discusses several novel feedback schemes that can aid bidders in formulating combinatorial bids in real-time. The feedback schemes allow us to conduct continuous combinatorial auctions, where bidders can submit bids at any time. Using laboratory experiments with two different setups, we compare the economic performance of the continuous mechanism under three progressively advanced levels of feedback. Our findings indicate that information feedback plays a major role in influencing the economic outcomes of combinatorial auctions. We compare several important bid characteristics to explain the observed differences in aggregate measures. This study advances the ongoing research on combinatorial auctions by developing continuous auctions that differentiate themselves from earlier combinatorial auction mechanisms by facilitating free-flowing participation of bidders and providing exact prices of bundles on demand in real time. For practitioners, the study provides insights on how the nature of feedback can influence the economic outcomes of a complex trading mechanism.",[],"Curley, Shawn P.",N/A,"Carlson School of Management, University of Minnesota, Minneapolis, MN 55455 U.S.A."
https://misq.umn.edu/misq/article/37/1/55/1495/Impact-of-Information-Feedback-in-Continuous,MIS Quarterly,Impact of Information Feedback in Continuous Combinatorial Auctions: An Experimental Study of Economic Performance1,"Volume 37, Issue 1",March 2013,"Advancements in information technology offer opportunities for designing and deploying innovative market mechanisms that can improve the allocation and procurement processes of businesses. For example, combinatorial auctions—in which bidders can bid on combinations of goods—have been shown to increase the economic efficiency of a trade when goods have complementarities. However, the lack of real-time decision support tools for bidders has prevented this mechanism from reaching its full potential. With the objective of facilitating bidder participation in combinatorial auctions, this study, using recent research in real-time bidder support metrics, discusses several novel feedback schemes that can aid bidders in formulating combinatorial bids in real-time. The feedback schemes allow us to conduct continuous combinatorial auctions, where bidders can submit bids at any time. Using laboratory experiments with two different setups, we compare the economic performance of the continuous mechanism under three progressively advanced levels of feedback. Our findings indicate that information feedback plays a major role in influencing the economic outcomes of combinatorial auctions. We compare several important bid characteristics to explain the observed differences in aggregate measures. This study advances the ongoing research on combinatorial auctions by developing continuous auctions that differentiate themselves from earlier combinatorial auction mechanisms by facilitating free-flowing participation of bidders and providing exact prices of bundles on demand in real time. For practitioners, the study provides insights on how the nature of feedback can influence the economic outcomes of a complex trading mechanism.",[],"Gupta, Alok",N/A,"Carlson School of Management, University of Minnesota, Minneapolis, MN 55455 U.S.A."
https://misq.umn.edu/misq/article/37/1/55/1495/Impact-of-Information-Feedback-in-Continuous,MIS Quarterly,Impact of Information Feedback in Continuous Combinatorial Auctions: An Experimental Study of Economic Performance1,"Volume 37, Issue 1",March 2013,"Advancements in information technology offer opportunities for designing and deploying innovative market mechanisms that can improve the allocation and procurement processes of businesses. For example, combinatorial auctions—in which bidders can bid on combinations of goods—have been shown to increase the economic efficiency of a trade when goods have complementarities. However, the lack of real-time decision support tools for bidders has prevented this mechanism from reaching its full potential. With the objective of facilitating bidder participation in combinatorial auctions, this study, using recent research in real-time bidder support metrics, discusses several novel feedback schemes that can aid bidders in formulating combinatorial bids in real-time. The feedback schemes allow us to conduct continuous combinatorial auctions, where bidders can submit bids at any time. Using laboratory experiments with two different setups, we compare the economic performance of the continuous mechanism under three progressively advanced levels of feedback. Our findings indicate that information feedback plays a major role in influencing the economic outcomes of combinatorial auctions. We compare several important bid characteristics to explain the observed differences in aggregate measures. This study advances the ongoing research on combinatorial auctions by developing continuous auctions that differentiate themselves from earlier combinatorial auction mechanisms by facilitating free-flowing participation of bidders and providing exact prices of bundles on demand in real time. For practitioners, the study provides insights on how the nature of feedback can influence the economic outcomes of a complex trading mechanism.",[],"Sanyal, Pallab",N/A,"School of Management, George Mason University, Fairfax, VA 22030 U.S.A."
https://misq.umn.edu/misq/article/37/1/77/1497/It-Mediated-Customer-Service-Content-and-Delivery,MIS Quarterly,It-Mediated Customer Service Content and Delivery in Electronic Governments: An Empirical Investigation of the Antecedents of Service Quality1,"Volume 37, Issue 1",March 2013,"Despite extensive deliberations in contemporary literature, the design of citizen-centric e-government websites remains an unresolved theoretical and pragmatic conundrum. Operationalizing e-government service quality to investigate and improve the design of e-government websites has been a much sought-after objective. Yet, there is a lack of actionable guidance on how to develop e-government websites that exhibit high levels of service quality. Drawing from marketing literature, we undertake a goal approach to this problem by delineating e-government service quality into aspects of IT-mediated service content and service delivery. Whereas service content describes the functions available on an e-government website that assist citizens in completing their transactional goals, service delivery defines the manner by which these functions are made accessible via the web interface as a delivery channel. We construct and empirically test a research model that depicts a comprehensive collection of web-enabled service content functions and delivery dimensions desirable by citizens. Empirical findings from an online survey of 647 respondents attest to the value of distinguishing between service content functions and delivery dimensions in designing e-government websites. Both service content and delivery are found to be significant contributors to achieving e-government service quality. These IT-mediated service content functions and delivery dimensions represent core areas of e-government website design where the application of technology makes a difference, especially when considered in tandem with the type of transactional activity. A split sample analysis of the data further demonstrates our model’s robustness when applied to e-government transactions of varying frequency.",[],"Tan, Chee-Wee",N/A,"Faculty of Economics and Business, University of Groningen, Nettelbosje 2, 9747 AE Groningen, The Netherlands"
https://misq.umn.edu/misq/article/37/1/77/1497/It-Mediated-Customer-Service-Content-and-Delivery,MIS Quarterly,It-Mediated Customer Service Content and Delivery in Electronic Governments: An Empirical Investigation of the Antecedents of Service Quality1,"Volume 37, Issue 1",March 2013,"Despite extensive deliberations in contemporary literature, the design of citizen-centric e-government websites remains an unresolved theoretical and pragmatic conundrum. Operationalizing e-government service quality to investigate and improve the design of e-government websites has been a much sought-after objective. Yet, there is a lack of actionable guidance on how to develop e-government websites that exhibit high levels of service quality. Drawing from marketing literature, we undertake a goal approach to this problem by delineating e-government service quality into aspects of IT-mediated service content and service delivery. Whereas service content describes the functions available on an e-government website that assist citizens in completing their transactional goals, service delivery defines the manner by which these functions are made accessible via the web interface as a delivery channel. We construct and empirically test a research model that depicts a comprehensive collection of web-enabled service content functions and delivery dimensions desirable by citizens. Empirical findings from an online survey of 647 respondents attest to the value of distinguishing between service content functions and delivery dimensions in designing e-government websites. Both service content and delivery are found to be significant contributors to achieving e-government service quality. These IT-mediated service content functions and delivery dimensions represent core areas of e-government website design where the application of technology makes a difference, especially when considered in tandem with the type of transactional activity. A split sample analysis of the data further demonstrates our model’s robustness when applied to e-government transactions of varying frequency.",[],"Benbasat, Izak",N/A,"Sauder School of Business, University of British Columbia, 2053 Main Mall, Vancouver, BC, Canada"
https://misq.umn.edu/misq/article/37/1/77/1497/It-Mediated-Customer-Service-Content-and-Delivery,MIS Quarterly,It-Mediated Customer Service Content and Delivery in Electronic Governments: An Empirical Investigation of the Antecedents of Service Quality1,"Volume 37, Issue 1",March 2013,"Despite extensive deliberations in contemporary literature, the design of citizen-centric e-government websites remains an unresolved theoretical and pragmatic conundrum. Operationalizing e-government service quality to investigate and improve the design of e-government websites has been a much sought-after objective. Yet, there is a lack of actionable guidance on how to develop e-government websites that exhibit high levels of service quality. Drawing from marketing literature, we undertake a goal approach to this problem by delineating e-government service quality into aspects of IT-mediated service content and service delivery. Whereas service content describes the functions available on an e-government website that assist citizens in completing their transactional goals, service delivery defines the manner by which these functions are made accessible via the web interface as a delivery channel. We construct and empirically test a research model that depicts a comprehensive collection of web-enabled service content functions and delivery dimensions desirable by citizens. Empirical findings from an online survey of 647 respondents attest to the value of distinguishing between service content functions and delivery dimensions in designing e-government websites. Both service content and delivery are found to be significant contributors to achieving e-government service quality. These IT-mediated service content functions and delivery dimensions represent core areas of e-government website design where the application of technology makes a difference, especially when considered in tandem with the type of transactional activity. A split sample analysis of the data further demonstrates our model’s robustness when applied to e-government transactions of varying frequency.",[],"Cenfetelli, Ronald T.",N/A,"Sauder School of Business, University of British Columbia, 2053 Main Mall, Vancouver, BC, Canada"
https://misq.umn.edu/misq/article/37/1/111/1492/Digital-Games-and-Beyond-What-Happens-When-Players,MIS Quarterly,Digital Games and Beyond: What Happens When Players Compete?1,"Volume 37, Issue 1",March 2013,"Because digital games are fun, engaging, and popular, organizations are attempting to integrate them within organizational activities as serious components, with the anticipation that they can improve employees’ motivation and performance. But in order to do so and to obtain the intended outcomes, it is necessary to first obtain an understanding of how different digital game designs impact players’ behaviors and emotional responses. Hence, in this study, we address one key element of popular game designs: competition. Using extant research on tournaments and intrinsic motivation, we model competitive games as a skill-based tournament and conduct an experimental study to understand player behaviors and emotional responses under different competition conditions. When players compete with players of similar skill levels, they apply more effort as indicated by more games played and longer duration of play. But when players compete with players of lower skill levels, they report higher levels of enjoyment and lower levels of arousal after game-playing. We discuss the implications for organizations seeking to introduce games premised on competition and provide a framework to guide information system researchers to embark on a study of games.",[],"Liu, De",N/A,"University of Kentucky, 425D Gatton College of Business and Economics, Lexington, KY 40506 U.S.A."
https://misq.umn.edu/misq/article/37/1/111/1492/Digital-Games-and-Beyond-What-Happens-When-Players,MIS Quarterly,Digital Games and Beyond: What Happens When Players Compete?1,"Volume 37, Issue 1",March 2013,"Because digital games are fun, engaging, and popular, organizations are attempting to integrate them within organizational activities as serious components, with the anticipation that they can improve employees’ motivation and performance. But in order to do so and to obtain the intended outcomes, it is necessary to first obtain an understanding of how different digital game designs impact players’ behaviors and emotional responses. Hence, in this study, we address one key element of popular game designs: competition. Using extant research on tournaments and intrinsic motivation, we model competitive games as a skill-based tournament and conduct an experimental study to understand player behaviors and emotional responses under different competition conditions. When players compete with players of similar skill levels, they apply more effort as indicated by more games played and longer duration of play. But when players compete with players of lower skill levels, they report higher levels of enjoyment and lower levels of arousal after game-playing. We discuss the implications for organizations seeking to introduce games premised on competition and provide a framework to guide information system researchers to embark on a study of games.",[],"Li, Xun",N/A,"Nicholls State University, College of Business Administration Thibodaux, LA 70310 U.S.A."
https://misq.umn.edu/misq/article/37/1/111/1492/Digital-Games-and-Beyond-What-Happens-When-Players,MIS Quarterly,Digital Games and Beyond: What Happens When Players Compete?1,"Volume 37, Issue 1",March 2013,"Because digital games are fun, engaging, and popular, organizations are attempting to integrate them within organizational activities as serious components, with the anticipation that they can improve employees’ motivation and performance. But in order to do so and to obtain the intended outcomes, it is necessary to first obtain an understanding of how different digital game designs impact players’ behaviors and emotional responses. Hence, in this study, we address one key element of popular game designs: competition. Using extant research on tournaments and intrinsic motivation, we model competitive games as a skill-based tournament and conduct an experimental study to understand player behaviors and emotional responses under different competition conditions. When players compete with players of similar skill levels, they apply more effort as indicated by more games played and longer duration of play. But when players compete with players of lower skill levels, they report higher levels of enjoyment and lower levels of arousal after game-playing. We discuss the implications for organizations seeking to introduce games premised on competition and provide a framework to guide information system researchers to embark on a study of games.",[],"Santhanam, Radhika",N/A,"University of Kentucky, 425G Gatton College of Business and Economics, Lexington, KY 40506 U.S.A."
https://misq.umn.edu/misq/article/37/1/125/1526/Data-Model-Development-for-Fire-Related-Extreme,MIS Quarterly,Data Model Development for Fire Related Extreme Events: An Activity Theory Approach1,"Volume 37, Issue 1",March 2013,"Post-analyses of major extreme events reveal that information sharing is critical for effective emergency response. The lack of consistent data standards for current emergency management practice, however, hinders efficient critical information flow among incident responders. In this paper, we adopt a third-generation activity theory guided approach to develop a data model that can be used in the response to fire-related extreme events. This data model prescribes the core data standards to reduce information interoperability barriers. The model is validated through a three-step approach including a request for comment (RFC) process, case application, and prototype system test. This study contributes to the literature in the area of interoperability and data modeling; it also informs practice in emergency response system design.",[],"Chen, Rui",N/A,"Information Systems and Operations Management, Miller College of Business, Ball State University, Muncie, IN 47306 U.S.A."
https://misq.umn.edu/misq/article/37/1/125/1526/Data-Model-Development-for-Fire-Related-Extreme,MIS Quarterly,Data Model Development for Fire Related Extreme Events: An Activity Theory Approach1,"Volume 37, Issue 1",March 2013,"Post-analyses of major extreme events reveal that information sharing is critical for effective emergency response. The lack of consistent data standards for current emergency management practice, however, hinders efficient critical information flow among incident responders. In this paper, we adopt a third-generation activity theory guided approach to develop a data model that can be used in the response to fire-related extreme events. This data model prescribes the core data standards to reduce information interoperability barriers. The model is validated through a three-step approach including a request for comment (RFC) process, case application, and prototype system test. This study contributes to the literature in the area of interoperability and data modeling; it also informs practice in emergency response system design.",[],"Sharman, Raj",N/A,"Management Science and Systems, School of Management, State University of New York at Buffalo, Jacobs Management Center, Buffalo, NY 14260 U.S.A."
https://misq.umn.edu/misq/article/37/1/125/1526/Data-Model-Development-for-Fire-Related-Extreme,MIS Quarterly,Data Model Development for Fire Related Extreme Events: An Activity Theory Approach1,"Volume 37, Issue 1",March 2013,"Post-analyses of major extreme events reveal that information sharing is critical for effective emergency response. The lack of consistent data standards for current emergency management practice, however, hinders efficient critical information flow among incident responders. In this paper, we adopt a third-generation activity theory guided approach to develop a data model that can be used in the response to fire-related extreme events. This data model prescribes the core data standards to reduce information interoperability barriers. The model is validated through a three-step approach including a request for comment (RFC) process, case application, and prototype system test. This study contributes to the literature in the area of interoperability and data modeling; it also informs practice in emergency response system design.",[],"Rao, H. Raghav",N/A,"Management Science and Systems, School of Management, State University of New York at Buffalo, Jacobs Management Center, Buffalo, NY 14260 U.S.A."
https://misq.umn.edu/misq/article/37/1/125/1526/Data-Model-Development-for-Fire-Related-Extreme,MIS Quarterly,Data Model Development for Fire Related Extreme Events: An Activity Theory Approach1,"Volume 37, Issue 1",March 2013,"Post-analyses of major extreme events reveal that information sharing is critical for effective emergency response. The lack of consistent data standards for current emergency management practice, however, hinders efficient critical information flow among incident responders. In this paper, we adopt a third-generation activity theory guided approach to develop a data model that can be used in the response to fire-related extreme events. This data model prescribes the core data standards to reduce information interoperability barriers. The model is validated through a three-step approach including a request for comment (RFC) process, case application, and prototype system test. This study contributes to the literature in the area of interoperability and data modeling; it also informs practice in emergency response system design.",[],"Upadhyaya, Shambhu J.",N/A,"Department of GSM, Sogang University, Seoul, South Korea"
https://misq.umn.edu/misq/article/37/1/149/1500/Examining-the-Relational-Benefits-of-Improved,MIS Quarterly,Examining the Relational Benefits of Improved Interfirm Information Processing Capability in Buyer–Supplier Dyads1,"Volume 37, Issue 1",March 2013,"Information Systems research has studied how buyers and suppliers can benefit from improved information visibility in supply chains characterized by uncertainty. However, the relation-specific information processing solutions that provide visibility can only be exploited if the two firms engage in sufficient coordination efforts. This work takes a nuanced look at how dyadic benefits are derived in the supply chain. Drawing on the information processing view, resource-based view, and transaction cost theory, this study explicates how buyer performance can result from buyer’s use of relation-specific information processing solutions and supplier’s relational responses. Two interfirm information processing solutions are proposed and examined: the use of IT-based systems for planning and control, and the use of relational (normative) contracts. Based on a sample of 144 manufacturing firms, eight of the nine proposed research hypotheses receive empirical support using PLS analysis. The findings suggest that as buyers and suppliers utilize the IT and relational solutions, they induce relation-specific responses represented as supplier’s business process investments and modification flexibility, which in turn lead to positive buyer outcomes. The results help us gain a more granular understanding on how relation-specific interfirm information processing solutions can lead to performance through enhanced interfirm governance capabilities.",[],"Wang, Eric T. G.",N/A,"Department of Information Management, School of Management, National Central University, No. 300 Jhongda Road, Jhongli City, Taiwan 32001 R.O.C."
https://misq.umn.edu/misq/article/37/1/149/1500/Examining-the-Relational-Benefits-of-Improved,MIS Quarterly,Examining the Relational Benefits of Improved Interfirm Information Processing Capability in Buyer–Supplier Dyads1,"Volume 37, Issue 1",March 2013,"Information Systems research has studied how buyers and suppliers can benefit from improved information visibility in supply chains characterized by uncertainty. However, the relation-specific information processing solutions that provide visibility can only be exploited if the two firms engage in sufficient coordination efforts. This work takes a nuanced look at how dyadic benefits are derived in the supply chain. Drawing on the information processing view, resource-based view, and transaction cost theory, this study explicates how buyer performance can result from buyer’s use of relation-specific information processing solutions and supplier’s relational responses. Two interfirm information processing solutions are proposed and examined: the use of IT-based systems for planning and control, and the use of relational (normative) contracts. Based on a sample of 144 manufacturing firms, eight of the nine proposed research hypotheses receive empirical support using PLS analysis. The findings suggest that as buyers and suppliers utilize the IT and relational solutions, they induce relation-specific responses represented as supplier’s business process investments and modification flexibility, which in turn lead to positive buyer outcomes. The results help us gain a more granular understanding on how relation-specific interfirm information processing solutions can lead to performance through enhanced interfirm governance capabilities.",[],"Tai, Jeffrey C. F.",N/A,"Department of Management Information Systems, College of Management, National Chiayi University, No. 580 Sinmin Road, Chiayi City, Taiwan 60054 R.O.C."
https://misq.umn.edu/misq/article/37/1/149/1500/Examining-the-Relational-Benefits-of-Improved,MIS Quarterly,Examining the Relational Benefits of Improved Interfirm Information Processing Capability in Buyer–Supplier Dyads1,"Volume 37, Issue 1",March 2013,"Information Systems research has studied how buyers and suppliers can benefit from improved information visibility in supply chains characterized by uncertainty. However, the relation-specific information processing solutions that provide visibility can only be exploited if the two firms engage in sufficient coordination efforts. This work takes a nuanced look at how dyadic benefits are derived in the supply chain. Drawing on the information processing view, resource-based view, and transaction cost theory, this study explicates how buyer performance can result from buyer’s use of relation-specific information processing solutions and supplier’s relational responses. Two interfirm information processing solutions are proposed and examined: the use of IT-based systems for planning and control, and the use of relational (normative) contracts. Based on a sample of 144 manufacturing firms, eight of the nine proposed research hypotheses receive empirical support using PLS analysis. The findings suggest that as buyers and suppliers utilize the IT and relational solutions, they induce relation-specific responses represented as supplier’s business process investments and modification flexibility, which in turn lead to positive buyer outcomes. The results help us gain a more granular understanding on how relation-specific interfirm information processing solutions can lead to performance through enhanced interfirm governance capabilities.",[],"Grover, Varun",N/A,"Department of Management, Clemson University, 132F Sirrine Hall, Clemson, SC 29634 U.S.A."
https://misq.umn.edu/misq/article/37/1/175/1505/Internationalization-Strategies-of-Chinese-It,MIS Quarterly,Internationalization Strategies of Chinese It Service Suppliers1,"Volume 37, Issue 1",March 2013,"With China emerging as a new frontier of global IT outsourcing, many Chinese IT service suppliers are actively expanding in three major markets: Asia, especially Japan, the West, especially the United States, and the Chinese domestic market. Compared to multinational suppliers and established Indian suppliers, Chinese IT service firms are at a relatively early, but rapidly growing stage, which offers a unique opportunity to explore an understudied topic in the information systems literature: internationalization strategies of IT service suppliers from emerging economies. Through a three-part qualitative case study of 13 China-based IT service firms, including almost all of the Chinese suppliers recognized globally, this study elaborates the inter-nationalization behavior and decision rationale of these suppliers. The findings show that these major Chinese suppliers include both firms that incrementally internationalize and firms that are “born global.” For both types of firms, the entry and growth in different markets is a highly dynamic activity combining a strategically planned, resource-seeking process and a flexible, opportunistic bricolage process based on existing operation capabilities and client relationships. The suppliers dynamically oscillate between these processes to exploit and create opportunities while expanding in multiple markets.",[],"Su, Ning",N/A,"Richard Ivey School of Business, University of Western Ontario, London, Ontario N6A 3K7 Canada"
https://misq.umn.edu/misq/article/37/1/201/1510/A-Rhetorical-Approach-to-It-Diffusion,MIS Quarterly,A Rhetorical Approach to It Diffusion: Reconceptualizing the Ideology-Framing Relationship in Computerization Movements1,"Volume 37, Issue 1",March 2013,"In this paper we propose rhetoric as a valuable yet underdeveloped alternative paradigm for examining IT diffusion. Building on recent developments of computerization movements theory, our rhetorical approach proposes that two central elements of the theory, framing and ideology, rather than being treated as separate can be usefully integrated. We suggest that IT diffusion can be usefully explored through examining the interrelationship of the deep structures underlying ideology and the type and sequence of rhetorical claims underpinning actors’ framing strategies. Our theoretical developments also allow us to better understand competing discourses influencing the diffusion process. These discourses reflect the ideologies and shape the framing strategies of actors in the broader field context. We illuminate our theoretical approach by drawing on the history of the diffusion of free and open source software.",[],"Barrett, Michael",N/A,"Judge Business School, University of Cambridge, Cambridge CB2 1AG United Kingdom"
https://misq.umn.edu/misq/article/37/1/201/1510/A-Rhetorical-Approach-to-It-Diffusion,MIS Quarterly,A Rhetorical Approach to It Diffusion: Reconceptualizing the Ideology-Framing Relationship in Computerization Movements1,"Volume 37, Issue 1",March 2013,"In this paper we propose rhetoric as a valuable yet underdeveloped alternative paradigm for examining IT diffusion. Building on recent developments of computerization movements theory, our rhetorical approach proposes that two central elements of the theory, framing and ideology, rather than being treated as separate can be usefully integrated. We suggest that IT diffusion can be usefully explored through examining the interrelationship of the deep structures underlying ideology and the type and sequence of rhetorical claims underpinning actors’ framing strategies. Our theoretical developments also allow us to better understand competing discourses influencing the diffusion process. These discourses reflect the ideologies and shape the framing strategies of actors in the broader field context. We illuminate our theoretical approach by drawing on the history of the diffusion of free and open source software.",[],"Heracleous, Loizos",N/A,"Warwick Business School, University of Warwick, Coventry CV4 7AL United Kingdom"
https://misq.umn.edu/misq/article/37/1/201/1510/A-Rhetorical-Approach-to-It-Diffusion,MIS Quarterly,A Rhetorical Approach to It Diffusion: Reconceptualizing the Ideology-Framing Relationship in Computerization Movements1,"Volume 37, Issue 1",March 2013,"In this paper we propose rhetoric as a valuable yet underdeveloped alternative paradigm for examining IT diffusion. Building on recent developments of computerization movements theory, our rhetorical approach proposes that two central elements of the theory, framing and ideology, rather than being treated as separate can be usefully integrated. We suggest that IT diffusion can be usefully explored through examining the interrelationship of the deep structures underlying ideology and the type and sequence of rhetorical claims underpinning actors’ framing strategies. Our theoretical developments also allow us to better understand competing discourses influencing the diffusion process. These discourses reflect the ideologies and shape the framing strategies of actors in the broader field context. We illuminate our theoretical approach by drawing on the history of the diffusion of free and open source software.",[],"Walsham, Geoff",N/A,"Judge Business School, University of Cambridge, Cambridge CB2 1AG United Kingdom"
https://misq.umn.edu/misq/article/37/1/221/1521/The-Embeddedness-of-Information-Systems-Habits-in,MIS Quarterly,The Embeddedness of Information Systems Habits in Organizational and Individual Level Routines: Development and Disruption1,"Volume 37, Issue 1",March 2013,"Despite recent interest in studying information system habits, our understanding of how these habits develop and operate in an organizational context is still limited. Within organizations, IS habits may develop over long periods of time and are typically embedded within larger, frequently practiced, higher-level work routines or task sequences. When new systems are introduced for the purpose of at least partially replacing incumbent systems, existing IS habits embedded in these routines may inhibit adoption and use of the new systems. Therefore, understanding how work-related IS habits form, how they enable and inhibit behavior, and how they can be disrupted or encouraged requires that we examine them within the context of organizational and individual level work routines. The current study integrates psychology and organizational behavior literature on cognitive scripts and work routines to examine IS habits as they occur embedded within larger, more complex task sequences. The objective of the paper is to contribute to the IS habit literature by (1) situating IS habits within the context of their associated work routines and task sequences, and (2) providing a theoretical understanding of how incumbent system habits can be disrupted, and how development of new system habits can be encouraged, within this context. We draw from extant research in psychology, organizational behavior, and consumer behavior to offer propositions about context-focused organizational interventions to break, or otherwise discourage, the continued performance of incumbent system habits and to encourage the development of new system habits. Specifically, our theoretical development includes script disruption techniques, training-in-context, and performance goal suspension as organizational interventions that disrupt incumbent system habits. We further theorize how stabilizing contextual variables associated with modified work routines can facilitate the development of new system habits. The paper concludes by discussing the importance of combining intervention strategies to successfully disrupt incumbent system habits and encourage development of new system habits and thus facilitate adoption of new systems.",[],"Polites, Greta L.",N/A,"Department of Management and Information Systems, College of Business Administration, Kent State University, Kent, OH 44242 U.S.A."
https://misq.umn.edu/misq/article/37/1/221/1521/The-Embeddedness-of-Information-Systems-Habits-in,MIS Quarterly,The Embeddedness of Information Systems Habits in Organizational and Individual Level Routines: Development and Disruption1,"Volume 37, Issue 1",March 2013,"Despite recent interest in studying information system habits, our understanding of how these habits develop and operate in an organizational context is still limited. Within organizations, IS habits may develop over long periods of time and are typically embedded within larger, frequently practiced, higher-level work routines or task sequences. When new systems are introduced for the purpose of at least partially replacing incumbent systems, existing IS habits embedded in these routines may inhibit adoption and use of the new systems. Therefore, understanding how work-related IS habits form, how they enable and inhibit behavior, and how they can be disrupted or encouraged requires that we examine them within the context of organizational and individual level work routines. The current study integrates psychology and organizational behavior literature on cognitive scripts and work routines to examine IS habits as they occur embedded within larger, more complex task sequences. The objective of the paper is to contribute to the IS habit literature by (1) situating IS habits within the context of their associated work routines and task sequences, and (2) providing a theoretical understanding of how incumbent system habits can be disrupted, and how development of new system habits can be encouraged, within this context. We draw from extant research in psychology, organizational behavior, and consumer behavior to offer propositions about context-focused organizational interventions to break, or otherwise discourage, the continued performance of incumbent system habits and to encourage the development of new system habits. Specifically, our theoretical development includes script disruption techniques, training-in-context, and performance goal suspension as organizational interventions that disrupt incumbent system habits. We further theorize how stabilizing contextual variables associated with modified work routines can facilitate the development of new system habits. The paper concludes by discussing the importance of combining intervention strategies to successfully disrupt incumbent system habits and encourage development of new system habits and thus facilitate adoption of new systems.",[],"Karahanna, Elena",N/A,"Management Information Systems Department, Terry College of Business, University of Georgia, Athens, GA 30602 U.S.A."
https://misq.umn.edu/misq/article/37/1/247/1519/The-Affective-Response-Model-A-Theoretical,MIS Quarterly,The Affective Response Model: A Theoretical Framework of Affective Concepts and Their Relationships in the ICT Context1,"Volume 37, Issue 1",March 2013,"Affect is a critical factor in human decisions and behaviors within many social contexts. In the information and communication technology (ICT) context, a growing number of studies consider the affective dimension of human interaction with ICTs. However, few of these studies take systematic approaches, resulting in inconsistent conclusions and contradictory advice for researchers and practitioners. Many of these issues stem from ambiguous conceptualizations of various affective concepts and their relationships. Before researchers can address questions such as “what causes affective responses in an ICT context” and “what impacts do affective responses have on human interaction with ICTs,” a theoretical foundation for affective concepts and their relationships has to be established.This theory and review paper addresses three research questions: (1) What are pertinent affective concepts in the ICT context? (2) In what ways are these affective concepts similar to, or different from each other? (3) How do these affective concepts relate to or influence one another? Based on theoretical reasoning and empirical evidence, the affective response model (ARM) is developed. ARM is a theoretically bound conceptual framework that provides a systematic and holistic reference map for any ICT study that considers affect. It includes a taxonomy that classifies affective concepts along five dimensions: the residing, the temporal, the particular/general stimulus, the object/behavior stimulus, and the process/outcome dimensions. ARM also provides a nomological network to indicate the causal or co-occurring relationships among the various types of affective concepts in an ICT interaction episode. ARM has the power for explaining and predicting, as well as prescribing, potential future research directions.",[],"Zhang, Ping",N/A,"School of Information Studies, Syracuse University, Syracuse, NY 13244 U.S.A."
https://misq.umn.edu/misq/article/37/1/275/1503/Internet-Privacy-Concerns-An-Integrated,MIS Quarterly,Internet Privacy Concerns: An Integrated Conceptualization and Four Empirical Studies1,"Volume 37, Issue 1",March 2013,"Internet privacy concerns (IPC) is an area of study that is receiving increased attention due to the huge amount of personal information being gathered, stored, transmitted, and published on the Internet. While there is an emerging literature on IPC, there is limited agreement about its conceptualization in terms of its key dimensions and its factor structure. Based on the multidimensional developmental theory and a review of the prior literature, we identify alternative conceptualizations of IPC. We examine the various conceptualizations of IPC with four online surveys involving nearly 4,000 Internet users. As a baseline, study 1 compares the integrated conceptualization of IPC to two existing conceptualizations in the literature. While the results provide support for the integrated conceptualization, the second-order factor model does not outperform the correlated first-order factor model. Study 2 replicates the study on a different sample and confirms the results of study 1. We also investigate whether the prior results are affected by the different perspectives adopted in the wording of items in the original instruments. In study 3, we find that focusing on one’s concern for website behavior (rather than one’s expectation of website behavior) and adopting a consistent perspective in the wording of the items help to improve the validity of the factor structure. We then examine the hypothesized third-order conceptualizations of IPC through a number of alternative higher-order models. The empirical results confirm that, in general, the third-order conceptualizations of IPC outperform their lower-order alternatives. In addition, the conceptualization of IPC that has the best fit with the data contains a third-order general IPC factor, two second-order factors of interaction management and information management, and six first-order factors (i.e., collection, secondary usage, errors, improper access, control, and awareness). Study 4 cross-validates the results with another data set and examines IPC within the context of a nomological network. The results confirm that the third-order conceptualization of IPC has nomological validity, and it is a significant determinant of both trusting beliefs and risk beliefs. Our research helps to resolve inconsistencies in the key underlying dimensions of IPC, the factor structure of IPC, and the wording of the original items in prior instruments of IPC. Finally, we discuss the implications of this research.",[],"Hong, Weiyin",N/A,"Department of MIS, University of Nevada, Las Vegas, NV 89154 U.S.A."
https://misq.umn.edu/misq/article/37/1/275/1503/Internet-Privacy-Concerns-An-Integrated,MIS Quarterly,Internet Privacy Concerns: An Integrated Conceptualization and Four Empirical Studies1,"Volume 37, Issue 1",March 2013,"Internet privacy concerns (IPC) is an area of study that is receiving increased attention due to the huge amount of personal information being gathered, stored, transmitted, and published on the Internet. While there is an emerging literature on IPC, there is limited agreement about its conceptualization in terms of its key dimensions and its factor structure. Based on the multidimensional developmental theory and a review of the prior literature, we identify alternative conceptualizations of IPC. We examine the various conceptualizations of IPC with four online surveys involving nearly 4,000 Internet users. As a baseline, study 1 compares the integrated conceptualization of IPC to two existing conceptualizations in the literature. While the results provide support for the integrated conceptualization, the second-order factor model does not outperform the correlated first-order factor model. Study 2 replicates the study on a different sample and confirms the results of study 1. We also investigate whether the prior results are affected by the different perspectives adopted in the wording of items in the original instruments. In study 3, we find that focusing on one’s concern for website behavior (rather than one’s expectation of website behavior) and adopting a consistent perspective in the wording of the items help to improve the validity of the factor structure. We then examine the hypothesized third-order conceptualizations of IPC through a number of alternative higher-order models. The empirical results confirm that, in general, the third-order conceptualizations of IPC outperform their lower-order alternatives. In addition, the conceptualization of IPC that has the best fit with the data contains a third-order general IPC factor, two second-order factors of interaction management and information management, and six first-order factors (i.e., collection, secondary usage, errors, improper access, control, and awareness). Study 4 cross-validates the results with another data set and examines IPC within the context of a nomological network. The results confirm that the third-order conceptualization of IPC has nomological validity, and it is a significant determinant of both trusting beliefs and risk beliefs. Our research helps to resolve inconsistencies in the key underlying dimensions of IPC, the factor structure of IPC, and the wording of the original items in prior instruments of IPC. Finally, we discuss the implications of this research.",[],"Thong, James Y. L.",N/A,"Department of ISOM, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong"
https://misq.umn.edu/misq/article/37/1/299/1508/Social-Influence-and-Knowledge-Management-Systems,MIS Quarterly,Social Influence and Knowledge Management Systems Use: Evidence From Panel Data1,"Volume 37, Issue 1",March 2013,"Theory suggests that coworkers may influence individuals’ technology use behaviors, but there is limited research in the technology diffusion literature that explicates how such social influence processes operate after initial adoption. We investigate how two key social influence mechanisms (identification and internalization) may explain the growth over time in individuals’ use of knowledge management systems (KMS)—a technology that because of its publicly visible use provides a rich context for investigating social influence. We test our hypotheses using longitudinal KMS usage data on over 80,000 employees of a management consulting firm. Our approach infers the presence of identification and internalization from associations between actual system use behaviors by a focal individual and prior system use by a range of reference groups. Evidence of these kinds of associations between system use behaviors helps construct a more complete picture of social influence mechanisms, and is to our knowledge novel to the technology diffusion literature. Our results confirm the utility of this approach for understanding social influence effects and reveal a fine-grained pattern of influence across different social groups: we found strong support for bottom-up social influence across hierarchical levels, limited support for peer-level influence within levels, and no support for top-down influence.",[],"Wang, Yinglei",N/A,"Fred C. Manning School of Business, Acadia University, Wolfville, Nova Scotia, B4P 2R6 Canada"
https://misq.umn.edu/misq/article/37/1/299/1508/Social-Influence-and-Knowledge-Management-Systems,MIS Quarterly,Social Influence and Knowledge Management Systems Use: Evidence From Panel Data1,"Volume 37, Issue 1",March 2013,"Theory suggests that coworkers may influence individuals’ technology use behaviors, but there is limited research in the technology diffusion literature that explicates how such social influence processes operate after initial adoption. We investigate how two key social influence mechanisms (identification and internalization) may explain the growth over time in individuals’ use of knowledge management systems (KMS)—a technology that because of its publicly visible use provides a rich context for investigating social influence. We test our hypotheses using longitudinal KMS usage data on over 80,000 employees of a management consulting firm. Our approach infers the presence of identification and internalization from associations between actual system use behaviors by a focal individual and prior system use by a range of reference groups. Evidence of these kinds of associations between system use behaviors helps construct a more complete picture of social influence mechanisms, and is to our knowledge novel to the technology diffusion literature. Our results confirm the utility of this approach for understanding social influence effects and reveal a fine-grained pattern of influence across different social groups: we found strong support for bottom-up social influence across hierarchical levels, limited support for peer-level influence within levels, and no support for top-down influence.",[],"Meister, Darren B.",N/A,"Richard Ivey School of Business, The University of Western Ontario, London, Ontario, N6A 3K7 Canada"
https://misq.umn.edu/misq/article/37/1/299/1508/Social-Influence-and-Knowledge-Management-Systems,MIS Quarterly,Social Influence and Knowledge Management Systems Use: Evidence From Panel Data1,"Volume 37, Issue 1",March 2013,"Theory suggests that coworkers may influence individuals’ technology use behaviors, but there is limited research in the technology diffusion literature that explicates how such social influence processes operate after initial adoption. We investigate how two key social influence mechanisms (identification and internalization) may explain the growth over time in individuals’ use of knowledge management systems (KMS)—a technology that because of its publicly visible use provides a rich context for investigating social influence. We test our hypotheses using longitudinal KMS usage data on over 80,000 employees of a management consulting firm. Our approach infers the presence of identification and internalization from associations between actual system use behaviors by a focal individual and prior system use by a range of reference groups. Evidence of these kinds of associations between system use behaviors helps construct a more complete picture of social influence mechanisms, and is to our knowledge novel to the technology diffusion literature. Our results confirm the utility of this approach for understanding social influence effects and reveal a fine-grained pattern of influence across different social groups: we found strong support for bottom-up social influence across hierarchical levels, limited support for peer-level influence within levels, and no support for top-down influence.",[],"Gray, Peter H.",N/A,"McIntire School of Commerce, University of Virginia, Charlottesville, VA 22904-4173 U.S.A."
https://misq.umn.edu/misq/article/37/1/315/1514/Information-Technology-Outsourcing-and-Non-It,MIS Quarterly,Information Technology Outsourcing and Non-It Operating Costs: An Empirical Investigation1,"Volume 37, Issue 1",March 2013,"Does information technology outsourcing reduce non-IT operating costs? This study examines this question and also asks whether internal IT investments moderate the relationship between IT outsourcing and non-IT operating costs. Using a panel data set of approximately 300 U.S. firms from 1999 to 2003, we find that IT outsourcing has a significant negative association with firms’ non-IT operating costs. However, this finding does not imply that firms should completely outsource their entire IT function. Our results suggest that firms benefit more in terms of reduction in non-IT operating costs when they also have higher levels of complementary investments in internal IT, especially IT labor. Investments in internal IT systems can make business processes more amenable to outsourcing, and complementary investments in internal IT staff can facilitate monitoring of vendor performance and coordination with vendors. We discuss the implications of these findings for further research and for practice.",[],"Han, Kunsoo",N/A,"Desautels Faculty of Management, McGill University, Montréal, Quebec Canada H3A 1G5"
https://misq.umn.edu/misq/article/37/1/315/1514/Information-Technology-Outsourcing-and-Non-It,MIS Quarterly,Information Technology Outsourcing and Non-It Operating Costs: An Empirical Investigation1,"Volume 37, Issue 1",March 2013,"Does information technology outsourcing reduce non-IT operating costs? This study examines this question and also asks whether internal IT investments moderate the relationship between IT outsourcing and non-IT operating costs. Using a panel data set of approximately 300 U.S. firms from 1999 to 2003, we find that IT outsourcing has a significant negative association with firms’ non-IT operating costs. However, this finding does not imply that firms should completely outsource their entire IT function. Our results suggest that firms benefit more in terms of reduction in non-IT operating costs when they also have higher levels of complementary investments in internal IT, especially IT labor. Investments in internal IT systems can make business processes more amenable to outsourcing, and complementary investments in internal IT staff can facilitate monitoring of vendor performance and coordination with vendors. We discuss the implications of these findings for further research and for practice.",[],"Mithas, Sunil",N/A,"Robert H. Smith School of Business, University of Maryland, College Park, MD 20742 U.S.A."
https://misq.umn.edu/misq/article/37/1/iii/1512/Editor-s-Comments,MIS Quarterly,Editor’s Comments,"Volume 37, Issue 1",March 2013,N/A,[],"Goes, Paulo B.",N/A,"Eller College of Management, University of Arizona"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/37/2/337/103/Positioning-and-Presenting-Design-Science-Research,MIS Quarterly,Positioning and Presenting Design Science Research for Maximum Impact1,"Volume 37, Issue 2",June 2013,"Design science research (DSR) has staked its rightful ground as an important and legitimate Information Systems (IS) research paradigm. We contend that DSR has yet to attain its full potential impact on the development and use of information systems due to gaps in the understanding and application of DSR concepts and methods. This essay aims to help researchers (1) appreciate the levels of artifact abstractions that may be DSR contributions, (2) identify appropriate ways of consuming and producing knowledge when they are preparing journal articles or other scholarly works, (3) understand and position the knowledge contributions of their research projects, and (4) structure a DSR article so that it emphasizes significant contributions to the knowledge base. Our focal contribution is the DSR knowledge contribution framework with two dimensions based on the existing state of knowledge in both the problem and solution domains for the research opportunity under study. In addition, we propose a DSR communication schema with similarities to more conventional publication patterns, but which substitutes the description of the DSR artifact in place of a traditional results section. We evaluate the DSR contribution framework and the DSR communication schema via examinations of DSR exemplar publications.",[],"Gregor, Shirley",N/A,"School of Accounting & Business Information Systems, College of Business and Economics, The Australian National University, Canberra ACT 0200 Australia"
https://misq.umn.edu/misq/article/37/2/337/103/Positioning-and-Presenting-Design-Science-Research,MIS Quarterly,Positioning and Presenting Design Science Research for Maximum Impact1,"Volume 37, Issue 2",June 2013,"Design science research (DSR) has staked its rightful ground as an important and legitimate Information Systems (IS) research paradigm. We contend that DSR has yet to attain its full potential impact on the development and use of information systems due to gaps in the understanding and application of DSR concepts and methods. This essay aims to help researchers (1) appreciate the levels of artifact abstractions that may be DSR contributions, (2) identify appropriate ways of consuming and producing knowledge when they are preparing journal articles or other scholarly works, (3) understand and position the knowledge contributions of their research projects, and (4) structure a DSR article so that it emphasizes significant contributions to the knowledge base. Our focal contribution is the DSR knowledge contribution framework with two dimensions based on the existing state of knowledge in both the problem and solution domains for the research opportunity under study. In addition, we propose a DSR communication schema with similarities to more conventional publication patterns, but which substitutes the description of the DSR artifact in place of a traditional results section. We evaluate the DSR contribution framework and the DSR communication schema via examinations of DSR exemplar publications.",[],"Hevner, Alan R.",N/A,"Information Systems and Decision Sciences, College of Business, University of South Florida, 4202 East Fowler Avenue, CIS1040, Tampa, FL 33620-7800 U.S.A."
https://misq.umn.edu/misq/article/37/2/357/542/The-Ambivalent-Ontology-of-Digital-Artifacts1,MIS Quarterly,The Ambivalent Ontology of Digital Artifacts1,"Volume 37, Issue 2",June 2013,"Digital artifacts are embedded in wider and constantly shifting ecosystems such that they become increasingly editable, interactive, reprogrammable, and distributable. This state of flux and constant transfiguration renders the value and utility of these artifacts contingent on shifting webs of functional relations with other artifacts across specific contexts and organizations. By the same token, it apportions control over the development and use of these artifacts over a range of dispersed stakeholders and makes their management a complex technical and social undertaking. These ideas are illustrated with reference to (1) provenance and authenticity of digital documents within the overall context of archiving and social memory and (2) the content dynamics occasioned by the findability of content mediated by Internet search engines. We conclude that the steady change and transfiguration of digital artifacts signal a shift of epochal dimensions that calls for rethinking some of the inherited wisdom in IS research and practice.",[],"Kallinikos, Jannis",N/A,"Department of Management, Information Systems and Innovation Group, The London School of Economics and Political Science, Houghton Street, London WC2A 2AE, United Kingdom"
https://misq.umn.edu/misq/article/37/2/357/542/The-Ambivalent-Ontology-of-Digital-Artifacts1,MIS Quarterly,The Ambivalent Ontology of Digital Artifacts1,"Volume 37, Issue 2",June 2013,"Digital artifacts are embedded in wider and constantly shifting ecosystems such that they become increasingly editable, interactive, reprogrammable, and distributable. This state of flux and constant transfiguration renders the value and utility of these artifacts contingent on shifting webs of functional relations with other artifacts across specific contexts and organizations. By the same token, it apportions control over the development and use of these artifacts over a range of dispersed stakeholders and makes their management a complex technical and social undertaking. These ideas are illustrated with reference to (1) provenance and authenticity of digital documents within the overall context of archiving and social memory and (2) the content dynamics occasioned by the findability of content mediated by Internet search engines. We conclude that the steady change and transfiguration of digital artifacts signal a shift of epochal dimensions that calls for rethinking some of the inherited wisdom in IS research and practice.",[],"Aaltonen, Aleksi",N/A,"Management and Organisation, Hanken School of Economics, Arkadiankatu 22, 00100 Helsinki, Finland"
https://misq.umn.edu/misq/article/37/2/357/542/The-Ambivalent-Ontology-of-Digital-Artifacts1,MIS Quarterly,The Ambivalent Ontology of Digital Artifacts1,"Volume 37, Issue 2",June 2013,"Digital artifacts are embedded in wider and constantly shifting ecosystems such that they become increasingly editable, interactive, reprogrammable, and distributable. This state of flux and constant transfiguration renders the value and utility of these artifacts contingent on shifting webs of functional relations with other artifacts across specific contexts and organizations. By the same token, it apportions control over the development and use of these artifacts over a range of dispersed stakeholders and makes their management a complex technical and social undertaking. These ideas are illustrated with reference to (1) provenance and authenticity of digital documents within the overall context of archiving and social memory and (2) the content dynamics occasioned by the findability of content mediated by Internet search engines. We conclude that the steady change and transfiguration of digital artifacts signal a shift of epochal dimensions that calls for rethinking some of the inherited wisdom in IS research and practice.",[],"Marton, Attila",N/A,"Department of IT Management, Copenhagen Business School, Howitzvej 60, 2000 Frederiksberg, Denmark"
https://misq.umn.edu/misq/article/37/2/371/528/Impactful-Research-on-Transformational-Information,MIS Quarterly,Impactful Research on Transformational Information Technology: An Opportunity to Inform New Audiences1,"Volume 37, Issue 2",June 2013,"Information technology has arguably been one of the most important drivers of economic and social value in the last 50 years, enabling transformational change in virtually every aspect of society. Although the Information Systems community is engaged in significant research on IT, the reach of our findings may be limited. In this commentary, our objective is to focus the IS community’s attention on the striking transformations in economic and social systems spawned by IT and to encourage more research that offers useful implications for policy. We present examples of transformations occurring in four distinct sectors of the economy and propose policy-relevant questions that need to be addressed. We urge researchers to write papers based on their findings that inform policy makers, managers, and decision makers about the issues that transformational technologies raise. Finally, we suggest a new outlet to publish these essays on the implications of transformational informational technology.",[],"Lucas, Henry C.",N/A,"Smith School of Business, University of Maryland, College Park, MD 20742-1815 U.S.A."
https://misq.umn.edu/misq/article/37/2/371/528/Impactful-Research-on-Transformational-Information,MIS Quarterly,Impactful Research on Transformational Information Technology: An Opportunity to Inform New Audiences1,"Volume 37, Issue 2",June 2013,"Information technology has arguably been one of the most important drivers of economic and social value in the last 50 years, enabling transformational change in virtually every aspect of society. Although the Information Systems community is engaged in significant research on IT, the reach of our findings may be limited. In this commentary, our objective is to focus the IS community’s attention on the striking transformations in economic and social systems spawned by IT and to encourage more research that offers useful implications for policy. We present examples of transformations occurring in four distinct sectors of the economy and propose policy-relevant questions that need to be addressed. We urge researchers to write papers based on their findings that inform policy makers, managers, and decision makers about the issues that transformational technologies raise. Finally, we suggest a new outlet to publish these essays on the implications of transformational informational technology.",[],"Agarwal, Ritu",N/A,"Smith School of Business, University of Maryland, College Park, MD 20742-1815 U.S.A."
https://misq.umn.edu/misq/article/37/2/371/528/Impactful-Research-on-Transformational-Information,MIS Quarterly,Impactful Research on Transformational Information Technology: An Opportunity to Inform New Audiences1,"Volume 37, Issue 2",June 2013,"Information technology has arguably been one of the most important drivers of economic and social value in the last 50 years, enabling transformational change in virtually every aspect of society. Although the Information Systems community is engaged in significant research on IT, the reach of our findings may be limited. In this commentary, our objective is to focus the IS community’s attention on the striking transformations in economic and social systems spawned by IT and to encourage more research that offers useful implications for policy. We present examples of transformations occurring in four distinct sectors of the economy and propose policy-relevant questions that need to be addressed. We urge researchers to write papers based on their findings that inform policy makers, managers, and decision makers about the issues that transformational technologies raise. Finally, we suggest a new outlet to publish these essays on the implications of transformational informational technology.",[],"Clemons, Eric K.",N/A,"The Wharton School, University of Pennsylvania, Philadelphia, PA 19104 U.S.A."
https://misq.umn.edu/misq/article/37/2/371/528/Impactful-Research-on-Transformational-Information,MIS Quarterly,Impactful Research on Transformational Information Technology: An Opportunity to Inform New Audiences1,"Volume 37, Issue 2",June 2013,"Information technology has arguably been one of the most important drivers of economic and social value in the last 50 years, enabling transformational change in virtually every aspect of society. Although the Information Systems community is engaged in significant research on IT, the reach of our findings may be limited. In this commentary, our objective is to focus the IS community’s attention on the striking transformations in economic and social systems spawned by IT and to encourage more research that offers useful implications for policy. We present examples of transformations occurring in four distinct sectors of the economy and propose policy-relevant questions that need to be addressed. We urge researchers to write papers based on their findings that inform policy makers, managers, and decision makers about the issues that transformational technologies raise. Finally, we suggest a new outlet to publish these essays on the implications of transformational informational technology.",[],"El Sawy, Omar A.",N/A,"Marshall School of Business, University of Southern California, Los Angeles, CA 90089 U.S.A."
https://misq.umn.edu/misq/article/37/2/371/528/Impactful-Research-on-Transformational-Information,MIS Quarterly,Impactful Research on Transformational Information Technology: An Opportunity to Inform New Audiences1,"Volume 37, Issue 2",June 2013,"Information technology has arguably been one of the most important drivers of economic and social value in the last 50 years, enabling transformational change in virtually every aspect of society. Although the Information Systems community is engaged in significant research on IT, the reach of our findings may be limited. In this commentary, our objective is to focus the IS community’s attention on the striking transformations in economic and social systems spawned by IT and to encourage more research that offers useful implications for policy. We present examples of transformations occurring in four distinct sectors of the economy and propose policy-relevant questions that need to be addressed. We urge researchers to write papers based on their findings that inform policy makers, managers, and decision makers about the issues that transformational technologies raise. Finally, we suggest a new outlet to publish these essays on the implications of transformational informational technology.",[],"Weber, Bruce",N/A,"Alfred Lerner College of Business and Economics, University of Delaware, Newark, DE 19716 U.S.A."
https://misq.umn.edu/misq/article/37/2/383/90/When-Filling-the-Wait-Makes-it-Feel-Longer-A,MIS Quarterly,When Filling the Wait Makes it Feel Longer: A Paradigm Shift Perspective for Managing Online Delay1,"Volume 37, Issue 2",June 2013,"As one of the most commonly experienced problems on the Internet, download delay is a significant impediment to the success of e-commerce websites. While some research has examined how such delays can be reduced and how much delay online users will tolerate, little research has taken a theoretically grounded approach to managing perceptions of the wait. Based on time perception theories, we develop a research model of the effects of actual wait time, amount of information, and direction of attention on perceptions of the wait. Two empirical studies were conducted using an experimental travel website to test the proposed hypotheses. The results show that with shorter waits, providing additional visual content, such as a travel picture, may make the wait feel longer. However, with longer waits, additional visual content that distracts the user from the passage of time makes the wait feel shorter and reduces users’ negative affect toward the wait. Further, the benefits of providing visual content in longer waits are enhanced as more content is provided. Visual content should also be chosen to distract the user from time and temporal processing, as reminding users of the passage of time can encourage temporal processing and make the wait feel longer, especially in longer waits or when the amount of temporal visual content is high. Our findings extend time perception theories and contribute to the literature by identifying a potential paradigm shift, from the retrospective to the prospective paradigm, when waiting times are prolonged. Post hoc study results confirm the practical contribution of our research, demonstrating that several key findings are counter-intuitive to professional web designers.",[],"Hong, Weiyin",N/A,"Department of ISOM, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong"
https://misq.umn.edu/misq/article/37/2/383/90/When-Filling-the-Wait-Makes-it-Feel-Longer-A,MIS Quarterly,When Filling the Wait Makes it Feel Longer: A Paradigm Shift Perspective for Managing Online Delay1,"Volume 37, Issue 2",June 2013,"As one of the most commonly experienced problems on the Internet, download delay is a significant impediment to the success of e-commerce websites. While some research has examined how such delays can be reduced and how much delay online users will tolerate, little research has taken a theoretically grounded approach to managing perceptions of the wait. Based on time perception theories, we develop a research model of the effects of actual wait time, amount of information, and direction of attention on perceptions of the wait. Two empirical studies were conducted using an experimental travel website to test the proposed hypotheses. The results show that with shorter waits, providing additional visual content, such as a travel picture, may make the wait feel longer. However, with longer waits, additional visual content that distracts the user from the passage of time makes the wait feel shorter and reduces users’ negative affect toward the wait. Further, the benefits of providing visual content in longer waits are enhanced as more content is provided. Visual content should also be chosen to distract the user from time and temporal processing, as reminding users of the passage of time can encourage temporal processing and make the wait feel longer, especially in longer waits or when the amount of temporal visual content is high. Our findings extend time perception theories and contribute to the literature by identifying a potential paradigm shift, from the retrospective to the prospective paradigm, when waiting times are prolonged. Post hoc study results confirm the practical contribution of our research, demonstrating that several key findings are counter-intuitive to professional web designers.",[],"Hess, Traci J.",N/A,"Department of MIS, University of Nevada, Las Vegas, Las Vegas, NV 89120 U.S.A."
https://misq.umn.edu/misq/article/37/2/383/90/When-Filling-the-Wait-Makes-it-Feel-Longer-A,MIS Quarterly,When Filling the Wait Makes it Feel Longer: A Paradigm Shift Perspective for Managing Online Delay1,"Volume 37, Issue 2",June 2013,"As one of the most commonly experienced problems on the Internet, download delay is a significant impediment to the success of e-commerce websites. While some research has examined how such delays can be reduced and how much delay online users will tolerate, little research has taken a theoretically grounded approach to managing perceptions of the wait. Based on time perception theories, we develop a research model of the effects of actual wait time, amount of information, and direction of attention on perceptions of the wait. Two empirical studies were conducted using an experimental travel website to test the proposed hypotheses. The results show that with shorter waits, providing additional visual content, such as a travel picture, may make the wait feel longer. However, with longer waits, additional visual content that distracts the user from the passage of time makes the wait feel shorter and reduces users’ negative affect toward the wait. Further, the benefits of providing visual content in longer waits are enhanced as more content is provided. Visual content should also be chosen to distract the user from time and temporal processing, as reminding users of the passage of time can encourage temporal processing and make the wait feel longer, especially in longer waits or when the amount of temporal visual content is high. Our findings extend time perception theories and contribute to the literature by identifying a potential paradigm shift, from the retrospective to the prospective paradigm, when waiting times are prolonged. Post hoc study results confirm the practical contribution of our research, demonstrating that several key findings are counter-intuitive to professional web designers.",[],"Hardin, Andrew",N/A,"Isenberg School of Management, University of Massachusetts, Amherst, Amhest, MA 01003 U.S.A."
https://misq.umn.edu/misq/article/37/2/407/95/Community-Intelligence-and-Social-Media-Services-A,MIS Quarterly,Community Intelligence and Social Media Services: A Rumor Theoretic Analysis of Tweets During Social Crises1,"Volume 37, Issue 2",June 2013,"Recent extreme events show that Twitter, a micro-blogging service, is emerging as the dominant social reporting tool to spread information on social crises. It is elevating the online public community to the status of first responders who can collectively cope with social crises. However, at the same time, many warnings have been raised about the reliability of community intelligence obtained through social reporting by the amateur online community. Using rumor theory, this paper studies citizen-driven information processing through Twitter services using data from three social crises: the Mumbai terrorist attacks in 2008, the Toyota recall in 2010, and the Seattle café shooting incident in 2012. We approach social crises as communal efforts for community intelligence gathering and collective information processing to cope with and adapt to uncertain external situations. We explore two issues: (1) collective social reporting as an information processing mechanism to address crisis problems and gather community intelligence, and (2) the degeneration of social reporting into collective rumor mills. Our analysis reveals that information with no clear source provided was the most important, personal involvement next in importance, and anxiety the least yet still important rumor causing factor on Twitter under social crisis situations.",[],"Oh, Onook",N/A,"Warwick Business School, University of Warwick, Coventry, CV4 7AL United Kingdom"
https://misq.umn.edu/misq/article/37/2/407/95/Community-Intelligence-and-Social-Media-Services-A,MIS Quarterly,Community Intelligence and Social Media Services: A Rumor Theoretic Analysis of Tweets During Social Crises1,"Volume 37, Issue 2",June 2013,"Recent extreme events show that Twitter, a micro-blogging service, is emerging as the dominant social reporting tool to spread information on social crises. It is elevating the online public community to the status of first responders who can collectively cope with social crises. However, at the same time, many warnings have been raised about the reliability of community intelligence obtained through social reporting by the amateur online community. Using rumor theory, this paper studies citizen-driven information processing through Twitter services using data from three social crises: the Mumbai terrorist attacks in 2008, the Toyota recall in 2010, and the Seattle café shooting incident in 2012. We approach social crises as communal efforts for community intelligence gathering and collective information processing to cope with and adapt to uncertain external situations. We explore two issues: (1) collective social reporting as an information processing mechanism to address crisis problems and gather community intelligence, and (2) the degeneration of social reporting into collective rumor mills. Our analysis reveals that information with no clear source provided was the most important, personal involvement next in importance, and anxiety the least yet still important rumor causing factor on Twitter under social crisis situations.",[],"Agrawal, Manish",N/A,"Department of Information Systems and Decision Sciences, College of Business Administration, University of South Florida, 4202 E. Fowler Avenue, CIS 1040, Tampa, FL 33620 U.S.A."
https://misq.umn.edu/misq/article/37/2/407/95/Community-Intelligence-and-Social-Media-Services-A,MIS Quarterly,Community Intelligence and Social Media Services: A Rumor Theoretic Analysis of Tweets During Social Crises1,"Volume 37, Issue 2",June 2013,"Recent extreme events show that Twitter, a micro-blogging service, is emerging as the dominant social reporting tool to spread information on social crises. It is elevating the online public community to the status of first responders who can collectively cope with social crises. However, at the same time, many warnings have been raised about the reliability of community intelligence obtained through social reporting by the amateur online community. Using rumor theory, this paper studies citizen-driven information processing through Twitter services using data from three social crises: the Mumbai terrorist attacks in 2008, the Toyota recall in 2010, and the Seattle café shooting incident in 2012. We approach social crises as communal efforts for community intelligence gathering and collective information processing to cope with and adapt to uncertain external situations. We explore two issues: (1) collective social reporting as an information processing mechanism to address crisis problems and gather community intelligence, and (2) the degeneration of social reporting into collective rumor mills. Our analysis reveals that information with no clear source provided was the most important, personal involvement next in importance, and anxiety the least yet still important rumor causing factor on Twitter under social crisis situations.",[],"Rao, H. Raghav",N/A,"Department of Management Science and Systems, School of Management, Jacobs Management Center, SUNY at Buffalo, Buffalo, NY 14269-4000 U.S.A."
https://misq.umn.edu/misq/article/37/2/427/98/Knowing-What-a-User-Likes-A-Design-Science,MIS Quarterly,Knowing What a User Likes: A Design Science Approach to Interfaces that Automatically Adapt to Culture1,"Volume 37, Issue 2",June 2013,"Adapting user interfaces to a user’s cultural background can increase satisfaction, revenue, and market share. Conventional approaches to catering for culture are restricted to adaptations for specific countries and modify only a limited number of interface components, such as the language or date and time formats. We argue that a more comprehensive personalization of interfaces to cultural background is needed to appeal to users in expanding markets. This paper introduces a low-cost, yet efficient method to achieve this goal: cultural adaptivity. Culturally adaptive interfaces are able to adapt their look and feel to suit visual preferences. In a design science approach, we have developed a number of artifacts that support cultural adaptivity, including a prototype web application. We evaluate the efficacy of the prototype’s automatically generated interfaces by comparing them with the preferred interfaces of 105 Rwandan, Swiss, Thai, and multicultural users. The findings demonstrate the feasibility of providing users with interfaces that correspond to their cultural preferences in a novel yet effective manner.",[],"Reinecke, Katharina",N/A,"Harvard School of Engineering and Applied Sciences, 33 Oxford Street, Cambridge, MA 02138 U.S.A."
https://misq.umn.edu/misq/article/37/2/427/98/Knowing-What-a-User-Likes-A-Design-Science,MIS Quarterly,Knowing What a User Likes: A Design Science Approach to Interfaces that Automatically Adapt to Culture1,"Volume 37, Issue 2",June 2013,"Adapting user interfaces to a user’s cultural background can increase satisfaction, revenue, and market share. Conventional approaches to catering for culture are restricted to adaptations for specific countries and modify only a limited number of interface components, such as the language or date and time formats. We argue that a more comprehensive personalization of interfaces to cultural background is needed to appeal to users in expanding markets. This paper introduces a low-cost, yet efficient method to achieve this goal: cultural adaptivity. Culturally adaptive interfaces are able to adapt their look and feel to suit visual preferences. In a design science approach, we have developed a number of artifacts that support cultural adaptivity, including a prototype web application. We evaluate the efficacy of the prototype’s automatically generated interfaces by comparing them with the preferred interfaces of 105 Rwandan, Swiss, Thai, and multicultural users. The findings demonstrate the feasibility of providing users with interfaces that correspond to their cultural preferences in a novel yet effective manner.",[],"Bernstein, Abraham",N/A,"Department of Informatics, University of Zurich, Binzmuehlestrasse 14, 8050 Zurich, Switzerland"
https://misq.umn.edu/misq/article/37/2/455/101/The-Impact-of-Shaping-on-Knowledge-Reuse-for,MIS Quarterly,The Impact of Shaping on Knowledge Reuse for Organizational Improvement with Wikis1,"Volume 37, Issue 2",June 2013,"In this study, we explore the Wiki affordance of enabling shaping behavior within organizational intranets supported by Wikis. Shaping is the continuous revision of one’s own and others’ contributions to a Wiki. Shaping promotes knowledge reuse through improved knowledge integration. Recognizing and clarifying the role of shaping allows us to theorize new ways in which knowledge resources affect knowledge reuse. We examine the role of three knowledge resources of a Wiki contributor: knowledge depth, knowledge breadth, and assessment of the level of development of the Wiki community’s transactive memory system. We offer preliminary evidence based on a sample of experienced organizational Wiki users that the three different knowledge resources have differential effects on shaping, that these effects differ from the effects on the more common user behavior of simply adding domain knowledge to a Wiki, and that shaping and adding each independently affect contributors’ perceptions that their knowledge in the Wiki has been reused for organizational improvement. By empirically distinguishing between the different knowledge antecedents and consequences of shaping and adding, we derive implications for theory and research on knowledge integration and reuse.",[],"Majchrzak, Ann",N/A,"Information Systems, Marshall School of Business, University of Southern California, Los Angeles, CA 90089 U.S.A."
https://misq.umn.edu/misq/article/37/2/455/101/The-Impact-of-Shaping-on-Knowledge-Reuse-for,MIS Quarterly,The Impact of Shaping on Knowledge Reuse for Organizational Improvement with Wikis1,"Volume 37, Issue 2",June 2013,"In this study, we explore the Wiki affordance of enabling shaping behavior within organizational intranets supported by Wikis. Shaping is the continuous revision of one’s own and others’ contributions to a Wiki. Shaping promotes knowledge reuse through improved knowledge integration. Recognizing and clarifying the role of shaping allows us to theorize new ways in which knowledge resources affect knowledge reuse. We examine the role of three knowledge resources of a Wiki contributor: knowledge depth, knowledge breadth, and assessment of the level of development of the Wiki community’s transactive memory system. We offer preliminary evidence based on a sample of experienced organizational Wiki users that the three different knowledge resources have differential effects on shaping, that these effects differ from the effects on the more common user behavior of simply adding domain knowledge to a Wiki, and that shaping and adding each independently affect contributors’ perceptions that their knowledge in the Wiki has been reused for organizational improvement. By empirically distinguishing between the different knowledge antecedents and consequences of shaping and adding, we derive implications for theory and research on knowledge integration and reuse.",[],"Wagner, Christian",N/A,"Esade Business School, Universidad Ramon Llull, Barcelona, Spain"
https://misq.umn.edu/misq/article/37/2/455/101/The-Impact-of-Shaping-on-Knowledge-Reuse-for,MIS Quarterly,The Impact of Shaping on Knowledge Reuse for Organizational Improvement with Wikis1,"Volume 37, Issue 2",June 2013,"In this study, we explore the Wiki affordance of enabling shaping behavior within organizational intranets supported by Wikis. Shaping is the continuous revision of one’s own and others’ contributions to a Wiki. Shaping promotes knowledge reuse through improved knowledge integration. Recognizing and clarifying the role of shaping allows us to theorize new ways in which knowledge resources affect knowledge reuse. We examine the role of three knowledge resources of a Wiki contributor: knowledge depth, knowledge breadth, and assessment of the level of development of the Wiki community’s transactive memory system. We offer preliminary evidence based on a sample of experienced organizational Wiki users that the three different knowledge resources have differential effects on shaping, that these effects differ from the effects on the more common user behavior of simply adding domain knowledge to a Wiki, and that shaping and adding each independently affect contributors’ perceptions that their knowledge in the Wiki has been reused for organizational improvement. By empirically distinguishing between the different knowledge antecedents and consequences of shaping and adding, we derive implications for theory and research on knowledge integration and reuse.",[],"Yates, Dave",N/A,"School of Creative Media and Department of Information Systems, City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong SAR"
https://misq.umn.edu/misq/article/37/2/471/104/Digital-Business-Strategy-Toward-a-Next-Generation,MIS Quarterly,Digital Business Strategy: Toward a Next Generation of Insights,"Volume 37, Issue 2",June 2013,"Over the last three decades, the prevailing view of information technology strategy has been that it is a functional-level strategy that must be aligned with the firm’s chosen business strategy. Even within this so-called alignment view, business strategy directed IT strategy. During the last decade, the business infrastructure has become digital with increased interconnections among products, processes, and services. Across many firms spanning different industries and sectors, digital technologies (viewed as combinations of information, computing, communication, and connectivity technologies) are fundamentally transforming business strategies, business processes, firm capabilities, products and services, and key interfirm relationships in extended business networks. Accordingly, we argue that the time is right to rethink the role of IT strategy, from that of a functional-level strategy—aligned but essentially always subordinate to business strategy—to one that reflects a fusion between IT strategy and business strategy. This fusion is herein termed digital business strategy.We identify four key themes to guide our thinking on digital business strategy and help provide a framework to define the next generation of insights. The four themes are (1) the scope of digital business strategy, (2) the scale of digital business strategy, (3) the speed of digital business strategy, and (4) the sources of business value creation and capture in digital business strategy. After elaborating on each of these four themes, we discuss the success metrics and potential performance implications from pursuing a digital business strategy. We also show how the papers in the special issue shed light on digital strategies and offer directions to advance insights and shape future research.",[],"Bharadwaj, Anandhi",N/A,"Goizueta Business School, Emory University, Atlanta, GA 30332 U.S.A."
https://misq.umn.edu/misq/article/37/2/471/104/Digital-Business-Strategy-Toward-a-Next-Generation,MIS Quarterly,Digital Business Strategy: Toward a Next Generation of Insights,"Volume 37, Issue 2",June 2013,"Over the last three decades, the prevailing view of information technology strategy has been that it is a functional-level strategy that must be aligned with the firm’s chosen business strategy. Even within this so-called alignment view, business strategy directed IT strategy. During the last decade, the business infrastructure has become digital with increased interconnections among products, processes, and services. Across many firms spanning different industries and sectors, digital technologies (viewed as combinations of information, computing, communication, and connectivity technologies) are fundamentally transforming business strategies, business processes, firm capabilities, products and services, and key interfirm relationships in extended business networks. Accordingly, we argue that the time is right to rethink the role of IT strategy, from that of a functional-level strategy—aligned but essentially always subordinate to business strategy—to one that reflects a fusion between IT strategy and business strategy. This fusion is herein termed digital business strategy.We identify four key themes to guide our thinking on digital business strategy and help provide a framework to define the next generation of insights. The four themes are (1) the scope of digital business strategy, (2) the scale of digital business strategy, (3) the speed of digital business strategy, and (4) the sources of business value creation and capture in digital business strategy. After elaborating on each of these four themes, we discuss the success metrics and potential performance implications from pursuing a digital business strategy. We also show how the papers in the special issue shed light on digital strategies and offer directions to advance insights and shape future research.",[],"El Sawy, Omar A.",N/A,"Marshall School of Business, University of Southern California Los Angeles, CA 90089-1421 U.S.A."
https://misq.umn.edu/misq/article/37/2/471/104/Digital-Business-Strategy-Toward-a-Next-Generation,MIS Quarterly,Digital Business Strategy: Toward a Next Generation of Insights,"Volume 37, Issue 2",June 2013,"Over the last three decades, the prevailing view of information technology strategy has been that it is a functional-level strategy that must be aligned with the firm’s chosen business strategy. Even within this so-called alignment view, business strategy directed IT strategy. During the last decade, the business infrastructure has become digital with increased interconnections among products, processes, and services. Across many firms spanning different industries and sectors, digital technologies (viewed as combinations of information, computing, communication, and connectivity technologies) are fundamentally transforming business strategies, business processes, firm capabilities, products and services, and key interfirm relationships in extended business networks. Accordingly, we argue that the time is right to rethink the role of IT strategy, from that of a functional-level strategy—aligned but essentially always subordinate to business strategy—to one that reflects a fusion between IT strategy and business strategy. This fusion is herein termed digital business strategy.We identify four key themes to guide our thinking on digital business strategy and help provide a framework to define the next generation of insights. The four themes are (1) the scope of digital business strategy, (2) the scale of digital business strategy, (3) the speed of digital business strategy, and (4) the sources of business value creation and capture in digital business strategy. After elaborating on each of these four themes, we discuss the success metrics and potential performance implications from pursuing a digital business strategy. We also show how the papers in the special issue shed light on digital strategies and offer directions to advance insights and shape future research.",[],"Pavlou, Paul A.",N/A,"Fox School of Business, Temple University, Philadelphia, PA 19122-6083 U.S.A."
https://misq.umn.edu/misq/article/37/2/471/104/Digital-Business-Strategy-Toward-a-Next-Generation,MIS Quarterly,Digital Business Strategy: Toward a Next Generation of Insights,"Volume 37, Issue 2",June 2013,"Over the last three decades, the prevailing view of information technology strategy has been that it is a functional-level strategy that must be aligned with the firm’s chosen business strategy. Even within this so-called alignment view, business strategy directed IT strategy. During the last decade, the business infrastructure has become digital with increased interconnections among products, processes, and services. Across many firms spanning different industries and sectors, digital technologies (viewed as combinations of information, computing, communication, and connectivity technologies) are fundamentally transforming business strategies, business processes, firm capabilities, products and services, and key interfirm relationships in extended business networks. Accordingly, we argue that the time is right to rethink the role of IT strategy, from that of a functional-level strategy—aligned but essentially always subordinate to business strategy—to one that reflects a fusion between IT strategy and business strategy. This fusion is herein termed digital business strategy.We identify four key themes to guide our thinking on digital business strategy and help provide a framework to define the next generation of insights. The four themes are (1) the scope of digital business strategy, (2) the scale of digital business strategy, (3) the speed of digital business strategy, and (4) the sources of business value creation and capture in digital business strategy. After elaborating on each of these four themes, we discuss the success metrics and potential performance implications from pursuing a digital business strategy. We also show how the papers in the special issue shed light on digital strategies and offer directions to advance insights and shape future research.",[],"Venkatraman, N.",N/A,"School of Management, Boston University Boston, MA 02215 U.S.A."
https://misq.umn.edu/misq/article/37/2/iii/97/Editor-s-CommentsCommonalities-Across-IS-Silos-and,MIS Quarterly,Editor’s CommentsCommonalities Across IS Silos and Intradisciplinary Information Systems Research,"Volume 37, Issue 2",June 2013,N/A,[],"Goes, Paulo B.",N/A,"Eller College of Management, University of Arizona"
https://misq.umn.edu/misq/article/37/2/483/106/Information-Technology-and-Business-Level-Strategy,MIS Quarterly,Information Technology and Business-Level Strategy: Toward an Integrated Theoretical Perspective1,"Volume 37, Issue 2",June 2013,"Information technology matters to business success because it directly affects the mechanisms through which they create and capture value to earn a profit: IT is thus integral to a firm’s business-level strategy. Much of the extant research on the IT/strategy relationship, however, inaccurately frames IT as only a functional-level strategy. This widespread under-appreciation of the business-level role of IT indicates a need for substantial retheorizing of its role in strategy and its complex and interdependent relationship with the mechanisms through which firms generate profit. Using a comprehensive framework of potential profit mechanisms, we argue that while IT activities remain integral to the functional-level strategies of the firm, they also play several significant roles in business strategy, with substantial performance implications. IT affects industry structure and the set of business-level strategic alternatives and value-creation opportunities that a firm may pursue. Along with complementary organizational changes, IT both enhances the firm’s current (ordinary) capabilities and enables new (dynamic) capabilities, including the flexibility to focus on rapidly changing opportunities or to abandon losing initiatives while salvaging substantial asset value. Such digitally attributable capabilities also determine how much of this value, once created, can be captured by the firm—and how much will be dissipated through competition or through the power of value chain partners, the governance of which itself depends on IT. We explore these business-level strategic roles of IT and discuss several provocative implications and future research directions in the converging information systems and strategy domains.",[],"Drnevich, Paul L.",N/A,"Culverhouse College of Commerce, the University of Alabama, 361 Stadium Drive – Dept. 870225, Tuscaloosa, AL 35487 U.S.A."
https://misq.umn.edu/misq/article/37/2/483/106/Information-Technology-and-Business-Level-Strategy,MIS Quarterly,Information Technology and Business-Level Strategy: Toward an Integrated Theoretical Perspective1,"Volume 37, Issue 2",June 2013,"Information technology matters to business success because it directly affects the mechanisms through which they create and capture value to earn a profit: IT is thus integral to a firm’s business-level strategy. Much of the extant research on the IT/strategy relationship, however, inaccurately frames IT as only a functional-level strategy. This widespread under-appreciation of the business-level role of IT indicates a need for substantial retheorizing of its role in strategy and its complex and interdependent relationship with the mechanisms through which firms generate profit. Using a comprehensive framework of potential profit mechanisms, we argue that while IT activities remain integral to the functional-level strategies of the firm, they also play several significant roles in business strategy, with substantial performance implications. IT affects industry structure and the set of business-level strategic alternatives and value-creation opportunities that a firm may pursue. Along with complementary organizational changes, IT both enhances the firm’s current (ordinary) capabilities and enables new (dynamic) capabilities, including the flexibility to focus on rapidly changing opportunities or to abandon losing initiatives while salvaging substantial asset value. Such digitally attributable capabilities also determine how much of this value, once created, can be captured by the firm—and how much will be dissipated through competition or through the power of value chain partners, the governance of which itself depends on IT. We explore these business-level strategic roles of IT and discuss several provocative implications and future research directions in the converging information systems and strategy domains.",[],"Croson, David C.",N/A,"Cox School of Business, Southern Methodist University, 6212 Bishop Boulevard, Dallas, TX 75275 U.S.A."
https://misq.umn.edu/misq/article/37/2/511/107/How-a-Firm-s-Competitive-Environment-and-Digital,MIS Quarterly,How a Firm’s Competitive Environment and Digital Strategic Posture Influence Digital Business Strategy1,"Volume 37, Issue 2",June 2013,"In this paper, we examine how the competitive industry environment shapes the way that digital strategic posture (defined as a focal firm’s degree of engagement in a particular class of digital business practices relative to the industry norm) influences firms’ realized digital business strategy. We focus on two forms of digital strategy: general IT investment and IT outsourcing investment. Drawing from prior literature on determinants of IT activity and competitive dynamics, we argue that three elements of the industry environment determine whether digital strategic posture has an increasingly convergent or divergent influence on digital business strategy. By divergent influence, we mean an influence that leads to spending substantially more or less on a particular strategic activity than industry norms. We predict that a digital strategic posture (difference from the industry mean) has an increasingly divergent effect on digital business strategy under higher industry turbulence, while having an increasingly convergent effect on digital business strategy under higher industry concentration and higher industry growth. The study uses archival data for 400 U.S.-based firms from 1999 to 2006. Our findings imply that digital business strategy is not solely a matter of optimizing firm operations internally or of responding to one or two focal competitors, but also arises strikingly from awareness and responsiveness to the digital business competitive environment. Collectively, the findings provide insights on how strategic posture and industry environment influence firms’ digital business strategy.",[],"Mithas, Sunil",william.mitchell@rotman.utoronto.ca,"Robert H. Smith School of Business, University of Maryland, Van Munching Hall, College Park, MD 20742 U.S.A."
https://misq.umn.edu/misq/article/37/2/511/107/How-a-Firm-s-Competitive-Environment-and-Digital,MIS Quarterly,How a Firm’s Competitive Environment and Digital Strategic Posture Influence Digital Business Strategy1,"Volume 37, Issue 2",June 2013,"In this paper, we examine how the competitive industry environment shapes the way that digital strategic posture (defined as a focal firm’s degree of engagement in a particular class of digital business practices relative to the industry norm) influences firms’ realized digital business strategy. We focus on two forms of digital strategy: general IT investment and IT outsourcing investment. Drawing from prior literature on determinants of IT activity and competitive dynamics, we argue that three elements of the industry environment determine whether digital strategic posture has an increasingly convergent or divergent influence on digital business strategy. By divergent influence, we mean an influence that leads to spending substantially more or less on a particular strategic activity than industry norms. We predict that a digital strategic posture (difference from the industry mean) has an increasingly divergent effect on digital business strategy under higher industry turbulence, while having an increasingly convergent effect on digital business strategy under higher industry concentration and higher industry growth. The study uses archival data for 400 U.S.-based firms from 1999 to 2006. Our findings imply that digital business strategy is not solely a matter of optimizing firm operations internally or of responding to one or two focal competitors, but also arises strikingly from awareness and responsiveness to the digital business competitive environment. Collectively, the findings provide insights on how strategic posture and industry environment influence firms’ digital business strategy.",[],"Tafti, Ali",will.mitchell@duke.edu,"College of Business, University of Illinois at Urbana-Champaign, Champaign, IL 61820 U.S.A."
https://misq.umn.edu/misq/article/37/2/511/107/How-a-Firm-s-Competitive-Environment-and-Digital,MIS Quarterly,How a Firm’s Competitive Environment and Digital Strategic Posture Influence Digital Business Strategy1,"Volume 37, Issue 2",June 2013,"In this paper, we examine how the competitive industry environment shapes the way that digital strategic posture (defined as a focal firm’s degree of engagement in a particular class of digital business practices relative to the industry norm) influences firms’ realized digital business strategy. We focus on two forms of digital strategy: general IT investment and IT outsourcing investment. Drawing from prior literature on determinants of IT activity and competitive dynamics, we argue that three elements of the industry environment determine whether digital strategic posture has an increasingly convergent or divergent influence on digital business strategy. By divergent influence, we mean an influence that leads to spending substantially more or less on a particular strategic activity than industry norms. We predict that a digital strategic posture (difference from the industry mean) has an increasingly divergent effect on digital business strategy under higher industry turbulence, while having an increasingly convergent effect on digital business strategy under higher industry concentration and higher industry growth. The study uses archival data for 400 U.S.-based firms from 1999 to 2006. Our findings imply that digital business strategy is not solely a matter of optimizing firm operations internally or of responding to one or two focal competitors, but also arises strikingly from awareness and responsiveness to the digital business competitive environment. Collectively, the findings provide insights on how strategic posture and industry environment influence firms’ digital business strategy.",[],"Mitchell, Will",N/A,"Rotman School of Management, University of Toronto, Toronto, ON Canada"
https://misq.umn.edu/misq/article/37/2/537/108/Design-Capital-and-Design-Moves-The-Logic-of,MIS Quarterly,Design Capital and Design Moves: The Logic of Digital Business Strategy1,"Volume 37, Issue 2",June 2013,"As information technology becomes integral to the products and services in a growing range of industries, there has been a corresponding surge of interest in understanding how firms can effectively formulate and execute digital business strategies. This fusion of IT within the business environment gives rise to a strategic tension between investing in digital artifacts for long-term value creation and exploiting them for short-term value appropriation. Further, relentless innovation and competitive pressures dictate that firms continually adapt these artifacts to changing market and technological conditions, but sustained profitability requires scalable architectures that can serve a large customer base and stable interfaces that support integration across a diverse ecosystem of complementary offerings. The study of digital business strategy needs new concepts and methods to examine how these forces are managed in pursuit of competitive advantage. We conceptualize the logic of digital business strategy in terms of two constructs: design capital (i.e., the cumulative stock of designs owned or controlled by a firm) and design moves (i.e., the discrete strategic actions that enlarge, reduce, or modify a firm’s stock of designs). We also identify two salient dimensions of design capital, namely, option value and technical debt. Using embedded case studies of four firms, we develop a rich conceptual model and testable propositions to lay out a design-based logic of digital business strategy. This logic highlights the interplay between design moves and design capital in the context of digital business strategy and contributes to a growing body of insights that link the design of digital artifacts to competitive strategy and firm-level performance.",[],"Woodard, C. Jason",N/A,"School of Information Systems, Singapore Management University, Singapore 178902 Singapore"
https://misq.umn.edu/misq/article/37/2/537/108/Design-Capital-and-Design-Moves-The-Logic-of,MIS Quarterly,Design Capital and Design Moves: The Logic of Digital Business Strategy1,"Volume 37, Issue 2",June 2013,"As information technology becomes integral to the products and services in a growing range of industries, there has been a corresponding surge of interest in understanding how firms can effectively formulate and execute digital business strategies. This fusion of IT within the business environment gives rise to a strategic tension between investing in digital artifacts for long-term value creation and exploiting them for short-term value appropriation. Further, relentless innovation and competitive pressures dictate that firms continually adapt these artifacts to changing market and technological conditions, but sustained profitability requires scalable architectures that can serve a large customer base and stable interfaces that support integration across a diverse ecosystem of complementary offerings. The study of digital business strategy needs new concepts and methods to examine how these forces are managed in pursuit of competitive advantage. We conceptualize the logic of digital business strategy in terms of two constructs: design capital (i.e., the cumulative stock of designs owned or controlled by a firm) and design moves (i.e., the discrete strategic actions that enlarge, reduce, or modify a firm’s stock of designs). We also identify two salient dimensions of design capital, namely, option value and technical debt. Using embedded case studies of four firms, we develop a rich conceptual model and testable propositions to lay out a design-based logic of digital business strategy. This logic highlights the interplay between design moves and design capital in the context of digital business strategy and contributes to a growing body of insights that link the design of digital artifacts to competitive strategy and firm-level performance.",[],"Ramasubbu, Narayan",N/A,"Katz Graduate School of Business, University of Pittsburgh, Pittsburgh, PA 15260 U.S.A."
https://misq.umn.edu/misq/article/37/2/537/108/Design-Capital-and-Design-Moves-The-Logic-of,MIS Quarterly,Design Capital and Design Moves: The Logic of Digital Business Strategy1,"Volume 37, Issue 2",June 2013,"As information technology becomes integral to the products and services in a growing range of industries, there has been a corresponding surge of interest in understanding how firms can effectively formulate and execute digital business strategies. This fusion of IT within the business environment gives rise to a strategic tension between investing in digital artifacts for long-term value creation and exploiting them for short-term value appropriation. Further, relentless innovation and competitive pressures dictate that firms continually adapt these artifacts to changing market and technological conditions, but sustained profitability requires scalable architectures that can serve a large customer base and stable interfaces that support integration across a diverse ecosystem of complementary offerings. The study of digital business strategy needs new concepts and methods to examine how these forces are managed in pursuit of competitive advantage. We conceptualize the logic of digital business strategy in terms of two constructs: design capital (i.e., the cumulative stock of designs owned or controlled by a firm) and design moves (i.e., the discrete strategic actions that enlarge, reduce, or modify a firm’s stock of designs). We also identify two salient dimensions of design capital, namely, option value and technical debt. Using embedded case studies of four firms, we develop a rich conceptual model and testable propositions to lay out a design-based logic of digital business strategy. This logic highlights the interplay between design moves and design capital in the context of digital business strategy and contributes to a growing body of insights that link the design of digital artifacts to competitive strategy and firm-level performance.",[],"Tschang, F. Ted",N/A,"Lee Kong Chian School of Business, Singapore Management University, Singapore 178899 Singapore"
https://misq.umn.edu/misq/article/37/2/537/108/Design-Capital-and-Design-Moves-The-Logic-of,MIS Quarterly,Design Capital and Design Moves: The Logic of Digital Business Strategy1,"Volume 37, Issue 2",June 2013,"As information technology becomes integral to the products and services in a growing range of industries, there has been a corresponding surge of interest in understanding how firms can effectively formulate and execute digital business strategies. This fusion of IT within the business environment gives rise to a strategic tension between investing in digital artifacts for long-term value creation and exploiting them for short-term value appropriation. Further, relentless innovation and competitive pressures dictate that firms continually adapt these artifacts to changing market and technological conditions, but sustained profitability requires scalable architectures that can serve a large customer base and stable interfaces that support integration across a diverse ecosystem of complementary offerings. The study of digital business strategy needs new concepts and methods to examine how these forces are managed in pursuit of competitive advantage. We conceptualize the logic of digital business strategy in terms of two constructs: design capital (i.e., the cumulative stock of designs owned or controlled by a firm) and design moves (i.e., the discrete strategic actions that enlarge, reduce, or modify a firm’s stock of designs). We also identify two salient dimensions of design capital, namely, option value and technical debt. Using embedded case studies of four firms, we develop a rich conceptual model and testable propositions to lay out a design-based logic of digital business strategy. This logic highlights the interplay between design moves and design capital in the context of digital business strategy and contributes to a growing body of insights that link the design of digital artifacts to competitive strategy and firm-level performance.",[],"Sambamurthy, V.",N/A,"Eli Broad College of Business, Michigan State University, East Lansing, MI 48824 U.S.A."
https://misq.umn.edu/misq/article/37/2/565/109/Leveraging-Digital-Technologies-How-Information,MIS Quarterly,Leveraging Digital Technologies: How Information Quality Leads to Localized Capabilities and Customer Service Performance1,"Volume 37, Issue 2",June 2013,"With the growing recognition of the customer’s role in service creation and delivery, there is an increased impetus on building customer-centric organizations. Digital technologies play a key role in such organizations. Prior research studying digital business strategies has largely focused on building production-side competencies and there has been little focus on customer-side digital business strategies to leverage these technologies. We propose a theory to understand the effectiveness of a customer-side digital business strategy focused on localized dynamics—here, a firm’s customer service units (CSUs). Specifically, we use a capabilities perspective to propose digital design as an antecedent to two customer service capabilities—namely, customer orientation capability and customer response capability—across a firm’s CSUs. These two capabilities will help a firm to locally sense and respond to customer needs, respectively. Information quality from the digital design of the CSU is proposed as the antecedent to the two capabilities. Proposed capability-building dynamics are tested using data collected from multiple respondents across 170 branches of a large bank. Findings suggest that the impacts of information quality in capability-building are contingent on the local process characteristics. We offer implications for a firm’s customer-side digital business strategy and present new areas for future examination of such strategies.",[],"Setia, Pankaj",N/A,"Department of Information Systems, Walton College of Business, University of Arkansas, Fayetteville, AR 72701 U.S.A."
https://misq.umn.edu/misq/article/37/2/565/109/Leveraging-Digital-Technologies-How-Information,MIS Quarterly,Leveraging Digital Technologies: How Information Quality Leads to Localized Capabilities and Customer Service Performance1,"Volume 37, Issue 2",June 2013,"With the growing recognition of the customer’s role in service creation and delivery, there is an increased impetus on building customer-centric organizations. Digital technologies play a key role in such organizations. Prior research studying digital business strategies has largely focused on building production-side competencies and there has been little focus on customer-side digital business strategies to leverage these technologies. We propose a theory to understand the effectiveness of a customer-side digital business strategy focused on localized dynamics—here, a firm’s customer service units (CSUs). Specifically, we use a capabilities perspective to propose digital design as an antecedent to two customer service capabilities—namely, customer orientation capability and customer response capability—across a firm’s CSUs. These two capabilities will help a firm to locally sense and respond to customer needs, respectively. Information quality from the digital design of the CSU is proposed as the antecedent to the two capabilities. Proposed capability-building dynamics are tested using data collected from multiple respondents across 170 branches of a large bank. Findings suggest that the impacts of information quality in capability-building are contingent on the local process characteristics. We offer implications for a firm’s customer-side digital business strategy and present new areas for future examination of such strategies.",[],"Venkatesh, Viswanath",N/A,"Department of Information Systems, Walton College of Business, University of Arkansas, Fayetteville, AR 72701 U.S.A."
https://misq.umn.edu/misq/article/37/2/565/109/Leveraging-Digital-Technologies-How-Information,MIS Quarterly,Leveraging Digital Technologies: How Information Quality Leads to Localized Capabilities and Customer Service Performance1,"Volume 37, Issue 2",June 2013,"With the growing recognition of the customer’s role in service creation and delivery, there is an increased impetus on building customer-centric organizations. Digital technologies play a key role in such organizations. Prior research studying digital business strategies has largely focused on building production-side competencies and there has been little focus on customer-side digital business strategies to leverage these technologies. We propose a theory to understand the effectiveness of a customer-side digital business strategy focused on localized dynamics—here, a firm’s customer service units (CSUs). Specifically, we use a capabilities perspective to propose digital design as an antecedent to two customer service capabilities—namely, customer orientation capability and customer response capability—across a firm’s CSUs. These two capabilities will help a firm to locally sense and respond to customer needs, respectively. Information quality from the digital design of the CSU is proposed as the antecedent to the two capabilities. Proposed capability-building dynamics are tested using data collected from multiple respondents across 170 branches of a large bank. Findings suggest that the impacts of information quality in capability-building are contingent on the local process characteristics. We offer implications for a firm’s customer-side digital business strategy and present new areas for future examination of such strategies.",[],"Joglekar, Supreet",N/A,"Department of Information Systems, Walton College of Business, University of Arkansas, Fayetteville, AR 72701 U.S.A."
https://misq.umn.edu/misq/article/37/2/591/100/Content-or-Community-A-Digital-Business-Strategy,MIS Quarterly,Content or Community? A Digital Business Strategy for Content Providers in the Social Age1,"Volume 37, Issue 2",June 2013,"The content industry has been undergoing a tremendous transformation in the last two decades. We focus in this paper on recent changes in the form of social computing. Although the content industry has implemented social computing to a large extent, it has done so from a techno-centric approach in which social features are viewed as complementary rather than integral to content. This approach does not capitalize on users’ social behavior in the website and does not answer the content industry’s need to elicit payment from consumers. We suggest that both of these objectives can be achieved by acknowledging the fusion between content and community, making the social experience central to the content website’s digital business strategy.We use data from Last.fm, a site offering both music consumption and online community features. The basic use of Last.fm is free, and premium services are provided for a fixed monthly subscription fee. Although the premium services on Last.fm are aimed primarily at improving the content consumption experience, we find that willingness to pay for premium services is strongly associated with the level of community participation of the user.Drawing from the literature on levels of participation in online communities, we show that consumers’ willingness to pay increases as they climb the so-called “ladder of participation” on the website. Moreover, we find that willingness to pay is more strongly linked to community participation than to the volume of content consumption. We control for self-selection bias by using propensity score matching. We extend our results by estimating a hazard model to study the effect of community activity on the time between joining the website and the subscription decision. Our results suggest that firms whose digital business models remain viable in a world of “freemium” will be those that take a strategic rather than techno-centric view of social media, that integrate social media into the consumption and purchase experience rather than use it merely as a substitute for offline soft marketing. We provide new evidence of the importance of fusing social computing with content delivery and, in the process, lay a foundation for a broader strategic path for the digital content industry in an age of growing user participation.",[],"Oestreicher-Singer, Gal",N/A,"Recanati Graduate School of Business, Tel Aviv University, Tel Aviv 69978 Israel"
https://misq.umn.edu/misq/article/37/2/591/100/Content-or-Community-A-Digital-Business-Strategy,MIS Quarterly,Content or Community? A Digital Business Strategy for Content Providers in the Social Age1,"Volume 37, Issue 2",June 2013,"The content industry has been undergoing a tremendous transformation in the last two decades. We focus in this paper on recent changes in the form of social computing. Although the content industry has implemented social computing to a large extent, it has done so from a techno-centric approach in which social features are viewed as complementary rather than integral to content. This approach does not capitalize on users’ social behavior in the website and does not answer the content industry’s need to elicit payment from consumers. We suggest that both of these objectives can be achieved by acknowledging the fusion between content and community, making the social experience central to the content website’s digital business strategy.We use data from Last.fm, a site offering both music consumption and online community features. The basic use of Last.fm is free, and premium services are provided for a fixed monthly subscription fee. Although the premium services on Last.fm are aimed primarily at improving the content consumption experience, we find that willingness to pay for premium services is strongly associated with the level of community participation of the user.Drawing from the literature on levels of participation in online communities, we show that consumers’ willingness to pay increases as they climb the so-called “ladder of participation” on the website. Moreover, we find that willingness to pay is more strongly linked to community participation than to the volume of content consumption. We control for self-selection bias by using propensity score matching. We extend our results by estimating a hazard model to study the effect of community activity on the time between joining the website and the subscription decision. Our results suggest that firms whose digital business models remain viable in a world of “freemium” will be those that take a strategic rather than techno-centric view of social media, that integrate social media into the consumption and purchase experience rather than use it merely as a substitute for offline soft marketing. We provide new evidence of the importance of fusing social computing with content delivery and, in the process, lay a foundation for a broader strategic path for the digital content industry in an age of growing user participation.",[],"Zalmanson, Lior",N/A,"Recanati Graduate School of Business, Tel Aviv University, Tel Aviv 69978 Israel"
https://misq.umn.edu/misq/article/37/2/617/105/Digital-Business-Strategy-and-Value-Creation,MIS Quarterly,Digital Business Strategy and Value Creation: Framing the Dynamic Cycle of Control Points1,"Volume 37, Issue 2",June 2013,"Within changing value networks, the profits and competitive advantages of participation reside dynamically at control points that are the positions of greatest value and/or power. The enterprises that hold these positions have a great deal of control over how the network operates, how the benefits are redistributed, and how this influences the execution of a digital business strategy. This article is based on a field study that provides preliminary, yet promising, empirical evidence that sheds light on the dynamic cycle of value creation and value capture points in digitally enabled networks in response to triggers related to technology and business strategy. The context used is that of the European and U.S. broadcasting industry. Specifically, the paper illustrates how incremental innovations may shift value networks from static, vertically integrated networks to more loosely coupled networks, and how cross-boundary industry disruptions may then, in turn, shift those to two-sided markets. Based on the analysis, insights and implications for digital business strategy research and practice are then provided.",[],"Pagani, Margherita",N/A,"Marketing Department, Bocconi University, Via Roentgen, 1, 20136 Milan Italy"
https://misq.umn.edu/misq/article/37/2/633/110/Visions-and-Voices-on-Emerging-Challenges-in,MIS Quarterly,Visions and Voices on Emerging Challenges in Digital Business Strategy1,"Volume 37, Issue 2",June 2013,N/A,[],"Bharadwaj, Anandhi",N/A,"Goizueta Business School, Emory University, Atlanta, GA 30332 U.S.A"
https://misq.umn.edu/misq/article/37/2/633/110/Visions-and-Voices-on-Emerging-Challenges-in,MIS Quarterly,Visions and Voices on Emerging Challenges in Digital Business Strategy1,"Volume 37, Issue 2",June 2013,N/A,[],"El Sawy, Omar A.",N/A,"Marshall School of Business, University of Southern California Los Angeles, CA 90089-1421 U.S.A"
https://misq.umn.edu/misq/article/37/2/633/110/Visions-and-Voices-on-Emerging-Challenges-in,MIS Quarterly,Visions and Voices on Emerging Challenges in Digital Business Strategy1,"Volume 37, Issue 2",June 2013,N/A,[],"Pavlou, Paul A.",N/A,"Fox School of Business, Temple University, Philadelphia, PA 19122-6083 U.S.A"
https://misq.umn.edu/misq/article/37/2/633/110/Visions-and-Voices-on-Emerging-Challenges-in,MIS Quarterly,Visions and Voices on Emerging Challenges in Digital Business Strategy1,"Volume 37, Issue 2",June 2013,N/A,[],"Venkatraman, N.",N/A,"School of Management, Boston University Boston, MA 02215 U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/37/3/665/1507/Discovering-Unobserved-Heterogeneity-in-Structural,MIS Quarterly,Discovering Unobserved Heterogeneity in Structural Equation Models to Avert Validity Threats1,"Volume 37, Issue 3",September 2013,"A large proportion of information systems research is concerned with developing and testing models pertaining to complex cognition, behaviors, and outcomes of individuals, teams, organizations, and other social systems that are involved in the development, implementation, and utilization of information technology. Given the complexity of these social and behavioral phenomena, heterogeneity is likely to exist in the samples used in IS studies. While researchers now routinely address observed heterogeneity by introducing moderators, a priori groupings, and contextual factors in their research models, they have not examined how unobserved heterogeneity may affect their findings. We describe why unobserved heterogeneity threatens different types of validity and use simulations to demonstrate that unobserved heterogeneity biases parameter estimates, thereby leading to Type I and Type II errors. We also review different methods that can be used to uncover unobserved heterogeneity in structural equation models. While methods to uncover unobserved heterogeneity in covariance-based structural equation models (CB-SEM) are relatively advanced, the methods for partial least squares (PLS) path models are limited and have relied on an extension of mixture regression—finite mixture partial least squares (FIMIX-PLS) and distance measure-based methods—that have mismatches with some characteristics of PLS path modeling. We propose a new method—prediction-oriented segmentation (PLS-POS)—to overcome the limitations of FIMIX-PLS and other distance measure-based methods and conduct extensive simulations to evaluate the ability of PLS-POS and FIMIX-PLS to discover unobserved heterogeneity in both structural and measurement models. Our results show that both PLS-POS and FIMIX-PLS perform well in discovering unobserved heterogeneity in structural paths when the measures are reflective and that PLS-POS also performs well in discovering unobserved heterogeneity in formative measures. We propose an unobserved heterogeneity discovery (UHD) process that researchers can apply to (1) avert validity threats by uncovering unobserved heterogeneity and (2) elaborate on theory by turning unobserved heterogeneity into observed heterogeneity, thereby expanding theory through the integration of new moderator or contextual variables.",[],"Becker, Jan-Michael",ringle@tuhh.de,"Department of Marketing and Brand Management, University of Cologne, Cologne, 50923, Germany"
https://misq.umn.edu/misq/article/37/3/665/1507/Discovering-Unobserved-Heterogeneity-in-Structural,MIS Quarterly,Discovering Unobserved Heterogeneity in Structural Equation Models to Avert Validity Threats1,"Volume 37, Issue 3",September 2013,"A large proportion of information systems research is concerned with developing and testing models pertaining to complex cognition, behaviors, and outcomes of individuals, teams, organizations, and other social systems that are involved in the development, implementation, and utilization of information technology. Given the complexity of these social and behavioral phenomena, heterogeneity is likely to exist in the samples used in IS studies. While researchers now routinely address observed heterogeneity by introducing moderators, a priori groupings, and contextual factors in their research models, they have not examined how unobserved heterogeneity may affect their findings. We describe why unobserved heterogeneity threatens different types of validity and use simulations to demonstrate that unobserved heterogeneity biases parameter estimates, thereby leading to Type I and Type II errors. We also review different methods that can be used to uncover unobserved heterogeneity in structural equation models. While methods to uncover unobserved heterogeneity in covariance-based structural equation models (CB-SEM) are relatively advanced, the methods for partial least squares (PLS) path models are limited and have relied on an extension of mixture regression—finite mixture partial least squares (FIMIX-PLS) and distance measure-based methods—that have mismatches with some characteristics of PLS path modeling. We propose a new method—prediction-oriented segmentation (PLS-POS)—to overcome the limitations of FIMIX-PLS and other distance measure-based methods and conduct extensive simulations to evaluate the ability of PLS-POS and FIMIX-PLS to discover unobserved heterogeneity in both structural and measurement models. Our results show that both PLS-POS and FIMIX-PLS perform well in discovering unobserved heterogeneity in structural paths when the measures are reflective and that PLS-POS also performs well in discovering unobserved heterogeneity in formative measures. We propose an unobserved heterogeneity discovery (UHD) process that researchers can apply to (1) avert validity threats by uncovering unobserved heterogeneity and (2) elaborate on theory by turning unobserved heterogeneity into observed heterogeneity, thereby expanding theory through the integration of new moderator or contextual variables.",[],"Rai, Arun",N/A,"Center for Process Innovation and Department of Computer Information Systems, Robinson College of Business, Georgia State University, Atlanta, GA 30303 U.S.A."
https://misq.umn.edu/misq/article/37/3/665/1507/Discovering-Unobserved-Heterogeneity-in-Structural,MIS Quarterly,Discovering Unobserved Heterogeneity in Structural Equation Models to Avert Validity Threats1,"Volume 37, Issue 3",September 2013,"A large proportion of information systems research is concerned with developing and testing models pertaining to complex cognition, behaviors, and outcomes of individuals, teams, organizations, and other social systems that are involved in the development, implementation, and utilization of information technology. Given the complexity of these social and behavioral phenomena, heterogeneity is likely to exist in the samples used in IS studies. While researchers now routinely address observed heterogeneity by introducing moderators, a priori groupings, and contextual factors in their research models, they have not examined how unobserved heterogeneity may affect their findings. We describe why unobserved heterogeneity threatens different types of validity and use simulations to demonstrate that unobserved heterogeneity biases parameter estimates, thereby leading to Type I and Type II errors. We also review different methods that can be used to uncover unobserved heterogeneity in structural equation models. While methods to uncover unobserved heterogeneity in covariance-based structural equation models (CB-SEM) are relatively advanced, the methods for partial least squares (PLS) path models are limited and have relied on an extension of mixture regression—finite mixture partial least squares (FIMIX-PLS) and distance measure-based methods—that have mismatches with some characteristics of PLS path modeling. We propose a new method—prediction-oriented segmentation (PLS-POS)—to overcome the limitations of FIMIX-PLS and other distance measure-based methods and conduct extensive simulations to evaluate the ability of PLS-POS and FIMIX-PLS to discover unobserved heterogeneity in both structural and measurement models. Our results show that both PLS-POS and FIMIX-PLS perform well in discovering unobserved heterogeneity in structural paths when the measures are reflective and that PLS-POS also performs well in discovering unobserved heterogeneity in formative measures. We propose an unobserved heterogeneity discovery (UHD) process that researchers can apply to (1) avert validity threats by uncovering unobserved heterogeneity and (2) elaborate on theory by turning unobserved heterogeneity into observed heterogeneity, thereby expanding theory through the integration of new moderator or contextual variables.",[],"Ringle, Christian M.",N/A,"Institute for Human Resource Management and Organizations, Hamburg University of Technology (TUHH), Hamburg, 21073, Germany, and Faculty of Business and Law, University of Newcastle, Callaghan, NSW 2308 Australia"
https://misq.umn.edu/misq/article/37/3/665/1507/Discovering-Unobserved-Heterogeneity-in-Structural,MIS Quarterly,Discovering Unobserved Heterogeneity in Structural Equation Models to Avert Validity Threats1,"Volume 37, Issue 3",September 2013,"A large proportion of information systems research is concerned with developing and testing models pertaining to complex cognition, behaviors, and outcomes of individuals, teams, organizations, and other social systems that are involved in the development, implementation, and utilization of information technology. Given the complexity of these social and behavioral phenomena, heterogeneity is likely to exist in the samples used in IS studies. While researchers now routinely address observed heterogeneity by introducing moderators, a priori groupings, and contextual factors in their research models, they have not examined how unobserved heterogeneity may affect their findings. We describe why unobserved heterogeneity threatens different types of validity and use simulations to demonstrate that unobserved heterogeneity biases parameter estimates, thereby leading to Type I and Type II errors. We also review different methods that can be used to uncover unobserved heterogeneity in structural equation models. While methods to uncover unobserved heterogeneity in covariance-based structural equation models (CB-SEM) are relatively advanced, the methods for partial least squares (PLS) path models are limited and have relied on an extension of mixture regression—finite mixture partial least squares (FIMIX-PLS) and distance measure-based methods—that have mismatches with some characteristics of PLS path modeling. We propose a new method—prediction-oriented segmentation (PLS-POS)—to overcome the limitations of FIMIX-PLS and other distance measure-based methods and conduct extensive simulations to evaluate the ability of PLS-POS and FIMIX-PLS to discover unobserved heterogeneity in both structural and measurement models. Our results show that both PLS-POS and FIMIX-PLS perform well in discovering unobserved heterogeneity in structural paths when the measures are reflective and that PLS-POS also performs well in discovering unobserved heterogeneity in formative measures. We propose an unobserved heterogeneity discovery (UHD) process that researchers can apply to (1) avert validity threats by uncovering unobserved heterogeneity and (2) elaborate on theory by turning unobserved heterogeneity into observed heterogeneity, thereby expanding theory through the integration of new moderator or contextual variables.",[],"Völckner, Franziska",N/A,"Department of Marketing and Brand Management, University of Cologne, Cologne, 50923, Germany"
https://misq.umn.edu/misq/article/37/3/695/1518/Explaining-Employee-Job-Performance-The-Role-of,MIS Quarterly,Explaining Employee Job Performance: The Role of Online and Offline Workplace Communication Networks1,"Volume 37, Issue 3",September 2013,"By distinguishing between employees’ online and offline workplace communication networks, this paper incorporates technology into social network theory to understand employees’ job performance. Specifically, we conceptualize network ties as direct and indirect ties in both online and offline workplace communication networks, thus resulting in four distinct types of ties. We theorize that employees’ ties in online and offline workplace communication networks are complementary resources that interact to influence their job performance. We found support for our model in a field study among 104 employees in a large telecommunication company. The paper concludes with theoretical and practical implications.",[],"Zhang, Xiaojun",N/A,"School of Business and Management, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong"
https://misq.umn.edu/misq/article/37/3/695/1518/Explaining-Employee-Job-Performance-The-Role-of,MIS Quarterly,Explaining Employee Job Performance: The Role of Online and Offline Workplace Communication Networks1,"Volume 37, Issue 3",September 2013,"By distinguishing between employees’ online and offline workplace communication networks, this paper incorporates technology into social network theory to understand employees’ job performance. Specifically, we conceptualize network ties as direct and indirect ties in both online and offline workplace communication networks, thus resulting in four distinct types of ties. We theorize that employees’ ties in online and offline workplace communication networks are complementary resources that interact to influence their job performance. We found support for our model in a field study among 104 employees in a large telecommunication company. The paper concludes with theoretical and practical implications.",[],"Venkatesh, Viswanath",N/A,"Department of Information Systems, Walton College of Business, University of Arkansas, Fayetteville, AR 72701 U.S.A."
https://misq.umn.edu/misq/article/37/3/723/1524/A-Dramaturgical-Model-of-the-Production-of,MIS Quarterly,A Dramaturgical Model of the Production of Performance Data1,"Volume 37, Issue 3",September 2013,"The production of performance data in organizations is often described as a functional process that managers enforce on their employees to provide leaders with accurate information about employees’ work and their achievements. This study draws on a 15-month ethnography of a desk sales unit to build a dramaturgical model that explains how managers participate in the production of performance data to impress rather than inform leaders. Research on management information systems is reviewed to outline a protective specification of this model where managers participate in the production of performance data to suppress information that threatens the image they present to leaders. Ethnographic data about the production and use of performance records and performance reports in a desk sales unit is examined to induce an exploitive specification of this dramaturgical model. This specification explains how people can take advantage of the opportunities, rather than just avoid the threats that performance data presents for impression management. It also demonstrates how managers can participate in the production of performance data to create an idealized version of their accomplishments and that leaders reify these data by using them in their own attempts at impressing others. By doing so, leaders and managers turn information systems into store windows to show achievement upward instead of transparent windows to monitor compliance downward.",[],"Vieira da Cunha, João",N/A,"European University, Estrada da Correira 1500-210 Lisbon, Portugal, and Department of Business Administration, Aarhus University, Nordre Ringgade 1, DK-8000 Aarhus C, Denmark"
https://misq.umn.edu/misq/article/37/3/749/1517/When-Does-Technology-Use-Enable-Network-Change-in,MIS Quarterly,When Does Technology Use Enable Network Change in organizations? A comparative Study of Feature Use and Shared Affordances1,"Volume 37, Issue 3",September 2013,"The goal of this study is to augment explanations of how newly implemented technologies enable network change within organizations with an understanding of when such change is likely to happen. Drawing on the emerging literature on technology affordances, the paper suggests that informal network change within interdependent organizational groups is unlikely to occur until users converge on a shared appropriation of the new technology’s features such that the affordances the technology enables are jointly realized. In making the argument for the importance of shared affordances, this paper suggests that group-level network change has its most profound implications at the organization level when individuals use the same subset of a new information technology’s features. To explore this tentative theory, we turn to a comparative, multimethod, longitudinal study of computer-based simulation technology use in automotive engineering. The findings of this explanatory case study show that engineers used the new technology for more than three months, during which time neither group experienced changes to their advice networks. Initially, divergent uses of the technology’s features by engineers in both groups precluded them from being able to coordinate their work in ways that allowed them to structure their advice networks differently. Eventually, engineers in only one of the two groups converged on the use of a common set of the technology’s features to enact a shared affordance. This convergence was necessary to turn the technology into a resource that could collectively afford group members the ability to compare their simulation outputs with one another and, in so doing, alter the content and structure of the group’s advice network. The implications of these findings for the literatures on technology feature use, affordances, social networks, and post-adoption behaviors in organizations are discussed.",[],"Leonardi, Paul M.",N/A,"Department of Communication Studies and Department of Industrial Engineering and Management Sciences, Northwestern University, 2240 Campus Drive, Evanston, IL 60208 U.S.A."
https://misq.umn.edu/misq/article/37/3/777/1499/Integrating-Service-Quality-with-System-and,MIS Quarterly,Integrating Service Quality with System and Information Quality: An Empirical Test in the E-Service Context1,"Volume 37, Issue 3",September 2013,"Wixom and Todd (2005) integrated the user satisfaction and the technology acceptance literatures to theorize about and account for the influence of the information technology artifact on usage. Based on Wixom and Todd’s integrated model of technology usage, we propose the 3Q model by investigating the role of service quality (SQ), in addition to system quality (SysQ) and information quality (IQ), in website adoption. Attention to SQ is critical, as consumer websites have increasingly become the target of SQ assessment made by consumers, not just traditional SysQ and IQ evaluations. As part of our study, we further theorize and empirically test the relationships among these three types of quality constructs and hypothesize that perceived SysQ influences perceived IQ and perceived SQ, and perceived IQ influences perceived SQ. Our study extends the Wixom and Todd model in the e-service context and is the first of its kind to empirically examine the combined impact of perceived SQ, perceived SysQ, and perceived IQ on usage intention. Our study advances the theoretical understanding of SQ and the relationships among perceptions of SysQ, IQ, and SQ in the e-service context. The results also inform practitioners that high IQ and SysQ can directly or indirectly improve SQ in the e-service context.",[],"Xu, Jingjun (David)",N/A,"W. Frank Barton School of Business, Wichita State University, 1845 Fairmount Street, Wichita, KS 67260 U.S.A."
https://misq.umn.edu/misq/article/37/3/777/1499/Integrating-Service-Quality-with-System-and,MIS Quarterly,Integrating Service Quality with System and Information Quality: An Empirical Test in the E-Service Context1,"Volume 37, Issue 3",September 2013,"Wixom and Todd (2005) integrated the user satisfaction and the technology acceptance literatures to theorize about and account for the influence of the information technology artifact on usage. Based on Wixom and Todd’s integrated model of technology usage, we propose the 3Q model by investigating the role of service quality (SQ), in addition to system quality (SysQ) and information quality (IQ), in website adoption. Attention to SQ is critical, as consumer websites have increasingly become the target of SQ assessment made by consumers, not just traditional SysQ and IQ evaluations. As part of our study, we further theorize and empirically test the relationships among these three types of quality constructs and hypothesize that perceived SysQ influences perceived IQ and perceived SQ, and perceived IQ influences perceived SQ. Our study extends the Wixom and Todd model in the e-service context and is the first of its kind to empirically examine the combined impact of perceived SQ, perceived SysQ, and perceived IQ on usage intention. Our study advances the theoretical understanding of SQ and the relationships among perceptions of SysQ, IQ, and SQ in the e-service context. The results also inform practitioners that high IQ and SysQ can directly or indirectly improve SQ in the e-service context.",[],"Benbasat, Izak",N/A,"Sauder School of Business, The University of British Columbia, 2053 Main Mall, Vancouver, BC V6T 1Z2 Canada"
https://misq.umn.edu/misq/article/37/3/777/1499/Integrating-Service-Quality-with-System-and,MIS Quarterly,Integrating Service Quality with System and Information Quality: An Empirical Test in the E-Service Context1,"Volume 37, Issue 3",September 2013,"Wixom and Todd (2005) integrated the user satisfaction and the technology acceptance literatures to theorize about and account for the influence of the information technology artifact on usage. Based on Wixom and Todd’s integrated model of technology usage, we propose the 3Q model by investigating the role of service quality (SQ), in addition to system quality (SysQ) and information quality (IQ), in website adoption. Attention to SQ is critical, as consumer websites have increasingly become the target of SQ assessment made by consumers, not just traditional SysQ and IQ evaluations. As part of our study, we further theorize and empirically test the relationships among these three types of quality constructs and hypothesize that perceived SysQ influences perceived IQ and perceived SQ, and perceived IQ influences perceived SQ. Our study extends the Wixom and Todd model in the e-service context and is the first of its kind to empirically examine the combined impact of perceived SQ, perceived SysQ, and perceived IQ on usage intention. Our study advances the theoretical understanding of SQ and the relationships among perceptions of SysQ, IQ, and SQ in the e-service context. The results also inform practitioners that high IQ and SysQ can directly or indirectly improve SQ in the e-service context.",[],"Cenfetelli, Ronald T.",N/A,"Sauder School of Business, The University of British Columbia, 2053 Main Mall, Vancouver, BC V6T 1Z2 Canada"
https://misq.umn.edu/misq/article/37/3/795/1502/Critical-Realism-in-Information-Systems-Research,MIS Quarterly,Critical Realism in Information Systems Research,"Volume 37, Issue 3",September 2013,N/A,[],"Mingers, John",N/A,"Kent Business School, University of Kent, Canterbury, Kent, CT2 7NZ United Kingdom"
https://misq.umn.edu/misq/article/37/3/795/1502/Critical-Realism-in-Information-Systems-Research,MIS Quarterly,Critical Realism in Information Systems Research,"Volume 37, Issue 3",September 2013,N/A,[],"Mutch, Alistair",N/A,"Nottingham Business School, Nottingham Trent University, Burton Street, Nottingham NG1 4BU United Kingdom"
https://misq.umn.edu/misq/article/37/3/795/1502/Critical-Realism-in-Information-Systems-Research,MIS Quarterly,Critical Realism in Information Systems Research,"Volume 37, Issue 3",September 2013,N/A,[],"Willcocks, Leslie",N/A,"London School of Economics and Political Science, Houghton Street, London WC2A 2AE United Kingdom"
https://misq.umn.edu/misq/article/37/3/iii/1501/Editor-s-CommentsInformation-Systems-Research-and,MIS Quarterly,Editor’s CommentsInformation Systems Research and Behavioral Economics,"Volume 37, Issue 3",September 2013,N/A,[],"Goes, Paulo B.",N/A,"Eller College of Management, University of Arizona"
https://misq.umn.edu/misq/article/37/3/803/1506/Technological-Objects-Social-Positions-and-the,MIS Quarterly,"Technological Objects, Social Positions, and the Transformational Model of Social Activity1","Volume 37, Issue 3",September 2013,"The transformational model of social activity (TMSA), in many ways the centerpiece of critical realism, has been widely used in areas of information systems research. However, little has been done so far to develop a systematic theory of the nature, position, and identity of technological objects within the context of the TMSA. Our aim in this paper is to fill this gap, paying particular attention to the important category of nonmaterial technological objects that lie at the heart of modern information systems.",[],"Faulkner, Philip",N/A,"Clare College, University of Cambridge, Cambridge, CB2 1TL, United Kingdom"
https://misq.umn.edu/misq/article/37/3/803/1506/Technological-Objects-Social-Positions-and-the,MIS Quarterly,"Technological Objects, Social Positions, and the Transformational Model of Social Activity1","Volume 37, Issue 3",September 2013,"The transformational model of social activity (TMSA), in many ways the centerpiece of critical realism, has been widely used in areas of information systems research. However, little has been done so far to develop a systematic theory of the nature, position, and identity of technological objects within the context of the TMSA. Our aim in this paper is to fill this gap, paying particular attention to the important category of nonmaterial technological objects that lie at the heart of modern information systems.",[],"Runde, Jochen",N/A,"Judge Business School and Girton College, University of Cambridge, Cambridge, CB2 1AG, United Kingdom"
https://misq.umn.edu/misq/article/37/3/819/1509/Critical-Realism-and-Affordances-Theorizing-IT,MIS Quarterly,Critical Realism and Affordances: Theorizing IT-Associated Organizational Change Processes1,"Volume 37, Issue 3",September 2013,"Convincing arguments for using critical realism as an underpinning for theories of IT-associated organizational change have appeared in the Information Systems literature. A central task in developing such theories is to uncover the generative mechanisms by which IT is implicated in organizational change processes, but to do so, we must explain how critical realism’s concept of generative mechanisms applies in an IS context. Similarly, convincing arguments have been made for using Gibson’s (1986) affordance theory from ecological psychology for developing theories of IT-associated organizational change, but this effort has been hampered due to insufficient attention to the ontological status of affordances. In this paper, we argue that affordances are the generative mechanisms we need to specify and explain how affordances are a specific type of generative mechanism. We use the core principles of critical realism to argue how affordances arise in the real domain from the relation between the complex assemblages of organizations and of IT artifacts, how affordances are actualized over time by organizational actors, and how these actualizations lead to the various effects we observe in the empirical domain. After presenting these arguments, we reanalyze two published cases in the literature, those of ACRO and Autoworks, to illustrate how affordance-based theories informed by critical realism enhance our ability to explain IT-associated organizational change. These examples show how researchers using this approach should proceed, and how managers can use these ideas to diagnose and address IT implementation problems.",[],"Volkoff, Olga",N/A,"Beedie School of Business, Simon Fraser University, Burnaby, BC V5A 1S6 Canada"
https://misq.umn.edu/misq/article/37/3/819/1509/Critical-Realism-and-Affordances-Theorizing-IT,MIS Quarterly,Critical Realism and Affordances: Theorizing IT-Associated Organizational Change Processes1,"Volume 37, Issue 3",September 2013,"Convincing arguments for using critical realism as an underpinning for theories of IT-associated organizational change have appeared in the Information Systems literature. A central task in developing such theories is to uncover the generative mechanisms by which IT is implicated in organizational change processes, but to do so, we must explain how critical realism’s concept of generative mechanisms applies in an IS context. Similarly, convincing arguments have been made for using Gibson’s (1986) affordance theory from ecological psychology for developing theories of IT-associated organizational change, but this effort has been hampered due to insufficient attention to the ontological status of affordances. In this paper, we argue that affordances are the generative mechanisms we need to specify and explain how affordances are a specific type of generative mechanism. We use the core principles of critical realism to argue how affordances arise in the real domain from the relation between the complex assemblages of organizations and of IT artifacts, how affordances are actualized over time by organizational actors, and how these actualizations lead to the various effects we observe in the empirical domain. After presenting these arguments, we reanalyze two published cases in the literature, those of ACRO and Autoworks, to illustrate how affordance-based theories informed by critical realism enhance our ability to explain IT-associated organizational change. These examples show how researchers using this approach should proceed, and how managers can use these ideas to diagnose and address IT implementation problems.",[],"Strong, Diane M.",N/A,"School of Business, Worcester Polytechnic Institute, Worcester, MA 01609 U.S.A."
https://misq.umn.edu/misq/article/37/3/835/1513/How-Should-Technology-Mediated-Organizational,MIS Quarterly,How Should Technology-Mediated Organizational Change Be Explained? A Comparison of the Contributions of Critical Realism and Activity theory1,"Volume 37, Issue 3",September 2013,"In this paper, critical realism and activity theory are compared within the context of theorizing technology-mediated organizational change. An activity theoretic analysis of the implementation of large-scale disruptive information systems in a public sector setting (in particular concerning paramedic treatment of heart attack patients and ambulance dispatch work activity) is used to illustrate how activity theory makes a significant contribution to critical realism, by (1) locating technology within “activity systems” and theorizing change through contradictions and congruencies within those systems; (2) developing recent critical realism-inspired theorization of the “inscription” of cultural and social relations within technology; and (3) developing recent insights of critical realist researchers regarding the way in which the performance management agenda is mediated through IS.",[],"Allen, David K.",N/A,"AIMTech Research Group, Leeds University Business School, Leeds University, Leeds United Kingdom"
https://misq.umn.edu/misq/article/37/3/835/1513/How-Should-Technology-Mediated-Organizational,MIS Quarterly,How Should Technology-Mediated Organizational Change Be Explained? A Comparison of the Contributions of Critical Realism and Activity theory1,"Volume 37, Issue 3",September 2013,"In this paper, critical realism and activity theory are compared within the context of theorizing technology-mediated organizational change. An activity theoretic analysis of the implementation of large-scale disruptive information systems in a public sector setting (in particular concerning paramedic treatment of heart attack patients and ambulance dispatch work activity) is used to illustrate how activity theory makes a significant contribution to critical realism, by (1) locating technology within “activity systems” and theorizing change through contradictions and congruencies within those systems; (2) developing recent critical realism-inspired theorization of the “inscription” of cultural and social relations within technology; and (3) developing recent insights of critical realist researchers regarding the way in which the performance management agenda is mediated through IS.",[],"Brown, Andrew",N/A,"Centre for Employment Relations Innovation and Change, Leeds University Business School, Leeds University, Leeds United Kingdom"
https://misq.umn.edu/misq/article/37/3/835/1513/How-Should-Technology-Mediated-Organizational,MIS Quarterly,How Should Technology-Mediated Organizational Change Be Explained? A Comparison of the Contributions of Critical Realism and Activity theory1,"Volume 37, Issue 3",September 2013,"In this paper, critical realism and activity theory are compared within the context of theorizing technology-mediated organizational change. An activity theoretic analysis of the implementation of large-scale disruptive information systems in a public sector setting (in particular concerning paramedic treatment of heart attack patients and ambulance dispatch work activity) is used to illustrate how activity theory makes a significant contribution to critical realism, by (1) locating technology within “activity systems” and theorizing change through contradictions and congruencies within those systems; (2) developing recent critical realism-inspired theorization of the “inscription” of cultural and social relations within technology; and (3) developing recent insights of critical realist researchers regarding the way in which the performance management agenda is mediated through IS.",[],"Karanasios, Stan",N/A,"AIMTech Research Group, Leeds University Business School, Leeds University, Leeds United Kingdom"
https://misq.umn.edu/misq/article/37/3/835/1513/How-Should-Technology-Mediated-Organizational,MIS Quarterly,How Should Technology-Mediated Organizational Change Be Explained? A Comparison of the Contributions of Critical Realism and Activity theory1,"Volume 37, Issue 3",September 2013,"In this paper, critical realism and activity theory are compared within the context of theorizing technology-mediated organizational change. An activity theoretic analysis of the implementation of large-scale disruptive information systems in a public sector setting (in particular concerning paramedic treatment of heart attack patients and ambulance dispatch work activity) is used to illustrate how activity theory makes a significant contribution to critical realism, by (1) locating technology within “activity systems” and theorizing change through contradictions and congruencies within those systems; (2) developing recent critical realism-inspired theorization of the “inscription” of cultural and social relations within technology; and (3) developing recent insights of critical realist researchers regarding the way in which the performance management agenda is mediated through IS.",[],"Norman, Alistair",N/A,"AIMTech Research Group, Leeds University Business School, Leeds University, Leeds United Kingdom"
https://misq.umn.edu/misq/article/37/3/855/1511/Methodological-Implications-of-Critical-Realism,MIS Quarterly,Methodological Implications of Critical Realism for Mixed-Methods Research1,"Volume 37, Issue 3",September 2013,"Building on recent developments in mixed methods, we discuss the methodological implications of critical realism and explore how these can guide dynamic mixed-methods research design in information systems. Specifically, we examine the core ontological assumptions of CR in order to gain some perspective on key epistemological issues such as causation and validity, and illustrate how these shape our logic of inference in the research process through what is known as retroduction. We demonstrate the value of a CR-led mixed-methods research approach by drawing on a study that examines the impact of ICT adoption in the financial services sector. In doing so, we provide insight into the interplay between qualitative and quantitative methods and the particular value of applying mixed methods guided by CR methodological principles. Our positioning of demi-regularities within the process of retroduction contributes a distinctive development in this regard. We argue that such a research design enables us to better address issues of validity and the development of more robust meta-inferences.",[],"Zachariadis, Markos",N/A,"Warwick Business School, The University of Warwick, Coventry CV4 7AL, United Kingdom"
https://misq.umn.edu/misq/article/37/3/855/1511/Methodological-Implications-of-Critical-Realism,MIS Quarterly,Methodological Implications of Critical Realism for Mixed-Methods Research1,"Volume 37, Issue 3",September 2013,"Building on recent developments in mixed methods, we discuss the methodological implications of critical realism and explore how these can guide dynamic mixed-methods research design in information systems. Specifically, we examine the core ontological assumptions of CR in order to gain some perspective on key epistemological issues such as causation and validity, and illustrate how these shape our logic of inference in the research process through what is known as retroduction. We demonstrate the value of a CR-led mixed-methods research approach by drawing on a study that examines the impact of ICT adoption in the financial services sector. In doing so, we provide insight into the interplay between qualitative and quantitative methods and the particular value of applying mixed methods guided by CR methodological principles. Our positioning of demi-regularities within the process of retroduction contributes a distinctive development in this regard. We argue that such a research design enables us to better address issues of validity and the development of more robust meta-inferences.",[],"Scott, Susan",N/A,"Information Systems & Innovation Group, Department of Management, London School of Economics and Political Science, Houghton Street, London WC2A 2AE United Kingdom"
https://misq.umn.edu/misq/article/37/3/855/1511/Methodological-Implications-of-Critical-Realism,MIS Quarterly,Methodological Implications of Critical Realism for Mixed-Methods Research1,"Volume 37, Issue 3",September 2013,"Building on recent developments in mixed methods, we discuss the methodological implications of critical realism and explore how these can guide dynamic mixed-methods research design in information systems. Specifically, we examine the core ontological assumptions of CR in order to gain some perspective on key epistemological issues such as causation and validity, and illustrate how these shape our logic of inference in the research process through what is known as retroduction. We demonstrate the value of a CR-led mixed-methods research approach by drawing on a study that examines the impact of ICT adoption in the financial services sector. In doing so, we provide insight into the interplay between qualitative and quantitative methods and the particular value of applying mixed methods guided by CR methodological principles. Our positioning of demi-regularities within the process of retroduction contributes a distinctive development in this regard. We argue that such a research design enables us to better address issues of validity and the development of more robust meta-inferences.",[],"Barrett, Michael",N/A,"Judge Business School, University of Cambridge, Trumpington Street, Cambridge CB2 1AG United Kingdom"
https://misq.umn.edu/misq/article/37/3/881/1522/The-Broader-Context-for-ICT4D-Projects-A,MIS Quarterly,The Broader Context for ICT4D Projects: A Morphogenetic Analysis1,"Volume 37, Issue 3",September 2013,"This paper demonstrates the value of Archer’s morphogenetic approach (MA) in understanding and explaining the complexity of the broader context within which many developing country information and communication technology (ICT) projects are implemented. It does this by using MA’s analytical and explanatory apparatus to examine the evolution of the context of public sector ICT provision in Kenya over the period 1963–2006. In addition to demonstrating the practical value of MA, the paper contributes to the Information Systems literature on ICT for development (ICT4D). The analysis identifies (1) global normative pressures, polity, the national socio-economic base, disruptive technology, and the emergence of multistakeholder networks as key forces in shaping the evolutionary trajectory, (2) the explicit treatment of time and temporality as key for understanding mechanisms underpinning the evolutionary process, and (3) the difficulty of cleanly isolating the implementation of individual public sector ICT projects from the broader context and ICT4D agendas. The discussion elaborates on the features of MA found to be particularly valuable in this study. The paper concludes that explicitly attending to time and temporality, and to the broader context for ICT4D projects, would contribute to the development of more nuanced accounts of such projects and a more emancipatory outlook for ICT4D research.",[],"Njihia, James Muranga",njihia@uonbi.ac.ke,"Department of Management Science, School of Business, University of Nairobi, PO Box 30197, GPO Nairobi 00100 Kenya ,"
https://misq.umn.edu/misq/article/37/3/881/1522/The-Broader-Context-for-ICT4D-Projects-A,MIS Quarterly,The Broader Context for ICT4D Projects: A Morphogenetic Analysis1,"Volume 37, Issue 3",September 2013,"This paper demonstrates the value of Archer’s morphogenetic approach (MA) in understanding and explaining the complexity of the broader context within which many developing country information and communication technology (ICT) projects are implemented. It does this by using MA’s analytical and explanatory apparatus to examine the evolution of the context of public sector ICT provision in Kenya over the period 1963–2006. In addition to demonstrating the practical value of MA, the paper contributes to the Information Systems literature on ICT for development (ICT4D). The analysis identifies (1) global normative pressures, polity, the national socio-economic base, disruptive technology, and the emergence of multistakeholder networks as key forces in shaping the evolutionary trajectory, (2) the explicit treatment of time and temporality as key for understanding mechanisms underpinning the evolutionary process, and (3) the difficulty of cleanly isolating the implementation of individual public sector ICT projects from the broader context and ICT4D agendas. The discussion elaborates on the features of MA found to be particularly valuable in this study. The paper concludes that explicitly attending to time and temporality, and to the broader context for ICT4D projects, would contribute to the development of more nuanced accounts of such projects and a more emancipatory outlook for ICT4D research.",[],"Merali, Yasmin",N/A,"Warwick Business School, The University of Warwick, Coventry CV4 7AL United Kingdom"
https://misq.umn.edu/misq/article/37/3/907/1504/The-Generative-Mechanisms-of-Digital,MIS Quarterly,The Generative Mechanisms of Digital Infrastructure Evolution1,"Volume 37, Issue 3",September 2013,"The current literature on digital infrastructure offers powerful lenses for conceptualizing the increasingly interconnected information system collectives found in contemporary organizations. However, little attention has been paid to the generative mechanisms of digital infrastructure, that is, the causal powers that explain how and why such infrastructure evolves over time. This is unfortunate, since more knowledge about what drives digital infrastructures would be highly valuable for managers and IT professionals confronted by the complexity of managing them. To this end, this paper adopts a critical realist view for developing a configurational perspective of infrastructure evolution. Our theorizing draws on a multimethod research design comprising an in-depth case study and a case survey. The in-depth case study, conducted at a Scandinavian airline, distinguishes three key mechanisms of digital infrastructure evolution: adoption, innovation, and scaling. The case survey research of 41 cases of digital infrastructure then identifies and analyzes causal paths through which configurations of these mechanisms lead to successful evolution outcomes. The study reported in this paper contributes to the infrastructure literature in two ways. First, we identify three generative mechanisms of digital infrastructure and how they contingently lead to evolution outcomes. Second, we use these mechanisms as a basis for developing a configurational perspective that advances current knowledge about why some digital infrastructures evolve successfully while others do not. In addition, the paper demonstrates and discusses the efficacy of critical realism as a philosophical tradition for developing substantive contributions in the field of information systems.",[],"Henfridsson, Ola",N/A,"Warwick Business School, The University of Warwick, Coventry CV4 7AL United Kingdom"
https://misq.umn.edu/misq/article/37/3/907/1504/The-Generative-Mechanisms-of-Digital,MIS Quarterly,The Generative Mechanisms of Digital Infrastructure Evolution1,"Volume 37, Issue 3",September 2013,"The current literature on digital infrastructure offers powerful lenses for conceptualizing the increasingly interconnected information system collectives found in contemporary organizations. However, little attention has been paid to the generative mechanisms of digital infrastructure, that is, the causal powers that explain how and why such infrastructure evolves over time. This is unfortunate, since more knowledge about what drives digital infrastructures would be highly valuable for managers and IT professionals confronted by the complexity of managing them. To this end, this paper adopts a critical realist view for developing a configurational perspective of infrastructure evolution. Our theorizing draws on a multimethod research design comprising an in-depth case study and a case survey. The in-depth case study, conducted at a Scandinavian airline, distinguishes three key mechanisms of digital infrastructure evolution: adoption, innovation, and scaling. The case survey research of 41 cases of digital infrastructure then identifies and analyzes causal paths through which configurations of these mechanisms lead to successful evolution outcomes. The study reported in this paper contributes to the infrastructure literature in two ways. First, we identify three generative mechanisms of digital infrastructure and how they contingently lead to evolution outcomes. Second, we use these mechanisms as a basis for developing a configurational perspective that advances current knowledge about why some digital infrastructures evolve successfully while others do not. In addition, the paper demonstrates and discusses the efficacy of critical realism as a philosophical tradition for developing substantive contributions in the field of information systems.",[],"Bygstad, Bendik",N/A,"Norwegian School of IT, Schweigaards gt. 14, 0185 Oslo NORWAY and Department of Informatics, University of Oslo, Oslo Norway"
https://misq.umn.edu/misq/article/37/3/933/1520/Causal-Explanation-in-the-Coordinating-Process-A,MIS Quarterly,Causal Explanation in the Coordinating Process: A Critical Realist Case Study of Federated IT Governance Structures1,"Volume 37, Issue 3",September 2013,"Large, multi-unit organizations are continually challenged to balance demands for centralization of information technology that lead to cost and service efficiencies through standardization while providing flexibility at the local unit level in order to meet unique business, customer, and service needs. This has led many organizations to adopt hybrid federated information technology governance (ITG) structures to find this balance. This approach to ITG establishes demand for various means to coordinate effectively across the organization to achieve the desired benefits. Past research has focused on the efficacy of various coordination mechanisms (e.g., steering committees, task forces) to coordinate activities related to information technology. However, we lack insights as to how and why these various coordination approaches help organizations achieve desired coordinated outcomes. This research specifically identifies coordinating as a process. Adopting the philosophy of critical realism, we conducted a longitudinal, comparative case study of two coordinating efforts in a federated ITG structure. Through a multifaceted approach to scientific logic employing deductive, inductive, and retroductive elements, we explicate two causal mechanisms, consensus making and unit aligning, which help to explain the coordinating process and the coordination outcomes observed in these efforts. We additionally elaborate the operation of the mechanisms through the typology of macro–micro–macro influences. Further, we demonstrate the value of the causal mechanisms to understanding the coordinating process by highlighting the complementarity in insights relative to the theories of power and politics and of rational choice. The study contributes to our understanding of coordinating as a process and of governance in federated IT organizations. Importantly, our study illustrates the value of applying critical realism to develop causal explanations and generate insights about a phenomenon.",[],"Williams, Clay K.",N/A,"Computer Management and Information Systems Department, School of Business, Southern Illinois University Edwardsville, Edwardsville, IL 62026 U.S.A."
https://misq.umn.edu/misq/article/37/3/933/1520/Causal-Explanation-in-the-Coordinating-Process-A,MIS Quarterly,Causal Explanation in the Coordinating Process: A Critical Realist Case Study of Federated IT Governance Structures1,"Volume 37, Issue 3",September 2013,"Large, multi-unit organizations are continually challenged to balance demands for centralization of information technology that lead to cost and service efficiencies through standardization while providing flexibility at the local unit level in order to meet unique business, customer, and service needs. This has led many organizations to adopt hybrid federated information technology governance (ITG) structures to find this balance. This approach to ITG establishes demand for various means to coordinate effectively across the organization to achieve the desired benefits. Past research has focused on the efficacy of various coordination mechanisms (e.g., steering committees, task forces) to coordinate activities related to information technology. However, we lack insights as to how and why these various coordination approaches help organizations achieve desired coordinated outcomes. This research specifically identifies coordinating as a process. Adopting the philosophy of critical realism, we conducted a longitudinal, comparative case study of two coordinating efforts in a federated ITG structure. Through a multifaceted approach to scientific logic employing deductive, inductive, and retroductive elements, we explicate two causal mechanisms, consensus making and unit aligning, which help to explain the coordinating process and the coordination outcomes observed in these efforts. We additionally elaborate the operation of the mechanisms through the typology of macro–micro–macro influences. Further, we demonstrate the value of the causal mechanisms to understanding the coordinating process by highlighting the complementarity in insights relative to the theories of power and politics and of rational choice. The study contributes to our understanding of coordinating as a process and of governance in federated IT organizations. Importantly, our study illustrates the value of applying critical realism to develop causal explanations and generate insights about a phenomenon.",[],"Karahanna, Elena",N/A,"Management Information Systems Department, Terry College of Business, University of Georgia, Athens, GA 30602 U.S.A."
https://misq.umn.edu/misq/article/37/3/965/1515/Explaining-Broadband-Adoption-in-Rural-Australia,MIS Quarterly,Explaining Broadband Adoption in Rural Australia: Modes of Reflexivity and the Morphogenetic Approach1,"Volume 37, Issue 3",September 2013,"Universal fast broadband is currently being implemented by the Australian government. It is the largest single project in Australia’s history. Represented as a nation-building exercise by the government and many public and private promoters, it is vilified by others as a massive waste of taxpayers’ money. Ultimately the target of successful universal availability will require that metropolitan installations subsidize rural adoption. The take-up of these facilities, particularly in regional and remote areas, constitutes a complex, multi-factorial scenario in which political, personal, and organizational decisions are shaped by physical, cultural, economic, and ideological elements. Critical realism is proposed here as an aid for examining the complex reality of rural adoption for communities and small businesses in the regions. This article highlights the importance of considering individual reflexivity in explaining the adoption decision and potential adoption barriers.",[],"Dobson, Philip",N/A,"Centre for Innovative Practice, School of Management, Edith Cowan University, 270 Joondalup Drive, Perth, Western Australia 6027 Australia"
https://misq.umn.edu/misq/article/37/3/965/1515/Explaining-Broadband-Adoption-in-Rural-Australia,MIS Quarterly,Explaining Broadband Adoption in Rural Australia: Modes of Reflexivity and the Morphogenetic Approach1,"Volume 37, Issue 3",September 2013,"Universal fast broadband is currently being implemented by the Australian government. It is the largest single project in Australia’s history. Represented as a nation-building exercise by the government and many public and private promoters, it is vilified by others as a massive waste of taxpayers’ money. Ultimately the target of successful universal availability will require that metropolitan installations subsidize rural adoption. The take-up of these facilities, particularly in regional and remote areas, constitutes a complex, multi-factorial scenario in which political, personal, and organizational decisions are shaped by physical, cultural, economic, and ideological elements. Critical realism is proposed here as an aid for examining the complex reality of rural adoption for communities and small businesses in the regions. This article highlights the importance of considering individual reflexivity in explaining the adoption decision and potential adoption barriers.",[],"Jackson, Paul",N/A,"Centre for Innovative Practice, School of Management, Edith Cowan University, 270 Joondalup Drive, Perth, Western Australia 6027 Australia"
https://misq.umn.edu/misq/article/37/3/965/1515/Explaining-Broadband-Adoption-in-Rural-Australia,MIS Quarterly,Explaining Broadband Adoption in Rural Australia: Modes of Reflexivity and the Morphogenetic Approach1,"Volume 37, Issue 3",September 2013,"Universal fast broadband is currently being implemented by the Australian government. It is the largest single project in Australia’s history. Represented as a nation-building exercise by the government and many public and private promoters, it is vilified by others as a massive waste of taxpayers’ money. Ultimately the target of successful universal availability will require that metropolitan installations subsidize rural adoption. The take-up of these facilities, particularly in regional and remote areas, constitutes a complex, multi-factorial scenario in which political, personal, and organizational decisions are shaped by physical, cultural, economic, and ideological elements. Critical realism is proposed here as an aid for examining the complex reality of rural adoption for communities and small businesses in the regions. This article highlights the importance of considering individual reflexivity in explaining the adoption decision and potential adoption barriers.",[],"Gengatharen, Denise",N/A,"Centre for Innovative Practice, School of Management, Edith Cowan University, 270 Joondalup Drive, Perth, Western Australia 6027 Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/37/4/993/575/Evaluating-Journal-Quality-and-the-Association-for,MIS Quarterly,Evaluating Journal Quality and the Association for Information Systems Senior Scholars’ Journal Basket Via Bibliometric Measures: Do Expert Journal Assessments Add Value?1,"Volume 37, Issue 4",December 2013,"Information systems journal rankings and ratings help scholars focus their publishing efforts and are widely used surrogates for judging the quality of research. Over the years, numerous approaches have been used to rank IS journals, approaches such as citation metrics, school lists, acceptance rates, and expert assessments. However, the results of these approaches often conflict due to a host of validity concerns. In the current scientometric study, we make significant strides toward correcting for these limitations in the ranking of mainstream IS journals. We compare expert rankings to bibliometric measures such as the ISI Impact Factor™, the h-index, and social network analysis metrics. Among other findings, we conclude that bibliometric measures provide very similar results to expert-based methods in determining a tiered structure of IS journals, thereby suggesting that bibliometrics can be a complete, less expensive, and more efficient substitute for expert assessment. We also find strong support for seven of the eight journals in the Association for Information Systems Senior Scholars’ “basket” of journals. A cluster analysis of our results indicates a two-tiered separation in the quality of the highest quality IS journals—with MIS Quarterly, Information Systems Research, and Journal of Management Information Systems belonging, in that order, to the highest A+ tier. Journal quality metrics fit nicely into the sociology of science literature and can be useful in models that attempt to explain how knowledge disseminates through scientific communities.",[],"Lowry, Paul Benjamin",N/A,"Department of Information Systems, College of Business, City, University of Hong Kong, Hong Kong, China"
https://misq.umn.edu/misq/article/37/4/993/575/Evaluating-Journal-Quality-and-the-Association-for,MIS Quarterly,Evaluating Journal Quality and the Association for Information Systems Senior Scholars’ Journal Basket Via Bibliometric Measures: Do Expert Journal Assessments Add Value?1,"Volume 37, Issue 4",December 2013,"Information systems journal rankings and ratings help scholars focus their publishing efforts and are widely used surrogates for judging the quality of research. Over the years, numerous approaches have been used to rank IS journals, approaches such as citation metrics, school lists, acceptance rates, and expert assessments. However, the results of these approaches often conflict due to a host of validity concerns. In the current scientometric study, we make significant strides toward correcting for these limitations in the ranking of mainstream IS journals. We compare expert rankings to bibliometric measures such as the ISI Impact Factor™, the h-index, and social network analysis metrics. Among other findings, we conclude that bibliometric measures provide very similar results to expert-based methods in determining a tiered structure of IS journals, thereby suggesting that bibliometrics can be a complete, less expensive, and more efficient substitute for expert assessment. We also find strong support for seven of the eight journals in the Association for Information Systems Senior Scholars’ “basket” of journals. A cluster analysis of our results indicates a two-tiered separation in the quality of the highest quality IS journals—with MIS Quarterly, Information Systems Research, and Journal of Management Information Systems belonging, in that order, to the highest A+ tier. Journal quality metrics fit nicely into the sociology of science literature and can be useful in models that attempt to explain how knowledge disseminates through scientific communities.",[],"Moody, Gregory D.",N/A,"Department of Management, Entrepreneurship and Technology, Lee Business School, University of Nevada, Las Vegas, Las Vegas, NV 89143 U.S.A."
https://misq.umn.edu/misq/article/37/4/993/575/Evaluating-Journal-Quality-and-the-Association-for,MIS Quarterly,Evaluating Journal Quality and the Association for Information Systems Senior Scholars’ Journal Basket Via Bibliometric Measures: Do Expert Journal Assessments Add Value?1,"Volume 37, Issue 4",December 2013,"Information systems journal rankings and ratings help scholars focus their publishing efforts and are widely used surrogates for judging the quality of research. Over the years, numerous approaches have been used to rank IS journals, approaches such as citation metrics, school lists, acceptance rates, and expert assessments. However, the results of these approaches often conflict due to a host of validity concerns. In the current scientometric study, we make significant strides toward correcting for these limitations in the ranking of mainstream IS journals. We compare expert rankings to bibliometric measures such as the ISI Impact Factor™, the h-index, and social network analysis metrics. Among other findings, we conclude that bibliometric measures provide very similar results to expert-based methods in determining a tiered structure of IS journals, thereby suggesting that bibliometrics can be a complete, less expensive, and more efficient substitute for expert assessment. We also find strong support for seven of the eight journals in the Association for Information Systems Senior Scholars’ “basket” of journals. A cluster analysis of our results indicates a two-tiered separation in the quality of the highest quality IS journals—with MIS Quarterly, Information Systems Research, and Journal of Management Information Systems belonging, in that order, to the highest A+ tier. Journal quality metrics fit nicely into the sociology of science literature and can be useful in models that attempt to explain how knowledge disseminates through scientific communities.",[],"Gaskin, James",N/A,"Information Systems Department, Marriott School of Management, Brigham Young University, Provo, UT 84602 U.S.A."
https://misq.umn.edu/misq/article/37/4/993/575/Evaluating-Journal-Quality-and-the-Association-for,MIS Quarterly,Evaluating Journal Quality and the Association for Information Systems Senior Scholars’ Journal Basket Via Bibliometric Measures: Do Expert Journal Assessments Add Value?1,"Volume 37, Issue 4",December 2013,"Information systems journal rankings and ratings help scholars focus their publishing efforts and are widely used surrogates for judging the quality of research. Over the years, numerous approaches have been used to rank IS journals, approaches such as citation metrics, school lists, acceptance rates, and expert assessments. However, the results of these approaches often conflict due to a host of validity concerns. In the current scientometric study, we make significant strides toward correcting for these limitations in the ranking of mainstream IS journals. We compare expert rankings to bibliometric measures such as the ISI Impact Factor™, the h-index, and social network analysis metrics. Among other findings, we conclude that bibliometric measures provide very similar results to expert-based methods in determining a tiered structure of IS journals, thereby suggesting that bibliometrics can be a complete, less expensive, and more efficient substitute for expert assessment. We also find strong support for seven of the eight journals in the Association for Information Systems Senior Scholars’ “basket” of journals. A cluster analysis of our results indicates a two-tiered separation in the quality of the highest quality IS journals—with MIS Quarterly, Information Systems Research, and Journal of Management Information Systems belonging, in that order, to the highest A+ tier. Journal quality metrics fit nicely into the sociology of science literature and can be useful in models that attempt to explain how knowledge disseminates through scientific communities.",[],"Galletta, Dennis F.",N/A,"Decision, Operations, and Information Technology, Katz School of Business, University of Pittsburgh, Pittsburgh, PA 15260 U.S.A."
https://misq.umn.edu/misq/article/37/4/993/575/Evaluating-Journal-Quality-and-the-Association-for,MIS Quarterly,Evaluating Journal Quality and the Association for Information Systems Senior Scholars’ Journal Basket Via Bibliometric Measures: Do Expert Journal Assessments Add Value?1,"Volume 37, Issue 4",December 2013,"Information systems journal rankings and ratings help scholars focus their publishing efforts and are widely used surrogates for judging the quality of research. Over the years, numerous approaches have been used to rank IS journals, approaches such as citation metrics, school lists, acceptance rates, and expert assessments. However, the results of these approaches often conflict due to a host of validity concerns. In the current scientometric study, we make significant strides toward correcting for these limitations in the ranking of mainstream IS journals. We compare expert rankings to bibliometric measures such as the ISI Impact Factor™, the h-index, and social network analysis metrics. Among other findings, we conclude that bibliometric measures provide very similar results to expert-based methods in determining a tiered structure of IS journals, thereby suggesting that bibliometrics can be a complete, less expensive, and more efficient substitute for expert assessment. We also find strong support for seven of the eight journals in the Association for Information Systems Senior Scholars’ “basket” of journals. A cluster analysis of our results indicates a two-tiered separation in the quality of the highest quality IS journals—with MIS Quarterly, Information Systems Research, and Journal of Management Information Systems belonging, in that order, to the highest A+ tier. Journal quality metrics fit nicely into the sociology of science literature and can be useful in models that attempt to explain how knowledge disseminates through scientific communities.",[],"Humpherys, Sean L.",N/A,"Computer Information & Decision Management, College of Business, West Texas A&M University, Canyon, TX 79016 U.S.A."
https://misq.umn.edu/misq/article/37/4/993/575/Evaluating-Journal-Quality-and-the-Association-for,MIS Quarterly,Evaluating Journal Quality and the Association for Information Systems Senior Scholars’ Journal Basket Via Bibliometric Measures: Do Expert Journal Assessments Add Value?1,"Volume 37, Issue 4",December 2013,"Information systems journal rankings and ratings help scholars focus their publishing efforts and are widely used surrogates for judging the quality of research. Over the years, numerous approaches have been used to rank IS journals, approaches such as citation metrics, school lists, acceptance rates, and expert assessments. However, the results of these approaches often conflict due to a host of validity concerns. In the current scientometric study, we make significant strides toward correcting for these limitations in the ranking of mainstream IS journals. We compare expert rankings to bibliometric measures such as the ISI Impact Factor™, the h-index, and social network analysis metrics. Among other findings, we conclude that bibliometric measures provide very similar results to expert-based methods in determining a tiered structure of IS journals, thereby suggesting that bibliometrics can be a complete, less expensive, and more efficient substitute for expert assessment. We also find strong support for seven of the eight journals in the Association for Information Systems Senior Scholars’ “basket” of journals. A cluster analysis of our results indicates a two-tiered separation in the quality of the highest quality IS journals—with MIS Quarterly, Information Systems Research, and Journal of Management Information Systems belonging, in that order, to the highest A+ tier. Journal quality metrics fit nicely into the sociology of science literature and can be useful in models that attempt to explain how knowledge disseminates through scientific communities.",[],"Barlow, Jordan B.",N/A,"Operations & Decision Technologies, Kelley School of Business, Indiana University, Bloomington, IN 47405 U.S.A."
https://misq.umn.edu/misq/article/37/4/993/575/Evaluating-Journal-Quality-and-the-Association-for,MIS Quarterly,Evaluating Journal Quality and the Association for Information Systems Senior Scholars’ Journal Basket Via Bibliometric Measures: Do Expert Journal Assessments Add Value?1,"Volume 37, Issue 4",December 2013,"Information systems journal rankings and ratings help scholars focus their publishing efforts and are widely used surrogates for judging the quality of research. Over the years, numerous approaches have been used to rank IS journals, approaches such as citation metrics, school lists, acceptance rates, and expert assessments. However, the results of these approaches often conflict due to a host of validity concerns. In the current scientometric study, we make significant strides toward correcting for these limitations in the ranking of mainstream IS journals. We compare expert rankings to bibliometric measures such as the ISI Impact Factor™, the h-index, and social network analysis metrics. Among other findings, we conclude that bibliometric measures provide very similar results to expert-based methods in determining a tiered structure of IS journals, thereby suggesting that bibliometrics can be a complete, less expensive, and more efficient substitute for expert assessment. We also find strong support for seven of the eight journals in the Association for Information Systems Senior Scholars’ “basket” of journals. A cluster analysis of our results indicates a two-tiered separation in the quality of the highest quality IS journals—with MIS Quarterly, Information Systems Research, and Journal of Management Information Systems belonging, in that order, to the highest A+ tier. Journal quality metrics fit nicely into the sociology of science literature and can be useful in models that attempt to explain how knowledge disseminates through scientific communities.",[],"Wilson, David W.",N/A,"Management Information Systems Department, Eller College of Management, University of Arizona, Tucson, AZ 85721 U.S.A."
https://misq.umn.edu/misq/article/37/4/1013/577/A-Longitudinal-Study-of-Herd-Behavior-in-the,MIS Quarterly,A Longitudinal Study of Herd Behavior in the Adoption and Continued Use of Technology1,"Volume 37, Issue 4",December 2013,"Herd literature suggests that people tend to discount their own beliefs and imitate others when making adoption decisions and that the resulting adoption decisions are fragile and can be easily reversed during the post-adoptive stage. This helps explain why the adoption of a number of new technologies―from Amazon’s Kindle, to Apple’s iPod, iPhone, and iPad, to various types of Web 2. 0 technologies―appears to have adoption patterns similar to those of new fashion trends (i. e., an initial en masse acquisition followed by subsequent abandonment). It is important to understand these phenomena because they are strongly related to the staying power of technology. From a herd behavior perspective, this study proposes two new concepts, namely discounting one’s own information and imitating others, to describe herd behavior in technology adoption. A research model is developed to describe the conditions under which herd behavior in technology adoption occurs, how it impacts technology adoption decision making, and how it influences post-adoptive system use. A longitudinal study is conducted to examine the research model. Findings from this research suggest that the discounting of one’s own beliefs and the imitating of others when adopting a new technology are provoked primarily by the observation of prior adoptions and perceptions of uncertainty regarding the adoption of new technology. Herd behavior has a significant influence on user technology adoption; however, it does not necessarily lead to the collapse of the user base, as predicted in the herd literature. Instead, imitation can help reduce post-adoption regret and thus serve as a legitimate strategy for choosing a good enough technology, which may or may not be the best option to enhance job performance. People tend to adjust their beliefs when herding and also to revive their discounted initial beliefs to modify their beliefs about the technology at the post-adoptive stage. Findings from this study have significant research and practical implications.",[],"Sun, Heshan",N/A,"Department of Management, Clemson University, 107 Sirrine Hall, Clemson, SC 29634-1305 U.S.A."
https://misq.umn.edu/misq/article/37/4/1043/545/Impact-of-Wikipedia-on-Market-Information,MIS Quarterly,Impact of Wikipedia on Market Information Environment: Evidence on Management Disclosure and Investor Reaction1,"Volume 37, Issue 4",December 2013,"In this paper, we seek to determine whether a typical social media platform, Wikipedia, improves the information environment for investors in the financial market. Our theoretical lens leads us to expect that information aggregation about public companies on Wikipedia may influence how management’s voluntary information disclosure reacts to market uncertainty with respect to investors’ information about these companies. Our empirical analysis is based on a unique data set collected from financial records, management disclosure records, news article coverage, and a Wikipedia modification history of public companies. On the supply side of information, we find that information aggregation on Wikipedia can moderate the timing of managers’ voluntary disclosure of companies’ earnings disappointments, or bad news. On the demand side of information, we find that Wikipedia’s information aggregation moderates investors’ negative reaction to bad news. Taken together, these findings support the view that Wikipedia improves the information environment in the financial market and underscore the value of information aggregation through the use of information technology.",[],"Xu, Sean Xin",N/A,"School of Economics and Management, Tsinghua University, Beijing, China"
https://misq.umn.edu/misq/article/37/4/1043/545/Impact-of-Wikipedia-on-Market-Information,MIS Quarterly,Impact of Wikipedia on Market Information Environment: Evidence on Management Disclosure and Investor Reaction1,"Volume 37, Issue 4",December 2013,"In this paper, we seek to determine whether a typical social media platform, Wikipedia, improves the information environment for investors in the financial market. Our theoretical lens leads us to expect that information aggregation about public companies on Wikipedia may influence how management’s voluntary information disclosure reacts to market uncertainty with respect to investors’ information about these companies. Our empirical analysis is based on a unique data set collected from financial records, management disclosure records, news article coverage, and a Wikipedia modification history of public companies. On the supply side of information, we find that information aggregation on Wikipedia can moderate the timing of managers’ voluntary disclosure of companies’ earnings disappointments, or bad news. On the demand side of information, we find that Wikipedia’s information aggregation moderates investors’ negative reaction to bad news. Taken together, these findings support the view that Wikipedia improves the information environment in the financial market and underscore the value of information aggregation through the use of information technology.",[],"Zhang, Xiaoquan (Michael)",N/A,"School of Business and Management, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong"
https://misq.umn.edu/misq/article/37/4/1069/549/Talking-about-Technology-The-Emergence-of-a-New,MIS Quarterly,Talking about Technology: The Emergence of a New Actor Category Through New Media1,"Volume 37, Issue 4",December 2013,"This paper examines how a new actor category may emerge in a field of discourse through the new media of the Internet. Existing literatures on professional and organizational identity have shown the importance of identity claims and of the tensions surrounding “optimal distinctiveness” for new actors in a field, but have not examined the roles of new media in these processes. The literature on information technology (IT) and identity has highlighted the identity-challenging and identity-enhancing aspects of new IT use for existing actor categories but has not examined the dynamics associated with the emergence of new actor categories. Here, we investigate how a new actor category may emerge through the use of new media as a dynamic interaction of discursive practices, identity claims, and new media use. Drawing on findings from a case study of technology bloggers, we identified discursive practices through which a group of technology bloggers enacted claims of a distinctive identity in the joint construction of their discourse and in response to continuous developments in new media. Emergence of this new category was characterized by ongoing, opposing yet coexisting tendencies toward coalescence, fragmentation, and dispersion. Socio-technical dynamics underlying bloggers’ use of new media and the actions of prominent (“A-list”) bloggers contributed to these tendencies. We untangle theoretically the identity-enabling and identity-unsettling effects of new media and conceptualize the emergence of a new actor category through new media as an ongoing process in which the category identity may remain fluid, rather than progress to an endpoint.",[],"Vaast, Emmanuelle",N/A,"Desautels Faculty of Management, McGill University, Montreal, Quebec H3A 2T5 Canada"
https://misq.umn.edu/misq/article/37/4/1069/549/Talking-about-Technology-The-Emergence-of-a-New,MIS Quarterly,Talking about Technology: The Emergence of a New Actor Category Through New Media1,"Volume 37, Issue 4",December 2013,"This paper examines how a new actor category may emerge in a field of discourse through the new media of the Internet. Existing literatures on professional and organizational identity have shown the importance of identity claims and of the tensions surrounding “optimal distinctiveness” for new actors in a field, but have not examined the roles of new media in these processes. The literature on information technology (IT) and identity has highlighted the identity-challenging and identity-enhancing aspects of new IT use for existing actor categories but has not examined the dynamics associated with the emergence of new actor categories. Here, we investigate how a new actor category may emerge through the use of new media as a dynamic interaction of discursive practices, identity claims, and new media use. Drawing on findings from a case study of technology bloggers, we identified discursive practices through which a group of technology bloggers enacted claims of a distinctive identity in the joint construction of their discourse and in response to continuous developments in new media. Emergence of this new category was characterized by ongoing, opposing yet coexisting tendencies toward coalescence, fragmentation, and dispersion. Socio-technical dynamics underlying bloggers’ use of new media and the actions of prominent (“A-list”) bloggers contributed to these tendencies. We untangle theoretically the identity-enabling and identity-unsettling effects of new media and conceptualize the emergence of a new actor category through new media as an ongoing process in which the category identity may remain fluid, rather than progress to an endpoint.",[],"Davidson, Elizabeth J.",N/A,"Shidler College of Business, University of Hawaii at Manoa, Honolulu, HI 96822 U.S.A."
https://misq.umn.edu/misq/article/37/4/1069/549/Talking-about-Technology-The-Emergence-of-a-New,MIS Quarterly,Talking about Technology: The Emergence of a New Actor Category Through New Media1,"Volume 37, Issue 4",December 2013,"This paper examines how a new actor category may emerge in a field of discourse through the new media of the Internet. Existing literatures on professional and organizational identity have shown the importance of identity claims and of the tensions surrounding “optimal distinctiveness” for new actors in a field, but have not examined the roles of new media in these processes. The literature on information technology (IT) and identity has highlighted the identity-challenging and identity-enhancing aspects of new IT use for existing actor categories but has not examined the dynamics associated with the emergence of new actor categories. Here, we investigate how a new actor category may emerge through the use of new media as a dynamic interaction of discursive practices, identity claims, and new media use. Drawing on findings from a case study of technology bloggers, we identified discursive practices through which a group of technology bloggers enacted claims of a distinctive identity in the joint construction of their discourse and in response to continuous developments in new media. Emergence of this new category was characterized by ongoing, opposing yet coexisting tendencies toward coalescence, fragmentation, and dispersion. Socio-technical dynamics underlying bloggers’ use of new media and the actions of prominent (“A-list”) bloggers contributed to these tendencies. We untangle theoretically the identity-enabling and identity-unsettling effects of new media and conceptualize the emergence of a new actor category through new media as an ongoing process in which the category identity may remain fluid, rather than progress to an endpoint.",[],"Mattson, Thomas",N/A,"Shidler College of Business, University of Hawaii at Manoa, Honolulu, HI 96822 U.S.A."
https://misq.umn.edu/misq/article/37/4/1093/555/Differential-Influence-of-Blogs-Across-Different,MIS Quarterly,Differential Influence of Blogs Across Different Stages of Decision Making: The Case of Venture Capitalists1,"Volume 37, Issue 4",December 2013,"In this paper, we study the differential influence of online user-generated content (UGC), specifically blogs, across the multiple stages of decision making of venture capitalists: screening stage, choice stage, and contract stage. We conjecture that, first, blogs are influential at the screening stage; second, after the screening stage, blogs are noninfluential since decision makers evaluate entities closely at later stages; third, blogs increase the interest from multiple decision makers which in turn increases the cost of the deal for a decision maker. This empirical investigation provides support for the hypotheses, which we tested for funding decisions by venture capitalists in information technology ventures. In particular, this study indicates that blogs can help managers in getting their products/services selected at the screening stage, but, beyond that, blogs do not help directly. However, since more decision makers screen products/services that receive blog coverage, the competition among decision makers helps managers in negotiating better contract terms. We advance the boundary of existing studies on the influence of UGC from single stage process to multiple stages.",[],"Aggarwal, Rohit",N/A,"Operations and Information Systems, David Eccles School of Business, University of Utah, Salt Lake City, UT 84112 U.S.A."
https://misq.umn.edu/misq/article/37/4/1093/555/Differential-Influence-of-Blogs-Across-Different,MIS Quarterly,Differential Influence of Blogs Across Different Stages of Decision Making: The Case of Venture Capitalists1,"Volume 37, Issue 4",December 2013,"In this paper, we study the differential influence of online user-generated content (UGC), specifically blogs, across the multiple stages of decision making of venture capitalists: screening stage, choice stage, and contract stage. We conjecture that, first, blogs are influential at the screening stage; second, after the screening stage, blogs are noninfluential since decision makers evaluate entities closely at later stages; third, blogs increase the interest from multiple decision makers which in turn increases the cost of the deal for a decision maker. This empirical investigation provides support for the hypotheses, which we tested for funding decisions by venture capitalists in information technology ventures. In particular, this study indicates that blogs can help managers in getting their products/services selected at the screening stage, but, beyond that, blogs do not help directly. However, since more decision makers screen products/services that receive blog coverage, the competition among decision makers helps managers in negotiating better contract terms. We advance the boundary of existing studies on the influence of UGC from single stage process to multiple stages.",[],"Singh, Harpreet",N/A,"Information Systems and Operations Management, Naveen Jindal School of Business, University of Texas at Dallas, Richardson, TX 75080 U.S.A."
https://misq.umn.edu/misq/article/37/4/1113/559/Changes-in-Employees-Job-Characteristics-During-an,MIS Quarterly,Changes in Employees’ Job Characteristics During an Enterprise System Implementation: A Latent Growth Modeling Perspective1,"Volume 37, Issue 4",December 2013,"Enterprise system implementations often create tension in organizations. On the one hand, these systems can provide significant operational and strategic benefits. On the other hand, implementation of these systems is risky and a source of major disruptions. In particular, employees experience significant changes in their work environment during an implementation. Although the relationship between ES implementations and employees’ jobs has been noted in prior research, there is limited research on the nature, extent, determinants, and outcomes of changes in employees’ job characteristics following an ES implementation. This paper develops and tests a model, termed the job characteristics change model (JCCM), that posits that employees will experience substantial changes in two job characteristics (i.e., job demands and job control) during the shakedown phase (i.e., immediately after the rollout) of an ES implementation. These changes are theorized to be predicted by work process characteristics, namely perceived process complexity, perceived process rigidity, and perceived process radicalness, that in turn will be influenced by technology characteristics (i.e., perceived technology complexity, perceived technology reconfigurability, and perceived technology customization). JCCM further posits that changes in job characteristics will influence employees’ job satisfaction. Longitudinal field studies conducted in two organizations (N = 281 and 141 respectively) provided support for the model. The scientific and practical implications of the findings are discussed.",[],"Bala, Hillol",N/A,"Operations and Decision Technologies, Kelley School of Business, Indiana University, Bloomington, IN 47405 U.S.A."
https://misq.umn.edu/misq/article/37/4/1113/559/Changes-in-Employees-Job-Characteristics-During-an,MIS Quarterly,Changes in Employees’ Job Characteristics During an Enterprise System Implementation: A Latent Growth Modeling Perspective1,"Volume 37, Issue 4",December 2013,"Enterprise system implementations often create tension in organizations. On the one hand, these systems can provide significant operational and strategic benefits. On the other hand, implementation of these systems is risky and a source of major disruptions. In particular, employees experience significant changes in their work environment during an implementation. Although the relationship between ES implementations and employees’ jobs has been noted in prior research, there is limited research on the nature, extent, determinants, and outcomes of changes in employees’ job characteristics following an ES implementation. This paper develops and tests a model, termed the job characteristics change model (JCCM), that posits that employees will experience substantial changes in two job characteristics (i.e., job demands and job control) during the shakedown phase (i.e., immediately after the rollout) of an ES implementation. These changes are theorized to be predicted by work process characteristics, namely perceived process complexity, perceived process rigidity, and perceived process radicalness, that in turn will be influenced by technology characteristics (i.e., perceived technology complexity, perceived technology reconfigurability, and perceived technology customization). JCCM further posits that changes in job characteristics will influence employees’ job satisfaction. Longitudinal field studies conducted in two organizations (N = 281 and 141 respectively) provided support for the model. The scientific and practical implications of the findings are discussed.",[],"Venkatesh, Viswanath",N/A,"Department of Information Systems, Walton College of Business, University of Arkansas, Fayetteville, AR 72701 U.S.A."
https://misq.umn.edu/misq/article/37/4/1141/562/Addressing-the-Personalization-Privacy-Paradox-An,MIS Quarterly,Addressing the Personalization–Privacy Paradox: An Empirical Assessment from a Field Experiment on Smartphone Users1,"Volume 37, Issue 4",December 2013,"Privacy has been an enduring concern associated with commercial information technology (IT) applications, in particular regarding the issue of personalization. IT-enabled personalization, while potentially making the user computing experience more gratifying, often relies heavily on the user’s personal information to deliver individualized services, which raises the user’s privacy concerns. We term the tension between personalization and privacy, which follows from marketers exploiting consumers’ data to offer personalized product information, the personalization–privacy paradox. To better understand this paradox, we build on the theoretical lenses of uses and gratification theory and information boundary theory to conceptualize the extent to which privacy impacts the process and content gratifications derived from personalization, and how an IT solution can be designed to alleviate privacy concerns.Set in the context of personalized advertising applications for smartphones, we propose and prototype an IT solution, referred to as a personalized, privacy-safe application, that retains users’ information locally on their smartphones while still providing them with personalized product messages. We validated this solution through a field experiment by benchmarking it against two more conventional applications: a base non-personalized application that broadcasts non-personalized product information to users, and a personalized, non-privacy safe application that transmits user information to a central marketer’s server. The results show that (compared to the non-personalized application), while personalized, privacy-safe or not increased application usage (reflecting process gratification), it was only when it was privacy-safe that users saved product messages (reflecting content gratification) more frequently. Follow-up surveys corroborated these nuanced findings and further revealed the users’ psychological states, which explained our field experiment results. We found that saving advertisements for content gratification led to a perceived intrusion of information boundary that made users reluctant to do so. Overall our proposed IT solution, which delivers a personalized service but avoids transmitting users’ personal information to third parties, reduces users’ perceptions that their information boundaries are being intruded upon, thus mitigating the personalization–privacy paradox and increasing both process and content gratification.",[],"Sutanto, Juliana",N/A,"Department of Management, Technology, and Economics, ETH Zürich, Weinbergstrasse 56/58, Zürich, Switzerland"
https://misq.umn.edu/misq/article/37/4/1141/562/Addressing-the-Personalization-Privacy-Paradox-An,MIS Quarterly,Addressing the Personalization–Privacy Paradox: An Empirical Assessment from a Field Experiment on Smartphone Users1,"Volume 37, Issue 4",December 2013,"Privacy has been an enduring concern associated with commercial information technology (IT) applications, in particular regarding the issue of personalization. IT-enabled personalization, while potentially making the user computing experience more gratifying, often relies heavily on the user’s personal information to deliver individualized services, which raises the user’s privacy concerns. We term the tension between personalization and privacy, which follows from marketers exploiting consumers’ data to offer personalized product information, the personalization–privacy paradox. To better understand this paradox, we build on the theoretical lenses of uses and gratification theory and information boundary theory to conceptualize the extent to which privacy impacts the process and content gratifications derived from personalization, and how an IT solution can be designed to alleviate privacy concerns.Set in the context of personalized advertising applications for smartphones, we propose and prototype an IT solution, referred to as a personalized, privacy-safe application, that retains users’ information locally on their smartphones while still providing them with personalized product messages. We validated this solution through a field experiment by benchmarking it against two more conventional applications: a base non-personalized application that broadcasts non-personalized product information to users, and a personalized, non-privacy safe application that transmits user information to a central marketer’s server. The results show that (compared to the non-personalized application), while personalized, privacy-safe or not increased application usage (reflecting process gratification), it was only when it was privacy-safe that users saved product messages (reflecting content gratification) more frequently. Follow-up surveys corroborated these nuanced findings and further revealed the users’ psychological states, which explained our field experiment results. We found that saving advertisements for content gratification led to a perceived intrusion of information boundary that made users reluctant to do so. Overall our proposed IT solution, which delivers a personalized service but avoids transmitting users’ personal information to third parties, reduces users’ perceptions that their information boundaries are being intruded upon, thus mitigating the personalization–privacy paradox and increasing both process and content gratification.",[],"Palme, Elia",N/A,"Newscron Ltd., Via Maderno 24, Lugano, Switzerland"
https://misq.umn.edu/misq/article/37/4/1141/562/Addressing-the-Personalization-Privacy-Paradox-An,MIS Quarterly,Addressing the Personalization–Privacy Paradox: An Empirical Assessment from a Field Experiment on Smartphone Users1,"Volume 37, Issue 4",December 2013,"Privacy has been an enduring concern associated with commercial information technology (IT) applications, in particular regarding the issue of personalization. IT-enabled personalization, while potentially making the user computing experience more gratifying, often relies heavily on the user’s personal information to deliver individualized services, which raises the user’s privacy concerns. We term the tension between personalization and privacy, which follows from marketers exploiting consumers’ data to offer personalized product information, the personalization–privacy paradox. To better understand this paradox, we build on the theoretical lenses of uses and gratification theory and information boundary theory to conceptualize the extent to which privacy impacts the process and content gratifications derived from personalization, and how an IT solution can be designed to alleviate privacy concerns.Set in the context of personalized advertising applications for smartphones, we propose and prototype an IT solution, referred to as a personalized, privacy-safe application, that retains users’ information locally on their smartphones while still providing them with personalized product messages. We validated this solution through a field experiment by benchmarking it against two more conventional applications: a base non-personalized application that broadcasts non-personalized product information to users, and a personalized, non-privacy safe application that transmits user information to a central marketer’s server. The results show that (compared to the non-personalized application), while personalized, privacy-safe or not increased application usage (reflecting process gratification), it was only when it was privacy-safe that users saved product messages (reflecting content gratification) more frequently. Follow-up surveys corroborated these nuanced findings and further revealed the users’ psychological states, which explained our field experiment results. We found that saving advertisements for content gratification led to a perceived intrusion of information boundary that made users reluctant to do so. Overall our proposed IT solution, which delivers a personalized service but avoids transmitting users’ personal information to third parties, reduces users’ perceptions that their information boundaries are being intruded upon, thus mitigating the personalization–privacy paradox and increasing both process and content gratification.",[],"Tan, Chuan-Hoo",N/A,"Department of Information Systems, City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong"
https://misq.umn.edu/misq/article/37/4/1141/562/Addressing-the-Personalization-Privacy-Paradox-An,MIS Quarterly,Addressing the Personalization–Privacy Paradox: An Empirical Assessment from a Field Experiment on Smartphone Users1,"Volume 37, Issue 4",December 2013,"Privacy has been an enduring concern associated with commercial information technology (IT) applications, in particular regarding the issue of personalization. IT-enabled personalization, while potentially making the user computing experience more gratifying, often relies heavily on the user’s personal information to deliver individualized services, which raises the user’s privacy concerns. We term the tension between personalization and privacy, which follows from marketers exploiting consumers’ data to offer personalized product information, the personalization–privacy paradox. To better understand this paradox, we build on the theoretical lenses of uses and gratification theory and information boundary theory to conceptualize the extent to which privacy impacts the process and content gratifications derived from personalization, and how an IT solution can be designed to alleviate privacy concerns.Set in the context of personalized advertising applications for smartphones, we propose and prototype an IT solution, referred to as a personalized, privacy-safe application, that retains users’ information locally on their smartphones while still providing them with personalized product messages. We validated this solution through a field experiment by benchmarking it against two more conventional applications: a base non-personalized application that broadcasts non-personalized product information to users, and a personalized, non-privacy safe application that transmits user information to a central marketer’s server. The results show that (compared to the non-personalized application), while personalized, privacy-safe or not increased application usage (reflecting process gratification), it was only when it was privacy-safe that users saved product messages (reflecting content gratification) more frequently. Follow-up surveys corroborated these nuanced findings and further revealed the users’ psychological states, which explained our field experiment results. We found that saving advertisements for content gratification led to a perceived intrusion of information boundary that made users reluctant to do so. Overall our proposed IT solution, which delivers a personalized service but avoids transmitting users’ personal information to third parties, reduces users’ perceptions that their information boundaries are being intruded upon, thus mitigating the personalization–privacy paradox and increasing both process and content gratification.",[],"Phang, Chee Wei",N/A,"Department of Information Management and Information Systems, Fudan University, 670 Guoshun Road, Shanghai, China"
https://misq.umn.edu/misq/article/37/4/1165/566/An-Investigation-of-Information-Systems-Use,MIS Quarterly,"An Investigation of Information Systems Use Patterns: Technological Events as Triggers, The Effect of Time, and Consequences for Performance1","Volume 37, Issue 4",December 2013,"Information systems use represents one of the core concepts defining the discipline. In this article, we develop a rich conceptualization of IS use patterns as individuals’ emotions, cognition, and behaviors while employing an information technology to accomplish a work-related task. By combining two novel perspectives—the affect–object paradigm and automaticity—with coping theory, we theorize how different patterns appear and disappear as a result of different IT events—expected and discrepant—as well as over time, and how these patterns influence short-term performance. In order to test our hypotheses, we conducted two studies, one qualitative and the other quantitative, that combined different methods (e.g., open-ended questions, physiological data, videos, protocol analysis) to study the influence of expected and discrepant events. The synergistic properties of the two studies demonstrate the existence of two IS use patterns, automatic and adjusting. Most interactions are automatic, and adjusting patterns, triggered by discrepant IT events, fade over time and transition into automatic ones. Further, automatic patterns result in enhanced short-term performance, while adjusting ones do not. Our conceptualization of IS use patterns is useful because it addresses important questions (such as why negative IT perceptions persist) and clarifies that it is how (rather than how much) people use IT that is pertinent for performance.",[],"de Guinea, Ana Ortiz",N/A,"HEC Montréal, 3000 chemin de la Côte Sainte-Catherine, Montréal, Québec H3T 2A7 Canada"
https://misq.umn.edu/misq/article/37/4/1165/566/An-Investigation-of-Information-Systems-Use,MIS Quarterly,"An Investigation of Information Systems Use Patterns: Technological Events as Triggers, The Effect of Time, and Consequences for Performance1","Volume 37, Issue 4",December 2013,"Information systems use represents one of the core concepts defining the discipline. In this article, we develop a rich conceptualization of IS use patterns as individuals’ emotions, cognition, and behaviors while employing an information technology to accomplish a work-related task. By combining two novel perspectives—the affect–object paradigm and automaticity—with coping theory, we theorize how different patterns appear and disappear as a result of different IT events—expected and discrepant—as well as over time, and how these patterns influence short-term performance. In order to test our hypotheses, we conducted two studies, one qualitative and the other quantitative, that combined different methods (e.g., open-ended questions, physiological data, videos, protocol analysis) to study the influence of expected and discrepant events. The synergistic properties of the two studies demonstrate the existence of two IS use patterns, automatic and adjusting. Most interactions are automatic, and adjusting patterns, triggered by discrepant IT events, fade over time and transition into automatic ones. Further, automatic patterns result in enhanced short-term performance, while adjusting ones do not. Our conceptualization of IS use patterns is useful because it addresses important questions (such as why negative IT perceptions persist) and clarifies that it is how (rather than how much) people use IT that is pertinent for performance.",[],"Webster, Jane",N/A,"Queen’s School of Business, Queen’s University, 143 Union Street, Kingston, Ontario K7L 3N6 Canada"
https://misq.umn.edu/misq/article/37/4/1189/570/Insiders-Protection-of-Organizational-Information,MIS Quarterly,Insiders’ Protection of Organizational Information Assets: Development of a Systematics-Based Taxonomy and Theory of Diversity for Protection-Motivated Behaviors1,"Volume 37, Issue 4",December 2013,"Protecting information from a variety of security threats is a daunting organizational activity. Organization managers must recognize the roles that organization insiders have in protecting information resources rather than solely relying upon technology to provide this protection. Unfortunately, compared to negative insider behaviors, the extant literature provides sparse coverage of beneficial insider activities. The few beneficial activities in the literature represent only a small portion of the diverse collection of insiders’ protective actions.This research focuses on protection-motivated behaviors (PMBs), which are volitional behaviors enacted by organization insiders to protect (1) organizationally relevant information and (2) the computer-based information systems in which the information is stored, collected, disseminated, and/or manipulated from information security threats. Based on systematics, we propose a six-step methodology of qualitative and quantitative approaches to develop a taxonomy and theory of diversity for PMBs. These approaches integrate the classification techniques of multidimensional scaling (MDS), property fitting (ProFit), and cluster analyses. We leverage these techniques to identify and display how insiders collectively classify 67 unique PMBs and their homogeneous classes. Our taxonomy provides researchers and practitioners a comprehensive guide and common nomenclature for PMBs. Our methodology can be similarly used to create other theories of diversity.",[],"Posey, Clay",N/A,"Department of Information Systems, Statistics, and Management Science, Culverhouse College of Commerce and Business, Administration, The University of Alabama, Tuscaloosa, AL 35487 U.S.A."
https://misq.umn.edu/misq/article/37/4/1189/570/Insiders-Protection-of-Organizational-Information,MIS Quarterly,Insiders’ Protection of Organizational Information Assets: Development of a Systematics-Based Taxonomy and Theory of Diversity for Protection-Motivated Behaviors1,"Volume 37, Issue 4",December 2013,"Protecting information from a variety of security threats is a daunting organizational activity. Organization managers must recognize the roles that organization insiders have in protecting information resources rather than solely relying upon technology to provide this protection. Unfortunately, compared to negative insider behaviors, the extant literature provides sparse coverage of beneficial insider activities. The few beneficial activities in the literature represent only a small portion of the diverse collection of insiders’ protective actions.This research focuses on protection-motivated behaviors (PMBs), which are volitional behaviors enacted by organization insiders to protect (1) organizationally relevant information and (2) the computer-based information systems in which the information is stored, collected, disseminated, and/or manipulated from information security threats. Based on systematics, we propose a six-step methodology of qualitative and quantitative approaches to develop a taxonomy and theory of diversity for PMBs. These approaches integrate the classification techniques of multidimensional scaling (MDS), property fitting (ProFit), and cluster analyses. We leverage these techniques to identify and display how insiders collectively classify 67 unique PMBs and their homogeneous classes. Our taxonomy provides researchers and practitioners a comprehensive guide and common nomenclature for PMBs. Our methodology can be similarly used to create other theories of diversity.",[],"Roberts, Tom L.",N/A,"Department of Management and Information Systems, College of Business, Louisiana Tech University, Ruston, LA 71272 U.S.A."
https://misq.umn.edu/misq/article/37/4/1189/570/Insiders-Protection-of-Organizational-Information,MIS Quarterly,Insiders’ Protection of Organizational Information Assets: Development of a Systematics-Based Taxonomy and Theory of Diversity for Protection-Motivated Behaviors1,"Volume 37, Issue 4",December 2013,"Protecting information from a variety of security threats is a daunting organizational activity. Organization managers must recognize the roles that organization insiders have in protecting information resources rather than solely relying upon technology to provide this protection. Unfortunately, compared to negative insider behaviors, the extant literature provides sparse coverage of beneficial insider activities. The few beneficial activities in the literature represent only a small portion of the diverse collection of insiders’ protective actions.This research focuses on protection-motivated behaviors (PMBs), which are volitional behaviors enacted by organization insiders to protect (1) organizationally relevant information and (2) the computer-based information systems in which the information is stored, collected, disseminated, and/or manipulated from information security threats. Based on systematics, we propose a six-step methodology of qualitative and quantitative approaches to develop a taxonomy and theory of diversity for PMBs. These approaches integrate the classification techniques of multidimensional scaling (MDS), property fitting (ProFit), and cluster analyses. We leverage these techniques to identify and display how insiders collectively classify 67 unique PMBs and their homogeneous classes. Our taxonomy provides researchers and practitioners a comprehensive guide and common nomenclature for PMBs. Our methodology can be similarly used to create other theories of diversity.",[],"Lowry, Paul Benjamin",N/A,"Department of Information Systems, College of Business, City University of Hong Kong, Hong Kong People’s Republic of China"
https://misq.umn.edu/misq/article/37/4/1189/570/Insiders-Protection-of-Organizational-Information,MIS Quarterly,Insiders’ Protection of Organizational Information Assets: Development of a Systematics-Based Taxonomy and Theory of Diversity for Protection-Motivated Behaviors1,"Volume 37, Issue 4",December 2013,"Protecting information from a variety of security threats is a daunting organizational activity. Organization managers must recognize the roles that organization insiders have in protecting information resources rather than solely relying upon technology to provide this protection. Unfortunately, compared to negative insider behaviors, the extant literature provides sparse coverage of beneficial insider activities. The few beneficial activities in the literature represent only a small portion of the diverse collection of insiders’ protective actions.This research focuses on protection-motivated behaviors (PMBs), which are volitional behaviors enacted by organization insiders to protect (1) organizationally relevant information and (2) the computer-based information systems in which the information is stored, collected, disseminated, and/or manipulated from information security threats. Based on systematics, we propose a six-step methodology of qualitative and quantitative approaches to develop a taxonomy and theory of diversity for PMBs. These approaches integrate the classification techniques of multidimensional scaling (MDS), property fitting (ProFit), and cluster analyses. We leverage these techniques to identify and display how insiders collectively classify 67 unique PMBs and their homogeneous classes. Our taxonomy provides researchers and practitioners a comprehensive guide and common nomenclature for PMBs. Our methodology can be similarly used to create other theories of diversity.",[],"Bennett, Rebecca J.",N/A,"Department of Management and Information Systems, College of Business, Louisiana Tech University, Ruston, LA 71272 U.S.A."
https://misq.umn.edu/misq/article/37/4/1189/570/Insiders-Protection-of-Organizational-Information,MIS Quarterly,Insiders’ Protection of Organizational Information Assets: Development of a Systematics-Based Taxonomy and Theory of Diversity for Protection-Motivated Behaviors1,"Volume 37, Issue 4",December 2013,"Protecting information from a variety of security threats is a daunting organizational activity. Organization managers must recognize the roles that organization insiders have in protecting information resources rather than solely relying upon technology to provide this protection. Unfortunately, compared to negative insider behaviors, the extant literature provides sparse coverage of beneficial insider activities. The few beneficial activities in the literature represent only a small portion of the diverse collection of insiders’ protective actions.This research focuses on protection-motivated behaviors (PMBs), which are volitional behaviors enacted by organization insiders to protect (1) organizationally relevant information and (2) the computer-based information systems in which the information is stored, collected, disseminated, and/or manipulated from information security threats. Based on systematics, we propose a six-step methodology of qualitative and quantitative approaches to develop a taxonomy and theory of diversity for PMBs. These approaches integrate the classification techniques of multidimensional scaling (MDS), property fitting (ProFit), and cluster analyses. We leverage these techniques to identify and display how insiders collectively classify 67 unique PMBs and their homogeneous classes. Our taxonomy provides researchers and practitioners a comprehensive guide and common nomenclature for PMBs. Our methodology can be similarly used to create other theories of diversity.",[],"Courtney, James F.",N/A,"Department of Management and Information Systems, College of Business, Louisiana Tech University, Ruston, LA 71272 U.S.A."
https://misq.umn.edu/misq/article/37/4/1211/564/Control-Balancing-in-Information-Systems,MIS Quarterly,Control Balancing in Information Systems Development Offshoring Projects1,"Volume 37, Issue 4",December 2013,"While much is known about selecting different types of control that can be exercised in information systems development projects, the control dynamics associated with ISD offshoring projects represent an important gap in our understanding. In this paper, we develop a substantive grounded theory of control balancing that addresses this theoretical gap. Based on a longitudinal case study of an ISD offshoring project in the financial services industry, we introduce a three-dimensional control configuration category that emerged from our data, suggesting that control type is only one dimension on which control configuration decisions need to be made. The other two dimensions that we identified are control degree (tight versus relaxed) and control style (uni-lateral versus bilateral). Furthermore, we illustrate that control execution during the life cycle of an ISD offshoring project is highly intertwined with the development of client–vendor shared understanding and that each influences the other. Based on these findings, we develop an integrative process model that explains how offshoring project managers make adjustments to the control configuration periodically to allow the ISD offshoring project and relationship to progress, yielding the iterative use of different three-dimensional control configurations that we conceptualize in the paper. Our process model of control balancing may trigger new ways of looking at control phenomena in temporary interfirm organizations such as client–vendor ISD off-shoring projects. Implications for research on organizational control and ISD offshoring are discussed. In addition, guidelines for ISD offshoring practitioners are presented.",[],"Gregory, Robert Wayne",N/A,"IESE Business School, Av. Pearson 21, 08034 Barcelona Spain"
https://misq.umn.edu/misq/article/37/4/1211/564/Control-Balancing-in-Information-Systems,MIS Quarterly,Control Balancing in Information Systems Development Offshoring Projects1,"Volume 37, Issue 4",December 2013,"While much is known about selecting different types of control that can be exercised in information systems development projects, the control dynamics associated with ISD offshoring projects represent an important gap in our understanding. In this paper, we develop a substantive grounded theory of control balancing that addresses this theoretical gap. Based on a longitudinal case study of an ISD offshoring project in the financial services industry, we introduce a three-dimensional control configuration category that emerged from our data, suggesting that control type is only one dimension on which control configuration decisions need to be made. The other two dimensions that we identified are control degree (tight versus relaxed) and control style (uni-lateral versus bilateral). Furthermore, we illustrate that control execution during the life cycle of an ISD offshoring project is highly intertwined with the development of client–vendor shared understanding and that each influences the other. Based on these findings, we develop an integrative process model that explains how offshoring project managers make adjustments to the control configuration periodically to allow the ISD offshoring project and relationship to progress, yielding the iterative use of different three-dimensional control configurations that we conceptualize in the paper. Our process model of control balancing may trigger new ways of looking at control phenomena in temporary interfirm organizations such as client–vendor ISD off-shoring projects. Implications for research on organizational control and ISD offshoring are discussed. In addition, guidelines for ISD offshoring practitioners are presented.",[],"Beck, Roman",N/A,"Goethe University Frankfurt, Grüneburgplatz 1, 60323 Frankfurt, Germany"
https://misq.umn.edu/misq/article/37/4/1211/564/Control-Balancing-in-Information-Systems,MIS Quarterly,Control Balancing in Information Systems Development Offshoring Projects1,"Volume 37, Issue 4",December 2013,"While much is known about selecting different types of control that can be exercised in information systems development projects, the control dynamics associated with ISD offshoring projects represent an important gap in our understanding. In this paper, we develop a substantive grounded theory of control balancing that addresses this theoretical gap. Based on a longitudinal case study of an ISD offshoring project in the financial services industry, we introduce a three-dimensional control configuration category that emerged from our data, suggesting that control type is only one dimension on which control configuration decisions need to be made. The other two dimensions that we identified are control degree (tight versus relaxed) and control style (uni-lateral versus bilateral). Furthermore, we illustrate that control execution during the life cycle of an ISD offshoring project is highly intertwined with the development of client–vendor shared understanding and that each influences the other. Based on these findings, we develop an integrative process model that explains how offshoring project managers make adjustments to the control configuration periodically to allow the ISD offshoring project and relationship to progress, yielding the iterative use of different three-dimensional control configurations that we conceptualize in the paper. Our process model of control balancing may trigger new ways of looking at control phenomena in temporary interfirm organizations such as client–vendor ISD off-shoring projects. Implications for research on organizational control and ISD offshoring are discussed. In addition, guidelines for ISD offshoring practitioners are presented.",[],"Keil, Mark",N/A,"Robinson College of Business, Georgia State University, 35 Broad Street, Atlanta, GA 30303 U.S.A."
https://misq.umn.edu/misq/article/37/4/1233/558/Media-Selection-as-a-Strategic-Component-of,MIS Quarterly,Media Selection as a Strategic Component of Communication1,"Volume 37, Issue 4",December 2013,"Why do people select the media they choose for a particular type of communication? The media choice literature has considered myriad contextual factors that influence media choice, from proximity of the communication partners, to the urgency of the situation, to time pressure, and so on. From this body of work, a contingency-based theory of media choice has emerged. An alternative approach is to investigate how communication strategies and media characteristics affect choice. We identified two approaches for investigating these issues: Te’eni’s (2001) model of organizational communication and Dennis et al.’s (2008) media synchronicity theory. Using a scenario-based methodology, we asked respondents which medium they would use for a deceptive communication task and why they made that choice. We analyzed the data from the perspective of both the Te’eni and MST frameworks, enabling us to compare the extent to which each was able to explain our respondents’ media choices. Both frameworks, at differing levels of communication granularity, suggest that the intent of the communication drives a strategy that ultimately informs media choice. The results suggest that the prior contingency-based explanations of media choice could be improved by not only understanding the intent of the communication, but also the strategy used by an individual to execute this communication. Additionally, we found that the more finely grained view of communication contained in MST explained more of the outcomes and was more parsimonious as well.",[],"George, Joey F.",N/A,"College of Business, Iowa State University, Ames, IA 50011 U.S.A."
https://misq.umn.edu/misq/article/37/4/1233/558/Media-Selection-as-a-Strategic-Component-of,MIS Quarterly,Media Selection as a Strategic Component of Communication1,"Volume 37, Issue 4",December 2013,"Why do people select the media they choose for a particular type of communication? The media choice literature has considered myriad contextual factors that influence media choice, from proximity of the communication partners, to the urgency of the situation, to time pressure, and so on. From this body of work, a contingency-based theory of media choice has emerged. An alternative approach is to investigate how communication strategies and media characteristics affect choice. We identified two approaches for investigating these issues: Te’eni’s (2001) model of organizational communication and Dennis et al.’s (2008) media synchronicity theory. Using a scenario-based methodology, we asked respondents which medium they would use for a deceptive communication task and why they made that choice. We analyzed the data from the perspective of both the Te’eni and MST frameworks, enabling us to compare the extent to which each was able to explain our respondents’ media choices. Both frameworks, at differing levels of communication granularity, suggest that the intent of the communication drives a strategy that ultimately informs media choice. The results suggest that the prior contingency-based explanations of media choice could be improved by not only understanding the intent of the communication, but also the strategy used by an individual to execute this communication. Additionally, we found that the more finely grained view of communication contained in MST explained more of the outcomes and was more parsimonious as well.",[],"Carlson, John R.",N/A,"Hankamer School of Business, Baylor University, Waco, TX 76798-8005 U.S.A."
https://misq.umn.edu/misq/article/37/4/1233/558/Media-Selection-as-a-Strategic-Component-of,MIS Quarterly,Media Selection as a Strategic Component of Communication1,"Volume 37, Issue 4",December 2013,"Why do people select the media they choose for a particular type of communication? The media choice literature has considered myriad contextual factors that influence media choice, from proximity of the communication partners, to the urgency of the situation, to time pressure, and so on. From this body of work, a contingency-based theory of media choice has emerged. An alternative approach is to investigate how communication strategies and media characteristics affect choice. We identified two approaches for investigating these issues: Te’eni’s (2001) model of organizational communication and Dennis et al.’s (2008) media synchronicity theory. Using a scenario-based methodology, we asked respondents which medium they would use for a deceptive communication task and why they made that choice. We analyzed the data from the perspective of both the Te’eni and MST frameworks, enabling us to compare the extent to which each was able to explain our respondents’ media choices. Both frameworks, at differing levels of communication granularity, suggest that the intent of the communication drives a strategy that ultimately informs media choice. The results suggest that the prior contingency-based explanations of media choice could be improved by not only understanding the intent of the communication, but also the strategy used by an individual to execute this communication. Additionally, we found that the more finely grained view of communication contained in MST explained more of the outcomes and was more parsimonious as well.",[],"Valacich, Joseph S.",N/A,"Eller College of Management, The University of Arizona, Tucson, AZ 85721 U.S.A."
https://misq.umn.edu/misq/article/37/4/1253/561/Inferring-App-Demand-from-Publicly-Available-Data1,MIS Quarterly,Inferring App Demand from Publicly Available Data1,"Volume 37, Issue 4",December 2013,"With an abundance of products available online, many online retailers provide sales rankings to make it easier for consumers to find the best-selling products. Successfully implementing product rankings online was done a decade ago by Amazon, and more recently by Apple’s App Store. However, neither market provides actual download data, a very useful statistic for both practitioners and researchers. In the past, researchers developed various strategies that allowed them to infer demand from rank data. Almost all of that work is based on an experiment that shifts sales or collaboration with a vendor to get actual sales data. In this research, we present an innovative method to use public data to infer the rank–demand relationship for the paid apps on Apple’s iTunes App Store. We find that the top-ranked paid app for iPhone generates 150 times more downloads compared to the paid app ranked at 200. Similarly, the top paid app on iPad generates 120 times more downloads compared to the paid app ranked at 200. We conclude with a discussion on an extension of this framework to the Android platform, in-app purchases, and free apps.",[],"Garg, Rajiv",N/A,"McCombs School of Business, The University of Texas at Austin, Austin, TX 78712 U.S.A."
https://misq.umn.edu/misq/article/37/4/1253/561/Inferring-App-Demand-from-Publicly-Available-Data1,MIS Quarterly,Inferring App Demand from Publicly Available Data1,"Volume 37, Issue 4",December 2013,"With an abundance of products available online, many online retailers provide sales rankings to make it easier for consumers to find the best-selling products. Successfully implementing product rankings online was done a decade ago by Amazon, and more recently by Apple’s App Store. However, neither market provides actual download data, a very useful statistic for both practitioners and researchers. In the past, researchers developed various strategies that allowed them to infer demand from rank data. Almost all of that work is based on an experiment that shifts sales or collaboration with a vendor to get actual sales data. In this research, we present an innovative method to use public data to infer the rank–demand relationship for the paid apps on Apple’s iTunes App Store. We find that the top-ranked paid app for iPhone generates 150 times more downloads compared to the paid app ranked at 200. Similarly, the top paid app on iPad generates 120 times more downloads compared to the paid app ranked at 200. We conclude with a discussion on an extension of this framework to the Android platform, in-app purchases, and free apps.",[],"Telang, Rahul",N/A,"School of Information Systems & Management. H. John Heinz III College, Carnegie Mellon University, Pittsburgh PA 15213 U.S.A."
https://misq.umn.edu/misq/article/37/4/1265/548/Spurring-Impactful-Research-on-Information-Systems,MIS Quarterly,Spurring Impactful Research on Information Systems for Environmental Sustainability,"Volume 37, Issue 4",December 2013,N/A,[],"Malhotra, Arvind",N/A,"Kenan-Flagler Business School, University of North Carolina at Chapel Hill, 300 Kenan Center Drive, Chapel Hill, NC 27599 U.S.A."
https://misq.umn.edu/misq/article/37/4/1265/548/Spurring-Impactful-Research-on-Information-Systems,MIS Quarterly,Spurring Impactful Research on Information Systems for Environmental Sustainability,"Volume 37, Issue 4",December 2013,N/A,[],"Melville, Nigel P.",N/A,"Stephen M. Ross School of Business, University of Michigan, 701 Tappan Street, Ann Arbor, MI 48109-1234 U.S.A."
https://misq.umn.edu/misq/article/37/4/1265/548/Spurring-Impactful-Research-on-Information-Systems,MIS Quarterly,Spurring Impactful Research on Information Systems for Environmental Sustainability,"Volume 37, Issue 4",December 2013,N/A,[],"Watson, Richard T.",N/A,"Terry College of Business, University of Georgia, 310 Herty Drive, Athens, GA 30602-6273 U.S.A."
https://misq.umn.edu/misq/article/37/4/ii/553/Guest-EditorialQualitative-Studies-in-Information,MIS Quarterly,Guest EditorialQualitative Studies in Information Systems: A Critical Review and Some Guiding Principles1,"Volume 37, Issue 4",December 2013,N/A,[],"Sarker, Suprateek",N/A,"McIntire School of Commerce, University of Virginia, USA"
https://misq.umn.edu/misq/article/37/4/ii/553/Guest-EditorialQualitative-Studies-in-Information,MIS Quarterly,Guest EditorialQualitative Studies in Information Systems: A Critical Review and Some Guiding Principles1,"Volume 37, Issue 4",December 2013,N/A,[],"Xiao, Xiao",N/A,"Department of IT Management, Copenhagen Business School, Denmark"
https://misq.umn.edu/misq/article/37/4/ii/553/Guest-EditorialQualitative-Studies-in-Information,MIS Quarterly,Guest EditorialQualitative Studies in Information Systems: A Critical Review and Some Guiding Principles1,"Volume 37, Issue 4",December 2013,N/A,[],"Beaulieu, Tanya",N/A,"Department of Management, Information Systems, and Entrepreneurship, Washington State University, USA"
https://misq.umn.edu/misq/article/37/4/1275/569/Sensemaking-and-Sustainable-Practicing-Functional,MIS Quarterly,Sensemaking and Sustainable Practicing: Functional Affordances of Information Systems in Green Transformations1,"Volume 37, Issue 4",December 2013,"This paper explores how a world-wide operating software solutions provider implemented environmentally sustainable business practices in response to emerging environmental concerns. Through an interpretive case study, we develop a theoretical framework that identifies four important functional affordances originating in information systems, which are required in environmental sustainability transformations as they create an actionable context in which (1) organizations can engage in a sensemaking process related to understanding emerging environmental requirements, and (2) individuals can implement environmentally sustainable work practices. Through our work, we provide several contributions, including a better understanding of IS-enabled organizational change and the types of functional affordances of information systems that are required in sustainability transformations. We describe implications relating to (1) how information systems can contribute to the creation of environmentally sustainable organizations, (2) the design of information systems to create required functional affordances, (3) the management of sustainability transformations, and (4) the further development of the concept of functional affordances in IS research.",[],"Seidel, Stefan",N/A,"Institute of Information Systems, University of Liechtenstein, Fürst-Franz-Josef-Strasse, 9490 Vaduz, Liechtenstein"
https://misq.umn.edu/misq/article/37/4/1275/569/Sensemaking-and-Sustainable-Practicing-Functional,MIS Quarterly,Sensemaking and Sustainable Practicing: Functional Affordances of Information Systems in Green Transformations1,"Volume 37, Issue 4",December 2013,"This paper explores how a world-wide operating software solutions provider implemented environmentally sustainable business practices in response to emerging environmental concerns. Through an interpretive case study, we develop a theoretical framework that identifies four important functional affordances originating in information systems, which are required in environmental sustainability transformations as they create an actionable context in which (1) organizations can engage in a sensemaking process related to understanding emerging environmental requirements, and (2) individuals can implement environmentally sustainable work practices. Through our work, we provide several contributions, including a better understanding of IS-enabled organizational change and the types of functional affordances of information systems that are required in sustainability transformations. We describe implications relating to (1) how information systems can contribute to the creation of environmentally sustainable organizations, (2) the design of information systems to create required functional affordances, (3) the management of sustainability transformations, and (4) the further development of the concept of functional affordances in IS research.",[],"Recker, Jan",N/A,"Information Systems School, Queensland University of Technology, 2 George Street, Brisbane QLD 4000, Australia"
https://misq.umn.edu/misq/article/37/4/1275/569/Sensemaking-and-Sustainable-Practicing-Functional,MIS Quarterly,Sensemaking and Sustainable Practicing: Functional Affordances of Information Systems in Green Transformations1,"Volume 37, Issue 4",December 2013,"This paper explores how a world-wide operating software solutions provider implemented environmentally sustainable business practices in response to emerging environmental concerns. Through an interpretive case study, we develop a theoretical framework that identifies four important functional affordances originating in information systems, which are required in environmental sustainability transformations as they create an actionable context in which (1) organizations can engage in a sensemaking process related to understanding emerging environmental requirements, and (2) individuals can implement environmentally sustainable work practices. Through our work, we provide several contributions, including a better understanding of IS-enabled organizational change and the types of functional affordances of information systems that are required in sustainability transformations. We describe implications relating to (1) how information systems can contribute to the creation of environmentally sustainable organizations, (2) the design of information systems to create required functional affordances, (3) the management of sustainability transformations, and (4) the further development of the concept of functional affordances in IS research.",[],"Brocke, Jan vom",N/A,"Institute of Information Systems, University of Liechtenstein, Fürst-Franz-Josef-Strasse, 9490 Vaduz, Liechtenstein"
https://misq.umn.edu/misq/article/37/4/1301/579/Assessing-the-Effects-of-Benefits-and,MIS Quarterly,Assessing the Effects of Benefits and Institutional Influences on the Continued Use of Environmentally Munificent Bypass Systems in Long-Haul Trucking1,"Volume 37, Issue 4",December 2013,"Commercial truck driving is an essential part of the national supply chain but one that adversely affects the environment. The purpose of this study is to determine the influence of the potential environmental benefits, among other factors, on continued use of bypass systems that can be discontinued at any time by a driver. The results from our study show that (1) economic benefits and industry pressures positively influence drivers’ use of bypass systems but (2) the environmental benefits of the technology do not, even though system vendors and state transportation agencies emphasize these benefits of the technology. Based on these findings, we conclude that sustainable information systems can be a viable option in a business context if usage leads to economic benefits. Our results and conclusions support the U.S. Environmental Protection Agency’s differentiation of public policy versus business perspectives on sustainable technology.",[],"Marett, Kent",N/A,"Department of Management Information Systems, College of Business, Mississippi State University, Mississippi State, MS 39762 U.S.A."
https://misq.umn.edu/misq/article/37/4/1301/579/Assessing-the-Effects-of-Benefits-and,MIS Quarterly,Assessing the Effects of Benefits and Institutional Influences on the Continued Use of Environmentally Munificent Bypass Systems in Long-Haul Trucking1,"Volume 37, Issue 4",December 2013,"Commercial truck driving is an essential part of the national supply chain but one that adversely affects the environment. The purpose of this study is to determine the influence of the potential environmental benefits, among other factors, on continued use of bypass systems that can be discontinued at any time by a driver. The results from our study show that (1) economic benefits and industry pressures positively influence drivers’ use of bypass systems but (2) the environmental benefits of the technology do not, even though system vendors and state transportation agencies emphasize these benefits of the technology. Based on these findings, we conclude that sustainable information systems can be a viable option in a business context if usage leads to economic benefits. Our results and conclusions support the U.S. Environmental Protection Agency’s differentiation of public policy versus business perspectives on sustainable technology.",[],"Otondo, Robert F.",N/A,"Department of Management Information Systems, College of Business, Mississippi State University, Mississippi State, MS 39762 U.S.A."
https://misq.umn.edu/misq/article/37/4/1301/579/Assessing-the-Effects-of-Benefits-and,MIS Quarterly,Assessing the Effects of Benefits and Institutional Influences on the Continued Use of Environmentally Munificent Bypass Systems in Long-Haul Trucking1,"Volume 37, Issue 4",December 2013,"Commercial truck driving is an essential part of the national supply chain but one that adversely affects the environment. The purpose of this study is to determine the influence of the potential environmental benefits, among other factors, on continued use of bypass systems that can be discontinued at any time by a driver. The results from our study show that (1) economic benefits and industry pressures positively influence drivers’ use of bypass systems but (2) the environmental benefits of the technology do not, even though system vendors and state transportation agencies emphasize these benefits of the technology. Based on these findings, we conclude that sustainable information systems can be a viable option in a business context if usage leads to economic benefits. Our results and conclusions support the U.S. Environmental Protection Agency’s differentiation of public policy versus business perspectives on sustainable technology.",[],"Taylor, G. Stephen",N/A,"Department of Management Information Systems, College of Business, Mississippi State University, Mississippi State, MS 39762 U.S.A."
https://misq.umn.edu/misq/article/37/4/1313/573/Motivating-Energy-Efficient-Behavior-with-Green-IS,MIS Quarterly,Motivating Energy-Efficient Behavior with Green IS: An Investigation of Goal Setting and the Role of Defaults1,"Volume 37, Issue 4",December 2013,"This study investigates the role of information systems in stimulating energy-efficient behavior in private households. We present the example of Velix, a web portal designed to motivate customers of a utility company to reduce their electricity consumption. In particular, we consider the effectiveness of goal setting functionality and defaults in influencing energy conservation behavior. For this purpose, we use the web portal as a test of the theoretical propositions underlying its design. Based on data collected from a field experiment with 1,791 electricity consumers, we test hypotheses regarding the structural relations between defaults and goals, the impact of defaults and goals on consumption behavior, and the moderating role of feedback on goal choice. Our results confirm the positive impact of goal setting on energy conservation. We show that default goals lead to statistically significant savings by affecting goal choice. However, if the default goals are set too low or too high with respect to a self-set goal, the defaults will detrimentally affect behavior. We also show that feedback on goal attainment moderates the effect of default goals on goal choice. The results extend the knowledge on goal setting and defaults and have implications for the design of effective energy feedback systems. The study’s approach, which combines hypothesis-driven work and design-oriented IS research, could serve as a blueprint for further research endeavors of this kind, particularly with regard to feedback systems based on future smart metering infrastructures.",[],"Loock, Claire-Michelle",N/A,"Information Management, ETH Zurich, Weinbergstrasse 56/58, 8092 Zurich, Switzerland"
https://misq.umn.edu/misq/article/37/4/1313/573/Motivating-Energy-Efficient-Behavior-with-Green-IS,MIS Quarterly,Motivating Energy-Efficient Behavior with Green IS: An Investigation of Goal Setting and the Role of Defaults1,"Volume 37, Issue 4",December 2013,"This study investigates the role of information systems in stimulating energy-efficient behavior in private households. We present the example of Velix, a web portal designed to motivate customers of a utility company to reduce their electricity consumption. In particular, we consider the effectiveness of goal setting functionality and defaults in influencing energy conservation behavior. For this purpose, we use the web portal as a test of the theoretical propositions underlying its design. Based on data collected from a field experiment with 1,791 electricity consumers, we test hypotheses regarding the structural relations between defaults and goals, the impact of defaults and goals on consumption behavior, and the moderating role of feedback on goal choice. Our results confirm the positive impact of goal setting on energy conservation. We show that default goals lead to statistically significant savings by affecting goal choice. However, if the default goals are set too low or too high with respect to a self-set goal, the defaults will detrimentally affect behavior. We also show that feedback on goal attainment moderates the effect of default goals on goal choice. The results extend the knowledge on goal setting and defaults and have implications for the design of effective energy feedback systems. The study’s approach, which combines hypothesis-driven work and design-oriented IS research, could serve as a blueprint for further research endeavors of this kind, particularly with regard to feedback systems based on future smart metering infrastructures.",[],"Staake, Thorsten",N/A,"MIS (Energy Efficient Systems), University of Bamberg, An der Weberei 5, 96047 Bamberg, Germany"
https://misq.umn.edu/misq/article/37/4/1313/573/Motivating-Energy-Efficient-Behavior-with-Green-IS,MIS Quarterly,Motivating Energy-Efficient Behavior with Green IS: An Investigation of Goal Setting and the Role of Defaults1,"Volume 37, Issue 4",December 2013,"This study investigates the role of information systems in stimulating energy-efficient behavior in private households. We present the example of Velix, a web portal designed to motivate customers of a utility company to reduce their electricity consumption. In particular, we consider the effectiveness of goal setting functionality and defaults in influencing energy conservation behavior. For this purpose, we use the web portal as a test of the theoretical propositions underlying its design. Based on data collected from a field experiment with 1,791 electricity consumers, we test hypotheses regarding the structural relations between defaults and goals, the impact of defaults and goals on consumption behavior, and the moderating role of feedback on goal choice. Our results confirm the positive impact of goal setting on energy conservation. We show that default goals lead to statistically significant savings by affecting goal choice. However, if the default goals are set too low or too high with respect to a self-set goal, the defaults will detrimentally affect behavior. We also show that feedback on goal attainment moderates the effect of default goals on goal choice. The results extend the knowledge on goal setting and defaults and have implications for the design of effective energy feedback systems. The study’s approach, which combines hypothesis-driven work and design-oriented IS research, could serve as a blueprint for further research endeavors of this kind, particularly with regard to feedback systems based on future smart metering infrastructures.",[],"Thiesse, Frédéric",N/A,"Information Systems Engineering, Julius-Maximilian University of Wuerzburg, Josef-Stangl-Platz 2, 97070 Wuerzburg, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
https://misq.umn.edu/misq/article/38/1/1/1550/Reliability-Generalization-of-Perceived-Ease-of,MIS Quarterly,"Reliability Generalization of Perceived Ease of Use, Perceived Usefulness, and Behavioral Intentions1","Volume 38, Issue 1",March 2014,"A reliability generalization study (a meta-analysis of reliability coefficients) was conducted on three widely studied information systems constructs from the technology acceptance model (TAM): perceived ease of use, perceived usefulness, and behavioral intentions. This form of meta-analysis summarizes the reliability coefficients of the scores on a specified scale across studies and identifies the study characteristics that influence the reliability of these scores. Reliability is a critical issue in conducting empirical research as the reliability of the scores on well-established scales can vary with study characteristics, attenuating effect sizes. In conducting this study, an extensive literature search was conducted, with 380 articles reviewed and coded to perform reliability generalization. Study characteristics, including technology, sample, and measurement characteristics, for these articles were recorded along with effect size data for the relationships among these variables. After controlling for number of items, sample size, and sampling error, differences in reliability coefficients were found with several study characteristics for the three technology acceptance constructs. The reliability coefficients of PEOU and PU were lower in hedonic contexts than in utilitarian contexts, and were higher when the originally validated scales were used as compared to when other items were substituted. Only 27 percent of the studies that provided the measurement items used the original PEOU items, while 39 percent used the original PU items. Scales that were administered in English had higher reliability coefficients for PU and BI, with a marginal effect for PEOU. Reliability differences were also found for other study characteristics, including reliability type, subject experience, and gender composition. While average reliability coefficients were high, the results show that, on average, relationships among these constructs are attenuated by 12 percent with maximum attenuation in the range of 35 to 43 percent. Implications for technology acceptance research are discussed and suggestions for addressing variation in reliability coefficients across studies are provided.",[],"Hess, Traci J.",N/A,"Operations & Information Management, Isenberg School of Management, University of Massachusetts, Amherst, Amherst, MA 01003 U.S.A"
https://misq.umn.edu/misq/article/38/1/1/1550/Reliability-Generalization-of-Perceived-Ease-of,MIS Quarterly,"Reliability Generalization of Perceived Ease of Use, Perceived Usefulness, and Behavioral Intentions1","Volume 38, Issue 1",March 2014,"A reliability generalization study (a meta-analysis of reliability coefficients) was conducted on three widely studied information systems constructs from the technology acceptance model (TAM): perceived ease of use, perceived usefulness, and behavioral intentions. This form of meta-analysis summarizes the reliability coefficients of the scores on a specified scale across studies and identifies the study characteristics that influence the reliability of these scores. Reliability is a critical issue in conducting empirical research as the reliability of the scores on well-established scales can vary with study characteristics, attenuating effect sizes. In conducting this study, an extensive literature search was conducted, with 380 articles reviewed and coded to perform reliability generalization. Study characteristics, including technology, sample, and measurement characteristics, for these articles were recorded along with effect size data for the relationships among these variables. After controlling for number of items, sample size, and sampling error, differences in reliability coefficients were found with several study characteristics for the three technology acceptance constructs. The reliability coefficients of PEOU and PU were lower in hedonic contexts than in utilitarian contexts, and were higher when the originally validated scales were used as compared to when other items were substituted. Only 27 percent of the studies that provided the measurement items used the original PEOU items, while 39 percent used the original PU items. Scales that were administered in English had higher reliability coefficients for PU and BI, with a marginal effect for PEOU. Reliability differences were also found for other study characteristics, including reliability type, subject experience, and gender composition. While average reliability coefficients were high, the results show that, on average, relationships among these constructs are attenuated by 12 percent with maximum attenuation in the range of 35 to 43 percent. Implications for technology acceptance research are discussed and suggestions for addressing variation in reliability coefficients across studies are provided.",[],"McNab, Anna L.",N/A,"Department of Management, Niagara University, Niagara University, NY 14109 U.S.A"
https://misq.umn.edu/misq/article/38/1/1/1550/Reliability-Generalization-of-Perceived-Ease-of,MIS Quarterly,"Reliability Generalization of Perceived Ease of Use, Perceived Usefulness, and Behavioral Intentions1","Volume 38, Issue 1",March 2014,"A reliability generalization study (a meta-analysis of reliability coefficients) was conducted on three widely studied information systems constructs from the technology acceptance model (TAM): perceived ease of use, perceived usefulness, and behavioral intentions. This form of meta-analysis summarizes the reliability coefficients of the scores on a specified scale across studies and identifies the study characteristics that influence the reliability of these scores. Reliability is a critical issue in conducting empirical research as the reliability of the scores on well-established scales can vary with study characteristics, attenuating effect sizes. In conducting this study, an extensive literature search was conducted, with 380 articles reviewed and coded to perform reliability generalization. Study characteristics, including technology, sample, and measurement characteristics, for these articles were recorded along with effect size data for the relationships among these variables. After controlling for number of items, sample size, and sampling error, differences in reliability coefficients were found with several study characteristics for the three technology acceptance constructs. The reliability coefficients of PEOU and PU were lower in hedonic contexts than in utilitarian contexts, and were higher when the originally validated scales were used as compared to when other items were substituted. Only 27 percent of the studies that provided the measurement items used the original PEOU items, while 39 percent used the original PU items. Scales that were administered in English had higher reliability coefficients for PU and BI, with a marginal effect for PEOU. Reliability differences were also found for other study characteristics, including reliability type, subject experience, and gender composition. While average reliability coefficients were high, the results show that, on average, relationships among these constructs are attenuated by 12 percent with maximum attenuation in the range of 35 to 43 percent. Implications for technology acceptance research are discussed and suggestions for addressing variation in reliability coefficients across studies are provided.",[],"Basoglu, K. Asli",N/A,"Department of Accounting & Management Information Systems, University of Delaware, Newark, DE 19716 U.S.A"
https://misq.umn.edu/misq/article/38/1/29/1530/Collaboration-Through-Open-Superposition-A-Theory,MIS Quarterly,Collaboration Through Open Superposition: A Theory of the Open Source Way1,"Volume 38, Issue 1",March 2014,"This paper develops and illustrates the theory of collaboration through open superposition: the process of depositing motivationally independent layers of work on top of each other over time. The theory is developed in a study of community-based free and open source software (FLOSS) development, through a research arc of discovery (participant observation), replication (two archival case studies), and theorization. The theory explains two key findings: (1) the overwhelming majority of work is accomplished with only a single programmer working on any one task, and (2) tasks that appear too large for any one individual are more likely to be deferred until they are easier rather than being undertaken through structured team work. Moreover, the theory explains how working through open superposition can lead to the discovery of a work breakdown that results in complex, functionally interdependent, work being accomplished without crippling search costs. We identify a set of socio-technical contingencies under which collaboration through open superposition is likely to be effective, including characteristics of artifacts made from information as the objects being worked on. We demonstrate the usefulness of the theory by using it to analyze difficulties in learning from FLOSS in other domains of work and in the IS function of for-profit organizations.",[],"Howison, James",N/A,"School of Information, University of Texas at Austin, 1616 Guadalupe Avenue, Austin, TX 78701 U.S.A"
https://misq.umn.edu/misq/article/38/1/29/1530/Collaboration-Through-Open-Superposition-A-Theory,MIS Quarterly,Collaboration Through Open Superposition: A Theory of the Open Source Way1,"Volume 38, Issue 1",March 2014,"This paper develops and illustrates the theory of collaboration through open superposition: the process of depositing motivationally independent layers of work on top of each other over time. The theory is developed in a study of community-based free and open source software (FLOSS) development, through a research arc of discovery (participant observation), replication (two archival case studies), and theorization. The theory explains two key findings: (1) the overwhelming majority of work is accomplished with only a single programmer working on any one task, and (2) tasks that appear too large for any one individual are more likely to be deferred until they are easier rather than being undertaken through structured team work. Moreover, the theory explains how working through open superposition can lead to the discovery of a work breakdown that results in complex, functionally interdependent, work being accomplished without crippling search costs. We identify a set of socio-technical contingencies under which collaboration through open superposition is likely to be effective, including characteristics of artifacts made from information as the objects being worked on. We demonstrate the usefulness of the theory by using it to analyze difficulties in learning from FLOSS in other domains of work and in the IS function of for-profit organizations.",[],"Crowston, Kevin",N/A,"School of Information Studies, Syracuse University, 343 Hinds Hall, Syracuse, NY 13244 U.S.A. and National Science Foundation"
https://misq.umn.edu/misq/article/38/1/51/1535/Enterprise-System-Implementation-and-Employee-Job,MIS Quarterly,Enterprise System Implementation and Employee Job Performance: Understanding the Role of Advice Networks1,"Volume 38, Issue 1",March 2014,"The implementation of enterprise systems, such as enterprise resource planning (ERP) systems, alters business processes and associated workflows, and introduces new software applications that employees must use. Employees frequently find such technology-enabled organizational change to be a major challenge. Although many challenges related to such changes have been discussed in prior work, little research has focused on post-implementation job outcomes of employees affected by such change. We draw from social network theory— specifically, advice networks—to understand a key post-implementation job outcome (i.e., job performance). We conducted a study among 87 employees, with data gathered before and after the implementation of an ERP system module in a business unit of a large organization. We found support for our hypotheses that workflow advice and software advice are associated with job performance. Further, as predicted, we found that the interactions of workflow and software get-advice, workflow and software give-advice, and software get- and give-advice were associated with job performance. This nuanced treatment of advice networks advances our understanding of post-implementation success of enterprise systems.",[],"Sykes, Tracy Ann",N/A,"Walton College of Business, University of Arkansas, Fayetteville, AR 72701 U.S.A"
https://misq.umn.edu/misq/article/38/1/51/1535/Enterprise-System-Implementation-and-Employee-Job,MIS Quarterly,Enterprise System Implementation and Employee Job Performance: Understanding the Role of Advice Networks1,"Volume 38, Issue 1",March 2014,"The implementation of enterprise systems, such as enterprise resource planning (ERP) systems, alters business processes and associated workflows, and introduces new software applications that employees must use. Employees frequently find such technology-enabled organizational change to be a major challenge. Although many challenges related to such changes have been discussed in prior work, little research has focused on post-implementation job outcomes of employees affected by such change. We draw from social network theory— specifically, advice networks—to understand a key post-implementation job outcome (i.e., job performance). We conducted a study among 87 employees, with data gathered before and after the implementation of an ERP system module in a business unit of a large organization. We found support for our hypotheses that workflow advice and software advice are associated with job performance. Further, as predicted, we found that the interactions of workflow and software get-advice, workflow and software give-advice, and software get- and give-advice were associated with job performance. This nuanced treatment of advice networks advances our understanding of post-implementation success of enterprise systems.",[],"Venkatesh, Viswanath",N/A,"Walton College of Business, University of Arkansas, Fayetteville, AR 72701 U.S.A"
https://misq.umn.edu/misq/article/38/1/51/1535/Enterprise-System-Implementation-and-Employee-Job,MIS Quarterly,Enterprise System Implementation and Employee Job Performance: Understanding the Role of Advice Networks1,"Volume 38, Issue 1",March 2014,"The implementation of enterprise systems, such as enterprise resource planning (ERP) systems, alters business processes and associated workflows, and introduces new software applications that employees must use. Employees frequently find such technology-enabled organizational change to be a major challenge. Although many challenges related to such changes have been discussed in prior work, little research has focused on post-implementation job outcomes of employees affected by such change. We draw from social network theory— specifically, advice networks—to understand a key post-implementation job outcome (i.e., job performance). We conducted a study among 87 employees, with data gathered before and after the implementation of an ERP system module in a business unit of a large organization. We found support for our hypotheses that workflow advice and software advice are associated with job performance. Further, as predicted, we found that the interactions of workflow and software get-advice, workflow and software give-advice, and software get- and give-advice were associated with job performance. This nuanced treatment of advice networks advances our understanding of post-implementation success of enterprise systems.",[],"Johnson, Jonathan L.",N/A,"Walton College of Business, University of Arkansas, Fayetteville, AR 72701 U.S.A"
https://misq.umn.edu/misq/article/38/1/73/1552/Explaining-Data-Driven-Document-Classifications1,MIS Quarterly,Explaining Data-Driven Document Classifications1,"Volume 38, Issue 1",March 2014,"Many document classification applications require human understanding of the reasons for data-driven classification decisions by managers, client-facing employees, and the technical team. Predictive models treat documents as data to be classified, and document data are characterized by very high dimensionality, often with tens of thousands to millions of variables (words). Unfortunately, due to the high dimensionality, understanding the decisions made by document classifiers is very difficult. This paper begins by extending the most relevant prior theoretical model of explanations for intelligent systems to account for some missing elements. The main theoretical contribution is the definition of a new sort of explanation as a minimal set of words (terms, generally), such that removing all words within this set from the document changes the predicted class from the class of interest. We present an algorithm to find such explanations, as well as a framework to assess such an algorithm’s performance. We demonstrate the value of the new approach with a case study from a real-world document classification task: classifying web pages as containing objectionable content, with the goal of allowing advertisers to choose not to have their ads appear on those pages. A second empirical demonstration on news-story topic classification shows the explanations to be concise and document-specific, and to be capable of providing understanding of the exact reasons for the classification decisions, of the workings of the classification models, and of the business application itself. We also illustrate how explaining the classifications of documents can help to improve data quality and model performance.",[],"Martens, David",N/A,"Department of Engineering Management, Faculty of Applied Economics, University of Antwerp, Prinsstraat 13, 2018 Antwerp, Belgium"
https://misq.umn.edu/misq/article/38/1/73/1552/Explaining-Data-Driven-Document-Classifications1,MIS Quarterly,Explaining Data-Driven Document Classifications1,"Volume 38, Issue 1",March 2014,"Many document classification applications require human understanding of the reasons for data-driven classification decisions by managers, client-facing employees, and the technical team. Predictive models treat documents as data to be classified, and document data are characterized by very high dimensionality, often with tens of thousands to millions of variables (words). Unfortunately, due to the high dimensionality, understanding the decisions made by document classifiers is very difficult. This paper begins by extending the most relevant prior theoretical model of explanations for intelligent systems to account for some missing elements. The main theoretical contribution is the definition of a new sort of explanation as a minimal set of words (terms, generally), such that removing all words within this set from the document changes the predicted class from the class of interest. We present an algorithm to find such explanations, as well as a framework to assess such an algorithm’s performance. We demonstrate the value of the new approach with a case study from a real-world document classification task: classifying web pages as containing objectionable content, with the goal of allowing advertisers to choose not to have their ads appear on those pages. A second empirical demonstration on news-story topic classification shows the explanations to be concise and document-specific, and to be capable of providing understanding of the exact reasons for the classification decisions, of the workings of the classification models, and of the business application itself. We also illustrate how explaining the classifications of documents can help to improve data quality and model performance.",[],"Provost, Foster",N/A,"Department of Information, Operations and Management Sciences, Stern School of Business, New York University, 44 West 4th Street, New York, NY 10012-1126 U.S.A"
https://misq.umn.edu/misq/article/38/1/101/1525/Social-Media-Traditional-Media-and-Music-Sales1,MIS Quarterly,"Social Media, Traditional Media, and Music Sales1","Volume 38, Issue 1",March 2014,"Motivated by the growing importance of social media, this paper examines the relationship between new media, old media, and sales in the context of the music industry. In particular, we study the interplay between blog buzz, radio play, and music sales at both the album and song levels of analysis. We employ the panel vector autoregression (PVAR) methodology, an extension of vector autoregression to panel data. We find that radio play is consistently and positively related to future sales at both the song and album levels. Blog buzz, however, is not related to album sales and negatively related to song sales, suggesting that sales displacement due to free online sampling dominates any positive word-of-mouth effects of song buzz on sales. Further, the negative relationship between song buzz and sales is stronger for niche music relative to mainstream music, and for less popular songs within albums. We discuss the implications of these results for both research and practice regarding the role of new media in the music industry.",[],"Dewan, Sanjeev",N/A,"Paul Merage School of Business, University of California, Irvine, Irvine, CA 92697 U.S.A"
https://misq.umn.edu/misq/article/38/1/101/1525/Social-Media-Traditional-Media-and-Music-Sales1,MIS Quarterly,"Social Media, Traditional Media, and Music Sales1","Volume 38, Issue 1",March 2014,"Motivated by the growing importance of social media, this paper examines the relationship between new media, old media, and sales in the context of the music industry. In particular, we study the interplay between blog buzz, radio play, and music sales at both the album and song levels of analysis. We employ the panel vector autoregression (PVAR) methodology, an extension of vector autoregression to panel data. We find that radio play is consistently and positively related to future sales at both the song and album levels. Blog buzz, however, is not related to album sales and negatively related to song sales, suggesting that sales displacement due to free online sampling dominates any positive word-of-mouth effects of song buzz on sales. Further, the negative relationship between song buzz and sales is stronger for niche music relative to mainstream music, and for less popular songs within albums. We discuss the implications of these results for both research and practice regarding the role of new media in the music industry.",[],"Ramaprasad, Jui",N/A,"Desautels Faculty of Management, McGill University, Montreal, QC H3A 1G5 Canada"
https://misq.umn.edu/misq/article/38/1/123/1529/Content-Sharing-in-a-Social-Broadcasting,MIS Quarterly,Content Sharing in a Social Broadcasting Environment: Evidence from Twitter1,"Volume 38, Issue 1",March 2014,"The rise of social broadcasting technologies has greatly facilitated open access to information worldwide, not only by powering decentralized information production and consumption, but also by expediting information diffusion through social interactions like content sharing. Voluntary information sharing by users in the context of Twitter, the predominant social broadcasting site, is studied by modeling both the technology and user behavior. A detailed data set about the official content-sharing function on Twitter, called retweet, is collected and the statistical relationships between users’ social network characteristics and their retweeting acts are documented. A two-stage consumption-sharing model is then estimated using the conditional maximum likelihood estimatio (MLE) method. The empirical results convincingly support our hypothesis that weak ties (in the form of unidirectional links) are more likely to engage in the social exchange process of content sharing. Specifically, we find that after a median quality tweet (as defined in the sample) is consumed, the likelihood that a unidirectional follower will retweet is 3.1 percentage point higher than the likelihood that a bidirectional follower will do so.",[],"Shi, Zhan",N/A,"Department of Information Systems, W. P. Carey School of Business, Arizona State University, Tempe, AZ 85287 U.S.A"
https://misq.umn.edu/misq/article/38/1/123/1529/Content-Sharing-in-a-Social-Broadcasting,MIS Quarterly,Content Sharing in a Social Broadcasting Environment: Evidence from Twitter1,"Volume 38, Issue 1",March 2014,"The rise of social broadcasting technologies has greatly facilitated open access to information worldwide, not only by powering decentralized information production and consumption, but also by expediting information diffusion through social interactions like content sharing. Voluntary information sharing by users in the context of Twitter, the predominant social broadcasting site, is studied by modeling both the technology and user behavior. A detailed data set about the official content-sharing function on Twitter, called retweet, is collected and the statistical relationships between users’ social network characteristics and their retweeting acts are documented. A two-stage consumption-sharing model is then estimated using the conditional maximum likelihood estimatio (MLE) method. The empirical results convincingly support our hypothesis that weak ties (in the form of unidirectional links) are more likely to engage in the social exchange process of content sharing. Specifically, we find that after a median quality tweet (as defined in the sample) is consumed, the likelihood that a unidirectional follower will retweet is 3.1 percentage point higher than the likelihood that a bidirectional follower will do so.",[],"Rui, Huaxia",N/A,"Simon Graduate School of Business, University of Rochester, Rochester, NY 14627 U.S.A"
https://misq.umn.edu/misq/article/38/1/123/1529/Content-Sharing-in-a-Social-Broadcasting,MIS Quarterly,Content Sharing in a Social Broadcasting Environment: Evidence from Twitter1,"Volume 38, Issue 1",March 2014,"The rise of social broadcasting technologies has greatly facilitated open access to information worldwide, not only by powering decentralized information production and consumption, but also by expediting information diffusion through social interactions like content sharing. Voluntary information sharing by users in the context of Twitter, the predominant social broadcasting site, is studied by modeling both the technology and user behavior. A detailed data set about the official content-sharing function on Twitter, called retweet, is collected and the statistical relationships between users’ social network characteristics and their retweeting acts are documented. A two-stage consumption-sharing model is then estimated using the conditional maximum likelihood estimatio (MLE) method. The empirical results convincingly support our hypothesis that weak ties (in the form of unidirectional links) are more likely to engage in the social exchange process of content sharing. Specifically, we find that after a median quality tweet (as defined in the sample) is consumed, the likelihood that a unidirectional follower will retweet is 3.1 percentage point higher than the likelihood that a bidirectional follower will do so.",[],"Whinston, Andrew B.",N/A,"McCombs School of Business, The University of Texas at Austin, Austin, TX 78712 U.S.A"
https://misq.umn.edu/misq/article/38/1/143/1533/Contribution-Behavior-in-Virtual-Communities,MIS Quarterly,"Contribution Behavior in Virtual Communities: Cognitive, Emotional, and Social Influences1","Volume 38, Issue 1",March 2014,"The long-term viability of virtual communities depends critically on contribution behavior by their members. We deepen and extend prior research by conceptualizing contributions to virtual communities in terms of small friendship group–referent intentional actions. Specifically, we investigate cognitive, emotional, and social determinants of shared we-intentions and their consequences for member contribution behavior to the small friendship group to which they belong within a larger community. Using multiple measurement sources and a longitudinal quasi-experimental design, we show that group norms and social identity, as well as attitudes and anticipated emotions, contribute to the development of behavioral desires, which in turn influence we-intentions. In addition, subjective norms are less effective than either group norms or social identity in encouraging contribution behavior. Finally, members’ experience levels positively moderate the relationship between we-intentions and contribution behaviors, and differences between collectivistic versus individualistic orientations moderate the effects of social identity and anticipated emotions on the desire to contribute to one’s friendship group in the virtual community. Tests for methods biases were conducted, as well as rival hypotheses. These findings have significant research and managerial implications.",[],"Tsai, Hsien-Tung",N/A,"College of Business, National Taipei University, 151 University Road, San Shia, Taipei 237 Taiwan"
https://misq.umn.edu/misq/article/38/1/143/1533/Contribution-Behavior-in-Virtual-Communities,MIS Quarterly,"Contribution Behavior in Virtual Communities: Cognitive, Emotional, and Social Influences1","Volume 38, Issue 1",March 2014,"The long-term viability of virtual communities depends critically on contribution behavior by their members. We deepen and extend prior research by conceptualizing contributions to virtual communities in terms of small friendship group–referent intentional actions. Specifically, we investigate cognitive, emotional, and social determinants of shared we-intentions and their consequences for member contribution behavior to the small friendship group to which they belong within a larger community. Using multiple measurement sources and a longitudinal quasi-experimental design, we show that group norms and social identity, as well as attitudes and anticipated emotions, contribute to the development of behavioral desires, which in turn influence we-intentions. In addition, subjective norms are less effective than either group norms or social identity in encouraging contribution behavior. Finally, members’ experience levels positively moderate the relationship between we-intentions and contribution behaviors, and differences between collectivistic versus individualistic orientations moderate the effects of social identity and anticipated emotions on the desire to contribute to one’s friendship group in the virtual community. Tests for methods biases were conducted, as well as rival hypotheses. These findings have significant research and managerial implications.",[],"Bagozzi, Richard P.",N/A,"Ross School of Business, University of Michigan, 701 Tappan Street, Room D7209, Ann Arbor, MI 48109-1234 U.S.A"
https://misq.umn.edu/misq/article/38/1/165/1537/Theorization-and-Translation-in-Information,MIS Quarterly,Theorization and Translation in Information Technology Institutionalization: Evidence from Danish Home Care1,"Volume 38, Issue 1",March 2014,N/A,[],"Nielsen, Jeppe Agger",N/A,"Center for IS Management and Center for Organization, Management & Administration, Department of Political Science, Aalborg University, Fibigerstraede 1, 9220 Aalborg, Denmark"
https://misq.umn.edu/misq/article/38/1/165/1537/Theorization-and-Translation-in-Information,MIS Quarterly,Theorization and Translation in Information Technology Institutionalization: Evidence from Danish Home Care1,"Volume 38, Issue 1",March 2014,N/A,[],"Mathiassen, Lars",N/A,"Center for Process Innovation, Computer Information Systems, J. Mack Robinson College of Business, Georgia State University, 35 Broad Street, NW, Suite 427, Atlanta, GA 30303 U.S.A"
https://misq.umn.edu/misq/article/38/1/165/1537/Theorization-and-Translation-in-Information,MIS Quarterly,Theorization and Translation in Information Technology Institutionalization: Evidence from Danish Home Care1,"Volume 38, Issue 1",March 2014,N/A,[],"Newell, Sue",N/A,"Department of Management, Bentley University, 175 Forest Street, Waltham, MA 02452 U.S.A. and Warwick Business School, Coventry CV4 7AL U.K"
https://misq.umn.edu/misq/article/38/1/187/1542/Leveraging-Philanthropic-Behavior-for-Customer,MIS Quarterly,Leveraging Philanthropic Behavior for Customer Support: The Case of user Support Forums1,"Volume 38, Issue 1",March 2014,N/A,[],"Jabr, Wael",N/A,"Haskayne School of Business, University of Calgary, Calgary, AB, T2N 1N4 Canada"
https://misq.umn.edu/misq/article/38/1/187/1542/Leveraging-Philanthropic-Behavior-for-Customer,MIS Quarterly,Leveraging Philanthropic Behavior for Customer Support: The Case of user Support Forums1,"Volume 38, Issue 1",March 2014,N/A,[],"Mookerjee, Radha",N/A,"Naveen Jindal School of Management, University of Texas at Dallas, Richardson, TX 75080 U.S.A."
https://misq.umn.edu/misq/article/38/1/187/1542/Leveraging-Philanthropic-Behavior-for-Customer,MIS Quarterly,Leveraging Philanthropic Behavior for Customer Support: The Case of user Support Forums1,"Volume 38, Issue 1",March 2014,N/A,[],"Tan, Yong",N/A,"Michael G. Foster School of Business, University of Washington, Seattle, WA 98195 U.S.A."
https://misq.umn.edu/misq/article/38/1/187/1542/Leveraging-Philanthropic-Behavior-for-Customer,MIS Quarterly,Leveraging Philanthropic Behavior for Customer Support: The Case of user Support Forums1,"Volume 38, Issue 1",March 2014,N/A,[],"Mookerjee, Vijay S.",N/A,"Naveen Jindal School of Management, University of Texas at Dallas, Richardson, TX 75080 U.S.A."
https://misq.umn.edu/misq/article/38/1/209/1545/Swift-Guanxi-in-Online-Marketplaces-The-Role-of,MIS Quarterly,Swift Guanxi in Online Marketplaces: The Role of Computer-Mediated Communication Technologies1,"Volume 38, Issue 1",March 2014,"The concept of guanxi (i.e., a close and pervasive interpersonal relationship) has received little attention in the literature on online marketplaces, perhaps due to their impersonal nature. However, we propose that computer-mediated communication (CMC) technologies can mimic traditional interactive face-to-face communications, thus enabling a form of guanxi in online marketplaces. Extending the literature on traditional guanxi, we herein introduce the concept of swift guanxi, conceptualized as the buyer’s perception of a swiftly formed interpersonal relationship with a seller, which consists of mutual understanding, reciprocal favors, and relationship harmony.Integrating theories of CMC and guanxi, we develop a model that explains how a set of CMC tools (i.e., instant messaging, message box, feedback system) facilitate repeat transactions with sellers by building swift guanxi and trust through interactivity and presence (social presence and telepresence) with sellers. Longitudinal data from 338 buyers in TaoBao, China’s leading online marketplace, support our structural model, showing that the buyers’ effective use of CMC tools enable swift guanxi and trust by enhancing the buyers’ perceptions of interactivity and presence. In turn, swift guanxi and trust predict buyers’ repurchase intentions and their actual repurchases from sellers. We discuss the implications of swift guanxi in online marketplaces with the aid of CMC technologies.",[],"Ou, Carol Xiaojuan",N/A,"Department of Information Management, Tilburg University, Warandelaan 2, 5000 LE Tilburg, The Netherlands"
https://misq.umn.edu/misq/article/38/1/209/1545/Swift-Guanxi-in-Online-Marketplaces-The-Role-of,MIS Quarterly,Swift Guanxi in Online Marketplaces: The Role of Computer-Mediated Communication Technologies1,"Volume 38, Issue 1",March 2014,"The concept of guanxi (i.e., a close and pervasive interpersonal relationship) has received little attention in the literature on online marketplaces, perhaps due to their impersonal nature. However, we propose that computer-mediated communication (CMC) technologies can mimic traditional interactive face-to-face communications, thus enabling a form of guanxi in online marketplaces. Extending the literature on traditional guanxi, we herein introduce the concept of swift guanxi, conceptualized as the buyer’s perception of a swiftly formed interpersonal relationship with a seller, which consists of mutual understanding, reciprocal favors, and relationship harmony.Integrating theories of CMC and guanxi, we develop a model that explains how a set of CMC tools (i.e., instant messaging, message box, feedback system) facilitate repeat transactions with sellers by building swift guanxi and trust through interactivity and presence (social presence and telepresence) with sellers. Longitudinal data from 338 buyers in TaoBao, China’s leading online marketplace, support our structural model, showing that the buyers’ effective use of CMC tools enable swift guanxi and trust by enhancing the buyers’ perceptions of interactivity and presence. In turn, swift guanxi and trust predict buyers’ repurchase intentions and their actual repurchases from sellers. We discuss the implications of swift guanxi in online marketplaces with the aid of CMC technologies.",[],"Pavlou, Paul A.",N/A,"The Fox School of Business, Temple University, 1801 Liacouras Walk, Philadelphia, PA 19122-6083 U.S.A"
https://misq.umn.edu/misq/article/38/1/209/1545/Swift-Guanxi-in-Online-Marketplaces-The-Role-of,MIS Quarterly,Swift Guanxi in Online Marketplaces: The Role of Computer-Mediated Communication Technologies1,"Volume 38, Issue 1",March 2014,"The concept of guanxi (i.e., a close and pervasive interpersonal relationship) has received little attention in the literature on online marketplaces, perhaps due to their impersonal nature. However, we propose that computer-mediated communication (CMC) technologies can mimic traditional interactive face-to-face communications, thus enabling a form of guanxi in online marketplaces. Extending the literature on traditional guanxi, we herein introduce the concept of swift guanxi, conceptualized as the buyer’s perception of a swiftly formed interpersonal relationship with a seller, which consists of mutual understanding, reciprocal favors, and relationship harmony.Integrating theories of CMC and guanxi, we develop a model that explains how a set of CMC tools (i.e., instant messaging, message box, feedback system) facilitate repeat transactions with sellers by building swift guanxi and trust through interactivity and presence (social presence and telepresence) with sellers. Longitudinal data from 338 buyers in TaoBao, China’s leading online marketplace, support our structural model, showing that the buyers’ effective use of CMC tools enable swift guanxi and trust by enhancing the buyers’ perceptions of interactivity and presence. In turn, swift guanxi and trust predict buyers’ repurchase intentions and their actual repurchases from sellers. We discuss the implications of swift guanxi in online marketplaces with the aid of CMC technologies.",[],"Davison, Robert M.",N/A,"Department of Information Systems, College of Business, City University of Hong Kong, 83 Tat Chee Avenue, Hong Kong"
https://misq.umn.edu/misq/article/38/1/231/1547/Complementary-Online-Services-in-Competitive,MIS Quarterly,Complementary Online Services in Competitive Markets: Maintaining Profitability in the Presence of Network Effects1,"Volume 38, Issue 1",March 2014,"A growing number of firms are strategically utilizing information technology and the Internet to provide online services to consumers who buy their products. Online services differ from traditional services because they often promote interactivity among users and exhibit positive network effects. While the service increases the value obtained by consumers, network effects are known to intensify price competition and thus may reduce firms’ profits. In this paper, we model the competition between two firms that sell a differentiated product when each firm can offer a complementary online service to its customers. We derive the market equilibrium and determine how firms should adjust their strategies to account for network effects. We find that when the service exhibits network effects, a firm’s decision whether or not to offer the service depends on both the competitor’s decision and the competitor’s service quality. When the service does not exhibit network effects, this is not the case. In addition, we show that a firm can benefit from the technological ability to offer the service, and from an increase in the strength of network effects or in the market size of the service, only when the value customers derive from the direct functionalities (those that do not rely on the network) of the service are sufficiently high. As a result, a firm’s investment in the direct functionalities of its service increases with the strength of network effects of the service as long as the marginal development cost is not too high. Finally, we show that inefficiencies in terms of the number of firms offering the service as well as the total number of service users may prevail.",[],"Etzion, Hila",N/A,"Department of Technology and Operations, Stephen M. Ross School of Business, University of Michigan, Ann Arbor, MI 48103 U.S.A"
https://misq.umn.edu/misq/article/38/1/231/1547/Complementary-Online-Services-in-Competitive,MIS Quarterly,Complementary Online Services in Competitive Markets: Maintaining Profitability in the Presence of Network Effects1,"Volume 38, Issue 1",March 2014,"A growing number of firms are strategically utilizing information technology and the Internet to provide online services to consumers who buy their products. Online services differ from traditional services because they often promote interactivity among users and exhibit positive network effects. While the service increases the value obtained by consumers, network effects are known to intensify price competition and thus may reduce firms’ profits. In this paper, we model the competition between two firms that sell a differentiated product when each firm can offer a complementary online service to its customers. We derive the market equilibrium and determine how firms should adjust their strategies to account for network effects. We find that when the service exhibits network effects, a firm’s decision whether or not to offer the service depends on both the competitor’s decision and the competitor’s service quality. When the service does not exhibit network effects, this is not the case. In addition, we show that a firm can benefit from the technological ability to offer the service, and from an increase in the strength of network effects or in the market size of the service, only when the value customers derive from the direct functionalities (those that do not rely on the network) of the service are sufficiently high. As a result, a firm’s investment in the direct functionalities of its service increases with the strength of network effects of the service as long as the marginal development cost is not too high. Finally, we show that inefficiencies in terms of the number of firms offering the service as well as the total number of service users may prevail.",[],"Pang, Min-Seok",N/A,"Department of Information Systems and Operations Management, School of Management, George Mason University, Fairfax, VA 22030 U.S.A"
https://misq.umn.edu/misq/article/38/1/249/1543/Distributed-Cognition-in-Software-Design-An,MIS Quarterly,Distributed Cognition in Software Design: An Experimental Investigation of the Role of Design Patterns and Collaboration1,"Volume 38, Issue 1",March 2014,"Software design is a knowledge intensive task that constitutes a critical part of the software development process. Using a controlled experiment involving software practitioners, this research examines two potentially useful mechanisms for improving the software design process. Specifically, this study examines the impact of structural distribution of cognition through design patterns and social distribution of cognition through collaborating pairs on design outcomes. The results indicate that the use of design patterns as external cognitive artifacts improves design quality, reduces time taken to solve a design problem, and leads to higher participant satisfaction. Collaborating pairs of software designers were compared to participants working alone but whose efforts were conjointly considered as the best and second-best members of nominal pairs. It was found that paired designers produced higher quality designs compared with the second-best members of nominal pairs, did not differ from the best member of a nominal pair, but took more time to complete a design task than either member of a nominal pair. The results also indicate that the availability of design patterns raises the performance level of the second-best member of a nominal pair, in terms of quality, and reduces task completion time when compared with a pair not using design patterns. Finally, paired designers were found to experience higher levels of task satisfaction when compared with individuals. Implications for research and practice are discussed.",[],"Mangalaraj, George",N/A,"College of Business and Technology, Western Illinois University, Macomb, IL 61455 U.S.A"
https://misq.umn.edu/misq/article/38/1/249/1543/Distributed-Cognition-in-Software-Design-An,MIS Quarterly,Distributed Cognition in Software Design: An Experimental Investigation of the Role of Design Patterns and Collaboration1,"Volume 38, Issue 1",March 2014,"Software design is a knowledge intensive task that constitutes a critical part of the software development process. Using a controlled experiment involving software practitioners, this research examines two potentially useful mechanisms for improving the software design process. Specifically, this study examines the impact of structural distribution of cognition through design patterns and social distribution of cognition through collaborating pairs on design outcomes. The results indicate that the use of design patterns as external cognitive artifacts improves design quality, reduces time taken to solve a design problem, and leads to higher participant satisfaction. Collaborating pairs of software designers were compared to participants working alone but whose efforts were conjointly considered as the best and second-best members of nominal pairs. It was found that paired designers produced higher quality designs compared with the second-best members of nominal pairs, did not differ from the best member of a nominal pair, but took more time to complete a design task than either member of a nominal pair. The results also indicate that the availability of design patterns raises the performance level of the second-best member of a nominal pair, in terms of quality, and reduces task completion time when compared with a pair not using design patterns. Finally, paired designers were found to experience higher levels of task satisfaction when compared with individuals. Implications for research and practice are discussed.",[],"Nerur, Sridhar",N/A,"College of Business Administration, University of Texas at Arlington, Arlington, TX 76019-0437 U.S.A"
https://misq.umn.edu/misq/article/38/1/249/1543/Distributed-Cognition-in-Software-Design-An,MIS Quarterly,Distributed Cognition in Software Design: An Experimental Investigation of the Role of Design Patterns and Collaboration1,"Volume 38, Issue 1",March 2014,"Software design is a knowledge intensive task that constitutes a critical part of the software development process. Using a controlled experiment involving software practitioners, this research examines two potentially useful mechanisms for improving the software design process. Specifically, this study examines the impact of structural distribution of cognition through design patterns and social distribution of cognition through collaborating pairs on design outcomes. The results indicate that the use of design patterns as external cognitive artifacts improves design quality, reduces time taken to solve a design problem, and leads to higher participant satisfaction. Collaborating pairs of software designers were compared to participants working alone but whose efforts were conjointly considered as the best and second-best members of nominal pairs. It was found that paired designers produced higher quality designs compared with the second-best members of nominal pairs, did not differ from the best member of a nominal pair, but took more time to complete a design task than either member of a nominal pair. The results also indicate that the availability of design patterns raises the performance level of the second-best member of a nominal pair, in terms of quality, and reduces task completion time when compared with a pair not using design patterns. Finally, paired designers were found to experience higher levels of task satisfaction when compared with individuals. Implications for research and practice are discussed.",[],"Mahapatra, RadhaKanta",N/A,"College of Business Administration, University of Texas at Arlington, Arlington, TX 76019-0437 U.S.A"
https://misq.umn.edu/misq/article/38/1/249/1543/Distributed-Cognition-in-Software-Design-An,MIS Quarterly,Distributed Cognition in Software Design: An Experimental Investigation of the Role of Design Patterns and Collaboration1,"Volume 38, Issue 1",March 2014,"Software design is a knowledge intensive task that constitutes a critical part of the software development process. Using a controlled experiment involving software practitioners, this research examines two potentially useful mechanisms for improving the software design process. Specifically, this study examines the impact of structural distribution of cognition through design patterns and social distribution of cognition through collaborating pairs on design outcomes. The results indicate that the use of design patterns as external cognitive artifacts improves design quality, reduces time taken to solve a design problem, and leads to higher participant satisfaction. Collaborating pairs of software designers were compared to participants working alone but whose efforts were conjointly considered as the best and second-best members of nominal pairs. It was found that paired designers produced higher quality designs compared with the second-best members of nominal pairs, did not differ from the best member of a nominal pair, but took more time to complete a design task than either member of a nominal pair. The results also indicate that the availability of design patterns raises the performance level of the second-best member of a nominal pair, in terms of quality, and reduces task completion time when compared with a pair not using design patterns. Finally, paired designers were found to experience higher levels of task satisfaction when compared with individuals. Implications for research and practice are discussed.",[],"Price, Kenneth H.",N/A,"College of Business Administration, University of Texas at Arlington, Arlington, TX 76019-0437 U.S.A"
https://misq.umn.edu/misq/article/38/1/305/1540/Information-Technology-Capability-and-Firm,MIS Quarterly,Information Technology Capability and Firm Performance: Contradictory Findings and Their Possible Causes1,"Volume 38, Issue 1",March 2014,"Several studies support the positive link between information technology capability and firm performance, including Bharadwaj (2000) and Santhanam and Hartono (2003), which appeared in MIS Quarterly. We conducted a study to see if this link is still statistically significant. It is now over a decade since the first study was published, during which several significant developments in the IT industry have taken place. Unlike the 1990s, when proprietary information systems prevailed, the 2000s are characterized by more standardized and homogeneous information systems and with the rapid adoption of ERP and web technologies. Thus, we attempted to reexamine the link between IT capability and firm performance with data from the 2000s. Surprisingly, the results of our current analysis showed no significant link between IT capability and firm performance. Contrary to earlier studies, IT leader firms in our study didn’t show better financial performance than control firms. We discuss several possible causes for the change in findings and present an in-depth comparison in business performance between the two groups—IT leader and control—over a period extending from 1991 to 2007.",[],"Chae, Ho-Chang",N/A,"Department of Finance, Accounting, and Information Systems, Gary E. West College of Business, West Liberty University, West Liberty, WV 26074 U.S.A"
https://misq.umn.edu/misq/article/38/1/305/1540/Information-Technology-Capability-and-Firm,MIS Quarterly,Information Technology Capability and Firm Performance: Contradictory Findings and Their Possible Causes1,"Volume 38, Issue 1",March 2014,"Several studies support the positive link between information technology capability and firm performance, including Bharadwaj (2000) and Santhanam and Hartono (2003), which appeared in MIS Quarterly. We conducted a study to see if this link is still statistically significant. It is now over a decade since the first study was published, during which several significant developments in the IT industry have taken place. Unlike the 1990s, when proprietary information systems prevailed, the 2000s are characterized by more standardized and homogeneous information systems and with the rapid adoption of ERP and web technologies. Thus, we attempted to reexamine the link between IT capability and firm performance with data from the 2000s. Surprisingly, the results of our current analysis showed no significant link between IT capability and firm performance. Contrary to earlier studies, IT leader firms in our study didn’t show better financial performance than control firms. We discuss several possible causes for the change in findings and present an in-depth comparison in business performance between the two groups—IT leader and control—over a period extending from 1991 to 2007.",[],"Koh, Chang E.",N/A,"Department of Information Technology and Decision Sciences, College of Business, University of North Texas, Denton, TX 76203-5017 U.S.A"
https://misq.umn.edu/misq/article/38/1/305/1540/Information-Technology-Capability-and-Firm,MIS Quarterly,Information Technology Capability and Firm Performance: Contradictory Findings and Their Possible Causes1,"Volume 38, Issue 1",March 2014,"Several studies support the positive link between information technology capability and firm performance, including Bharadwaj (2000) and Santhanam and Hartono (2003), which appeared in MIS Quarterly. We conducted a study to see if this link is still statistically significant. It is now over a decade since the first study was published, during which several significant developments in the IT industry have taken place. Unlike the 1990s, when proprietary information systems prevailed, the 2000s are characterized by more standardized and homogeneous information systems and with the rapid adoption of ERP and web technologies. Thus, we attempted to reexamine the link between IT capability and firm performance with data from the 2000s. Surprisingly, the results of our current analysis showed no significant link between IT capability and firm performance. Contrary to earlier studies, IT leader firms in our study didn’t show better financial performance than control firms. We discuss several possible causes for the change in findings and present an in-depth comparison in business performance between the two groups—IT leader and control—over a period extending from 1991 to 2007.",[],"Prybutok, Victor R.",N/A,"Department of Information Technology and Decision Sciences, College of Business, University of North Texas, Denton, TX 76203-5017 U.S.A"
https://misq.umn.edu/misq/article/38/1/275/1548/What-s-Different-about-Social-Media-Networks-A,MIS Quarterly,What’s Different about Social Media Networks? A Framework and Research Agenda1,"Volume 38, Issue 1",March 2014,"In recent years, we have witnessed the rapid proliferation and widespread adoption of a new class of information technologies, commonly known as social media. Researchers often rely on social network analysis (SNA) when attempting to understand these technologies, often without considering how the novel capabilities of social media platforms might affect the underlying theories of SNA, which were developed primarily through studies of offline social networks. This article outlines several key differences between traditional offline social networks and online social media networks by juxtaposing an established typology of social network research with a well-regarded definition of social media platforms that articulates four key features. The results show that at four major points of intersection, social media has considerable theoretical implications for SNA. In exploring these points of intersection, this study outlines a series of theoretically distinct research questions for SNA in social media contexts. These points of intersection offer considerable opportunities for researchers to investigate the theoretical implications introduced by social media and lay the groundwork for a robust social media agenda potentially spanning multiple disciplines.",[],"Kane, Gerald C.",N/A,"Boston College, 140 Commonwealth Avenue, Chestnut Hill, MA 02467 U.S.A"
https://misq.umn.edu/misq/article/38/1/275/1548/What-s-Different-about-Social-Media-Networks-A,MIS Quarterly,What’s Different about Social Media Networks? A Framework and Research Agenda1,"Volume 38, Issue 1",March 2014,"In recent years, we have witnessed the rapid proliferation and widespread adoption of a new class of information technologies, commonly known as social media. Researchers often rely on social network analysis (SNA) when attempting to understand these technologies, often without considering how the novel capabilities of social media platforms might affect the underlying theories of SNA, which were developed primarily through studies of offline social networks. This article outlines several key differences between traditional offline social networks and online social media networks by juxtaposing an established typology of social network research with a well-regarded definition of social media platforms that articulates four key features. The results show that at four major points of intersection, social media has considerable theoretical implications for SNA. In exploring these points of intersection, this study outlines a series of theoretically distinct research questions for SNA in social media contexts. These points of intersection offer considerable opportunities for researchers to investigate the theoretical implications introduced by social media and lay the groundwork for a robust social media agenda potentially spanning multiple disciplines.",[],"Alavi, Maryam",N/A,"Emory University, 1300 Clifton Road NE, Atlanta, GA 30322 U.S.A"
https://misq.umn.edu/misq/article/38/1/275/1548/What-s-Different-about-Social-Media-Networks-A,MIS Quarterly,What’s Different about Social Media Networks? A Framework and Research Agenda1,"Volume 38, Issue 1",March 2014,"In recent years, we have witnessed the rapid proliferation and widespread adoption of a new class of information technologies, commonly known as social media. Researchers often rely on social network analysis (SNA) when attempting to understand these technologies, often without considering how the novel capabilities of social media platforms might affect the underlying theories of SNA, which were developed primarily through studies of offline social networks. This article outlines several key differences between traditional offline social networks and online social media networks by juxtaposing an established typology of social network research with a well-regarded definition of social media platforms that articulates four key features. The results show that at four major points of intersection, social media has considerable theoretical implications for SNA. In exploring these points of intersection, this study outlines a series of theoretically distinct research questions for SNA in social media contexts. These points of intersection offer considerable opportunities for researchers to investigate the theoretical implications introduced by social media and lay the groundwork for a robust social media agenda potentially spanning multiple disciplines.",[],"Labianca, Giuseppe (Joe)",N/A,"LINKS Center, Gatton College of Business and Economics, University of Kentucky, Lexington, KY 40506 U.S.A"
https://misq.umn.edu/misq/article/38/1/275/1548/What-s-Different-about-Social-Media-Networks-A,MIS Quarterly,What’s Different about Social Media Networks? A Framework and Research Agenda1,"Volume 38, Issue 1",March 2014,"In recent years, we have witnessed the rapid proliferation and widespread adoption of a new class of information technologies, commonly known as social media. Researchers often rely on social network analysis (SNA) when attempting to understand these technologies, often without considering how the novel capabilities of social media platforms might affect the underlying theories of SNA, which were developed primarily through studies of offline social networks. This article outlines several key differences between traditional offline social networks and online social media networks by juxtaposing an established typology of social network research with a well-regarded definition of social media platforms that articulates four key features. The results show that at four major points of intersection, social media has considerable theoretical implications for SNA. In exploring these points of intersection, this study outlines a series of theoretically distinct research questions for SNA in social media contexts. These points of intersection offer considerable opportunities for researchers to investigate the theoretical implications introduced by social media and lay the groundwork for a robust social media agenda potentially spanning multiple disciplines.",[],"Borgatti, Stephen P.",N/A,"LINKS Center, Gatton College of Business and Economics, University of Kentucky, Lexington, KY 40506 U.S.A"
https://misq.umn.edu/misq/article/38/1/iii/1527/Editor-s-CommentsDesign-Science-Research-in-Top,MIS Quarterly,Editor’s CommentsDesign Science Research in Top Information Systems Journals of Technology and Management Head,"Volume 38, Issue 1",March 2014,N/A,[],"Goes, Paulo B.",N/A,Management Information Systems Eller College of Management University of Arizona
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Günther, Wendy Arianne",N/A,"KIN Center for Digital Innovation, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Joshi, Mayur",N/A,"Telfer School of Management, University of Ottawa Ottawa, Ontario, Canada"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory,MIS Quarterly,Trustworthiness in Computational Theory Construction: Dimensionalization and Category Surfacing1,N/A,N/A,"In this methods article, we unpack how researchers can foster trustworthiness in dimensionalization and category surfacing (DCS), a key method family within the genre of computational theory construction (CTC). Information systems (IS), management, and organizational scholars are increasingly leveraging DCS tools such as topic modeling, word embeddings, and clustering to surface latent categories and dimensions from textual data for theory construction. Yet they struggle because evaluations of such research often default to transparency, operationalized as replicability and accountability, which obscures the analytical choices that actually make DCS research rigorous. In this study, we recast transparency as a means toward trustworthiness. We treat researchers’ analytical moves as the primary unit of methodological reasoning in how they design, conduct, and disclose their choices across research phases. We develop a framework that authors, reviewers, and editors can use to construct and evaluate DCS research. The framework specifies how trustworthiness arises from the interplay of two research design choices: primacy to theoretical versus practice lexicons, and whether the content of texts or the structure of the corpus carries the theoretical load. We articulate expectations for conduct and disclosure across these design choices, clarifying how proportionate reasoning anchors trustworthiness. We conclude with implications for advancing trustworthiness within the broader CTC community and across other computational approaches to research.",[],"Constantinides, Panos",N/A,"Alliance Manchester Business School, The University of Manchester, Manchester, United Kingdom"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Ostern, Nadine Kathrin",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Maruping, Likoebe M.",N/A,"Center for Digital Innovation, J. Mack Robinson College of Business, Georgia State University, Atlanta, GA U.S.A."
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Kowalkiewicz, Marek",N/A,"Centre for Future Enterprise, School of Management, Queensland University of Technology, Brisbane, QLD, Australia"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Weking, Jörg",N/A,"TUM School for Computation, Information and Technology, Technical University of Munich, Munich, Germany"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and,MIS Quarterly,When Bots Evaluate Humans: Delegation to Bots and the Reshaping of Authority,N/A,N/A,"Information systems scholars typically frame the delegation of tasks to AI-based bots as a means of improving efficiency and supporting decision-making. Yet when evaluative tasks are delegated to supervisory bots, authority is transferred to them as well. This study examines how such authority, once conferred on bots, is contested and redistributed by the humans whose work is subject to the bot’s evaluation. We investigate this process in the context of Wikipedia, an online peer-production community in which an antivandalism bot was given the task of autonomously reverting vandalized, i.e., maliciously edited, contributions. Drawing on the concept of performative authority, we traced how community members negotiated the enactment of the bot’s newfound authority with the bot’s developers through authority-negotiation design moves. We found that these moves were the outcome of a recursive process marked by the redistribution of authority across actors in decisions on vandalism, the institutionalization of structures that shaped how authority was influenced, and the evolving actions of the bot itself, which triggered renewed contestation over time. Within the context of a peer-production community, our study offers fresh insight into how authority delegated to bots evolves as a dynamic cocreation process after delegation and how humans reclaim authority once it has been ceded to a supervisory bot. We shed light on how humans preserve authority, professional discretion, and the meaningfulness of their work in AI-mediated settings.",[],"Thatcher, Jason Bennett",N/A,"Leeds School of Business, University of Colorado Boulder, Boulder, CO, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Rai, Arun",N/A,"Center for Digital Innovation and Computer Information Systems Department, Robinson College of Business, Georgia State University, Atlanta, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Tian, Jing",N/A,"Department of Supply Chain and Information System, Smeal College of Business, Pennsylvania State University, University Park, PA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence,MIS Quarterly,FAIR: A Design Theory for Artificial Intelligence Fairness,N/A,N/A,"Artificial intelligence (AI)-automated decision systems encounter persistent, interdependent, and dynamic fairness tensions that traditional one-off interventions cannot resolve. Because these tensions persist due to interdependence and dynamic interaction, organizations require both a theory of the problem to explain their persistence and a theory of the solution to prescribe how they can be managed. Our design theory, FAIR (Fairness Adaptation through AI-augmented Responsiveness), provides a theory of the problem by reframing AI fairness as a sociotechnical paradox constituted within AI artifacts that automate decision tasks, through interdependent organizational, technical, and governance choices and their interaction with regulatory mandates and societal norms. Synthesizing four fairness perspectives (Ethics, Organizational Justice, Economic Fairness, and Rawlsian Justice), we identify three metatheoretical dimensions (principles, goals, foci) and show that the interdependence within and among these dimensions is the root, endogenous source that constitutes paradoxical fairness tensions. Building on this diagnosis, FAIR provides a theory of the solution by specifying an organizational capability grounded in three design foundations. First, the paradox lens motivates iterative adaptive cycles (Surfacing and Resolving) to continually surface and resolve AI fairness tensions. Second, design science in information systems and computer science distinguishes AI artifacts (the “what”) from the actors (the “who”) responsible for adapting them, establishing the basis for complementary human–AI agent collaboration in the adaptive cycles: AI agents execute monitoring to surface and refinement to resolve tensions, whereas human agents specify objectives, adjudicate trade-offs, and exercise contextual judgment and oversight. Third, the managing-with-AI literature informs how this human–AI agent collaboration should be governed. These foundations yield two reinforcing mechanisms: (i) artifact-level adaptation, achieved through structured human–AI agent collaboration, within and across the layers of the AI decision pipeline—Representation (data), Learning (model), and Calibration (decision); and (ii) portfolio-level, risk-tiered federated governance that structures how human–AI agent collaboration scales across tasks and artifacts, balancing process standardization with configuration choices and human control with AI autonomy based on task risk. Enabled by organizational “fairness complements”—namely, human skills to work with AI agents and structured stakeholder feedback—this sociotechnical design provides organizations with a sustained capability to harmonize global coherence and local flexibility in the responsive adaptation of AI fairness.",[],"Xue, Ling",N/A,"Department of Management Information Systems, Terry College of Business, University of Georgia, Athens, GA, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Kuang, Junwei",N/A,"School of Business Administration, South China University of Technology, Guangzhou, Guangdong, China"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Xie, Jiaheng",N/A,"Department of Accounting and Management Information Systems, Lerner College of Business & Economics, University of Delaware, Newark, DE, U.S.A"
https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An,MIS Quarterly,Symptoms and Their Temporal Distributions: An Interpretable AI Approach for Depression Detection in Social Media,N/A,N/A,"Depression is a common mental disorder involving a depressed mood or loss of pleasure for long periods, which induces grave financial and societal ramifications. Social media-based depression detection is an effective method for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few studies explain this decision based on the importance of linguistic or demographic features, these explanations do not directly relate to depression diagnosis criteria that are based on symptoms. To fill this gap, we develop a Focused Temporal Prototype Network (FTPNet) to detect depression and provide interpretations based on depressive symptoms as well as their temporal distributions. Extensive evaluations using large-scale datasets show that FTPNet outperforms comprehensive benchmark methods with an F1-score of 0.864. Our result also reveals fine-grained and emerging manifestations of depressive symptoms, such as sharing admiration for a different life, that are unnoted in traditional depression surveys like the Patient Health Questionnaire-9 (PHQ-9). We further conduct a user study to demonstrate improved interpretability over the benchmark. This study contributes to the Information Systems (IS) literature by introducing an interpretable depression detection approach that models the temporal distribution of depressive symptoms. In practice, multiple stakeholders, such as social media platforms and volunteers, can apply our approach to identify depressed users and deliver targeted assistance.",[],"Yan, Zhijun",N/A,"School of Management, Beijing Institute of Technology, Beijing, China"
