{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb5005a",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install selenium pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d07d1f8",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1003917",
      "metadata": {},
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "# Initialize the chrome webdriver\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# Starting URL - browse by year page\n",
        "START_URL = 'https://misq.umn.edu/misq/issue/browse-by-year'\n",
        "\n",
        "# Years to scrape (2010 to 2025)\n",
        "START_YEAR = 2010\n",
        "END_YEAR = 2025\n",
        "\n",
        "# Save CSV file in the same directory as this notebook (MIS_Quarterly folder)\n",
        "OUT_FILE = os.path.join(os.getcwd(), 'MISQ_Issues.csv')\n",
        "print(f\"CSV file will be saved to: {OUT_FILE}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\\n\")\n",
        "data = []\n",
        "\n",
        "def write_to_csv(rows):\n",
        "    file_exists = os.path.exists(OUT_FILE) and os.path.getsize(OUT_FILE) > 0\n",
        "    with open(OUT_FILE, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"Title\", \"URL\", \"Volume Issue\", \"Vol Issue Year\"])\n",
        "            print(f\"Created CSV file: {OUT_FILE}\")\n",
        "        writer.writerows(rows)\n",
        "        file.flush()  # Ensure data is written immediately\n",
        "    print(f\"  ✓ Saved {len(rows)} articles to {OUT_FILE}\")\n",
        "\n",
        "def scrape_issue_page(issue_url, vol_issue, year):\n",
        "    \"\"\"Scrape articles from a single issue page\"\"\"\n",
        "    try:\n",
        "        driver.get(issue_url)\n",
        "        driver.implicitly_wait(15)\n",
        "        time.sleep(2)\n",
        "        \n",
        "        print(f\"  Page loaded: {driver.title}\")\n",
        "        \n",
        "        # Find all article links - try multiple selectors\n",
        "        articles = []\n",
        "        \n",
        "        # Try different selectors for article links\n",
        "        selectors = [\n",
        "            'a[href*=\"/misq/vol\"]',\n",
        "            'a[href*=\"/article\"]',\n",
        "            '.article-title a',\n",
        "            '.article a',\n",
        "            'h3 a',\n",
        "            'h4 a',\n",
        "            '.title a',\n",
        "            'article a',\n",
        "            '.entry-title a',\n",
        "            'li a[href*=\"/article\"]',\n",
        "            'div.article a',\n",
        "            'table a[href*=\"/article\"]'\n",
        "        ]\n",
        "        \n",
        "        print(\"  Searching for articles...\")\n",
        "        for selector in selectors:\n",
        "            try:\n",
        "                found = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                if found:\n",
        "                    print(f\"    Selector '{selector}': Found {len(found)} links\")\n",
        "                    # Filter for article links (not issue links)\n",
        "                    filtered = [a for a in found if a.get_attribute('href') and \n",
        "                               ('/article' in a.get_attribute('href') or \n",
        "                                ('/misq/vol' in a.get_attribute('href') and '/issue' not in a.get_attribute('href')))]\n",
        "                    if filtered:\n",
        "                        print(f\"      → {len(filtered)} are article links\")\n",
        "                        articles.extend(filtered)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        \n",
        "        # Remove duplicates\n",
        "        seen_urls = set()\n",
        "        unique_articles = []\n",
        "        for article in articles:\n",
        "            try:\n",
        "                url = article.get_attribute('href')\n",
        "                if url and url not in seen_urls:\n",
        "                    seen_urls.add(url)\n",
        "                    unique_articles.append(article)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        if not unique_articles:\n",
        "            # Fallback: find all links and filter\n",
        "            print(\"  Trying fallback: checking all links...\")\n",
        "            all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "            for link in all_links:\n",
        "                try:\n",
        "                    url = link.get_attribute('href') or ''\n",
        "                    if url and ('/article' in url or ('/misq/vol' in url and '/issue' not in url)) and url not in seen_urls:\n",
        "                        seen_urls.add(url)\n",
        "                        unique_articles.append(link)\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        print(f\"  Total unique articles found: {len(unique_articles)}\")\n",
        "        \n",
        "        rows = []\n",
        "        for article in unique_articles:\n",
        "            try:\n",
        "                article_url = article.get_attribute('href')\n",
        "                if not article_url:\n",
        "                    continue\n",
        "                    \n",
        "                # Make sure URL is absolute\n",
        "                if article_url.startswith('/'):\n",
        "                    article_url = 'https://misq.umn.edu' + article_url\n",
        "                \n",
        "                if not article_url.startswith('http'):\n",
        "                    continue\n",
        "                \n",
        "                # Get article title\n",
        "                article_title = article.text.strip()\n",
        "                if not article_title or len(article_title) < 5:\n",
        "                    # Try to get title from parent or nearby element\n",
        "                    try:\n",
        "                        parent = article.find_element(By.XPATH, './..')\n",
        "                        article_title = parent.text.strip()\n",
        "                    except:\n",
        "                        try:\n",
        "                            # Try sibling or nearby heading\n",
        "                            heading = article.find_element(By.XPATH, './preceding-sibling::h3 | ./preceding-sibling::h4 | ./following-sibling::h3 | ./following-sibling::h4')\n",
        "                            article_title = heading.text.strip()\n",
        "                        except:\n",
        "                            article_title = \"N/A\"\n",
        "                \n",
        "                if article_url and article_title and article_title != \"N/A\" and len(article_title) > 5:\n",
        "                    rows.append([article_title, article_url, vol_issue, year])\n",
        "                    print(f\"    ✓ {article_title[:60]}...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"    Error extracting article: {e}\")\n",
        "                continue\n",
        "        \n",
        "        if rows:\n",
        "            write_to_csv(rows)\n",
        "            return len(rows)\n",
        "        else:\n",
        "            print(f\"  ⚠ No articles found on this page\")\n",
        "            # Debug: show page structure\n",
        "            try:\n",
        "                page_text = driver.find_element(By.TAG_NAME, 'body').text[:300]\n",
        "                print(f\"  Page content preview: {page_text[:200]}...\")\n",
        "            except:\n",
        "                pass\n",
        "            return 0\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error scraping issue page: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 0\n",
        "\n",
        "def scrape_year_page(year_url, year):\n",
        "    \"\"\"Scrape all issues from a year page\"\"\"\n",
        "    try:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Navigating to year page: {year_url}\")\n",
        "        driver.get(year_url)\n",
        "        driver.implicitly_wait(15)\n",
        "        time.sleep(3)  # Give page more time to load\n",
        "        \n",
        "        print(f\"Page title: {driver.title}\")\n",
        "        print(f\"Current URL: {driver.current_url}\")\n",
        "        \n",
        "        # Debug: Print some page content to understand structure\n",
        "        try:\n",
        "            page_text = driver.find_element(By.TAG_NAME, 'body').text[:500]\n",
        "            print(f\"Page content preview: {page_text}...\")\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Find all issue links - try comprehensive approach\n",
        "        issue_links = []\n",
        "        \n",
        "        # Try different selectors for issue links\n",
        "        selectors = [\n",
        "            'a[href*=\"/misq/vol\"]',\n",
        "            'a[href*=\"/vol\"]',\n",
        "            '.issue-link a',\n",
        "            '.issue a',\n",
        "            'h2 a',\n",
        "            'h3 a',\n",
        "            'h4 a',\n",
        "            'li a',\n",
        "            '.volume a',\n",
        "            'article a',\n",
        "            'div a[href*=\"/vol\"]',\n",
        "            'table a[href*=\"/vol\"]'\n",
        "        ]\n",
        "        \n",
        "        print(\"\\nTrying to find issue links...\")\n",
        "        for selector in selectors:\n",
        "            try:\n",
        "                links = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                if links:\n",
        "                    print(f\"  Selector '{selector}': Found {len(links)} links\")\n",
        "                    # Filter for actual issue links\n",
        "                    filtered = [l for l in links if l.get_attribute('href') and ('/misq/vol' in l.get_attribute('href') or '/vol' in l.get_attribute('href'))]\n",
        "                    if filtered:\n",
        "                        print(f\"    → {len(filtered)} are issue links\")\n",
        "                        issue_links.extend(filtered)\n",
        "            except Exception as e:\n",
        "                print(f\"  Selector '{selector}': Error - {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Remove duplicates\n",
        "        seen_urls = set()\n",
        "        unique_issue_links = []\n",
        "        for link in issue_links:\n",
        "            try:\n",
        "                url = link.get_attribute('href')\n",
        "                if url and url not in seen_urls:\n",
        "                    seen_urls.add(url)\n",
        "                    unique_issue_links.append(link)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        print(f\"\\nTotal unique issue links found: {len(unique_issue_links)}\")\n",
        "        \n",
        "        if not unique_issue_links:\n",
        "            # Fallback: find ALL links and filter\n",
        "            print(\"Trying fallback: checking all links on page...\")\n",
        "            all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "            print(f\"Total links on page: {len(all_links)}\")\n",
        "            \n",
        "            for link in all_links[:50]:  # Check first 50 links\n",
        "                try:\n",
        "                    url = link.get_attribute('href') or ''\n",
        "                    text = link.text.strip()\n",
        "                    if url and ('/misq/vol' in url or '/vol' in url) and url not in seen_urls:\n",
        "                        print(f\"  Found issue link: {text[:50]} -> {url}\")\n",
        "                        seen_urls.add(url)\n",
        "                        unique_issue_links.append(link)\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        # Extract unique issue URLs with metadata\n",
        "        unique_issues = {}\n",
        "        for link in unique_issue_links:\n",
        "            try:\n",
        "                url = link.get_attribute('href')\n",
        "                if not url:\n",
        "                    continue\n",
        "                    \n",
        "                # Make sure URL is absolute\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://misq.umn.edu' + url\n",
        "                \n",
        "                # Extract volume/issue from URL or text\n",
        "                link_text = link.text.strip()\n",
        "                \n",
        "                # Try to extract from URL\n",
        "                match = re.search(r'vol[^\\d]*(\\d+)[^\\d]*issue[^\\d]*(\\d+)', url, re.I)\n",
        "                if match:\n",
        "                    vol_issue = f\"Vol {match.group(1)}, Issue {match.group(2)}\"\n",
        "                elif link_text and len(link_text) > 3:\n",
        "                    vol_issue = link_text\n",
        "                else:\n",
        "                    # Extract from URL path\n",
        "                    parts = url.split('/')\n",
        "                    vol_issue = parts[-1] if parts else f\"Vol {year}\"\n",
        "                \n",
        "                unique_issues[url] = vol_issue\n",
        "                print(f\"  Issue: {vol_issue} -> {url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing link: {e}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing {len(unique_issues)} issues for year {year}...\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        if len(unique_issues) == 0:\n",
        "            print(f\"WARNING: No issues found for year {year}!\")\n",
        "            print(\"Page HTML snippet:\")\n",
        "            try:\n",
        "                html_snippet = driver.page_source[:1000]\n",
        "                print(html_snippet)\n",
        "            except:\n",
        "                pass\n",
        "            return 0\n",
        "        \n",
        "        total_articles = 0\n",
        "        for issue_url, vol_issue in unique_issues.items():\n",
        "            print(f\"\\n{'─'*60}\")\n",
        "            print(f\"Scraping: {vol_issue}\")\n",
        "            print(f\"URL: {issue_url}\")\n",
        "            count = scrape_issue_page(issue_url, vol_issue, str(year))\n",
        "            total_articles += count\n",
        "            print(f\"  → Found {count} articles\")\n",
        "            time.sleep(1)  # Be respectful\n",
        "        \n",
        "        return total_articles\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping year page {year_url}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 0\n",
        "\n",
        "# Main scraping logic\n",
        "try:\n",
        "    driver.get(START_URL)\n",
        "    driver.implicitly_wait(15)\n",
        "    time.sleep(2)\n",
        "    \n",
        "    print(\"Starting MIS Quarterly scraper (2010-2025)...\")\n",
        "    print(f\"Browse page: {START_URL}\\n\")\n",
        "    \n",
        "    # Find year links for 2010-2025\n",
        "    year_links = {}\n",
        "    \n",
        "    print(\"Searching for year links on browse-by-year page...\")\n",
        "    print(f\"Page title: {driver.title}\")\n",
        "    print(f\"Current URL: {driver.current_url}\\n\")\n",
        "    \n",
        "    # Get all links and filter by year\n",
        "    all_links = driver.find_elements(By.TAG_NAME, 'a')\n",
        "    print(f\"Total links found on page: {len(all_links)}\")\n",
        "    \n",
        "    # First pass: look for links with year in URL or text\n",
        "    for link in all_links:\n",
        "        try:\n",
        "            url = link.get_attribute('href') or ''\n",
        "            text = link.text.strip()\n",
        "            \n",
        "            # Make URL absolute if relative\n",
        "            if url.startswith('/'):\n",
        "                url = 'https://misq.umn.edu' + url\n",
        "            \n",
        "            # Check if link contains year in URL or text\n",
        "            for year in range(START_YEAR, END_YEAR + 1):\n",
        "                year_str = str(year)\n",
        "                if (year_str in url or year_str in text) and url.startswith('http'):\n",
        "                    if year not in year_links:\n",
        "                        year_links[year] = url\n",
        "                        print(f\"  ✓ Found year {year}: {text[:40]} -> {url}\")\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\nFound {len(year_links)} year links directly from page\")\n",
        "    \n",
        "    # If we didn't find enough year links, try to construct URLs\n",
        "    if len(year_links) < (END_YEAR - START_YEAR + 1) / 2:  # If less than half found\n",
        "        print(\"\\nTrying to construct year URLs...\")\n",
        "        # Common patterns for year pages\n",
        "        base_patterns = [\n",
        "            'https://misq.umn.edu/misq/issue/browse-by-year/{}',\n",
        "            'https://misq.umn.edu/misq/issue/{}',\n",
        "            'https://misq.umn.edu/misq/vol/{}'\n",
        "        ]\n",
        "        \n",
        "        for year in range(START_YEAR, END_YEAR + 1):\n",
        "            if year in year_links:\n",
        "                continue  # Skip if already found\n",
        "                \n",
        "            for pattern in base_patterns:\n",
        "                test_url = pattern.format(year)\n",
        "                try:\n",
        "                    driver.get(test_url)\n",
        "                    time.sleep(1)\n",
        "                    if '404' not in driver.title.lower() and 'not found' not in driver.title.lower():\n",
        "                        year_links[year] = test_url\n",
        "                        print(f\"  ✓ Constructed year {year}: {test_url}\")\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "    \n",
        "    # Go back to browse page\n",
        "    driver.get(START_URL)\n",
        "    time.sleep(2)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Total year links found: {len(year_links)}\")\n",
        "    print(f\"Years: {sorted(year_links.keys())}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Scrape each year\n",
        "    total_articles_scraped = 0\n",
        "    for year in sorted(year_links.keys()):\n",
        "        if START_YEAR <= year <= END_YEAR:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Scraping year {year}\")\n",
        "            print(f\"{'='*60}\")\n",
        "            count = scrape_year_page(year_links[year], year)\n",
        "            total_articles_scraped += count\n",
        "            print(f\"Year {year}: {count} articles scraped\")\n",
        "            time.sleep(2)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Scraping complete!\")\n",
        "    print(f\"Total articles scraped: {total_articles_scraped}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Exception: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04506403",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4af3fb56",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a63cb4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "16710dbf",
      "metadata": {},
      "source": [
        "## STEP - 2 : EXTRACTING AUTHOR LEVEL DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6a1ad51d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[701/800] https://misq.umn.edu/misq/article/43/3/695/1790/The-Sociotechnical-Axis-of-Cohesion-for-the-is\n",
            "   Saved: 4 author rows\n",
            "[702/800] https://misq.umn.edu/misq/article/43/3/721/1788/Designing-Real-Time-Feedback-for-Bidders-in\n",
            "   Saved: 3 author rows\n",
            "[703/800] https://misq.umn.edu/misq/article/43/3/745/1765/Operationalizing-Regulatory-Focus-in-the-Digital\n",
            "   Saved: 3 author rows\n",
            "[704/800] https://misq.umn.edu/misq/article/43/3/765/1775/The-Right-Music-at-the-Right-Time-Adaptive\n",
            "   Saved: 3 author rows\n",
            "[705/800] https://misq.umn.edu/misq/article/43/3/787/1779/Optimal-Market-Entry-Timing-for-Successive\n",
            "   Saved: 3 author rows\n",
            "[706/800] https://misq.umn.edu/misq/article/43/3/807/1760/Using-Polynomial-Modeling-to-Understand-Service\n",
            "   Saved: 3 author rows\n",
            "[707/800] https://misq.umn.edu/misq/article/43/3/827/1781/Mobile-App-Recommendation-An-Involvement-Enhanced\n",
            "   Saved: 4 author rows\n",
            "[708/800] https://misq.umn.edu/misq/article/43/3/851/1784/An-fMRI-Exploration-of-Information-Processing-in\n",
            "   Saved: 4 author rows\n",
            "[709/800] https://misq.umn.edu/misq/article/43/3/873/1785/Institutional-Logics-and-Pluralistic-Responses-to\n",
            "   Saved: 4 author rows\n",
            "[710/800] https://misq.umn.edu/misq/article/43/3/903/1754/Children-s-Internet-Addiction-Family-to-Work\n",
            "   Saved: 5 author rows\n",
            "[711/800] https://misq.umn.edu/misq/article/43/3/929/1759/Can-Outsourcing-of-Information-Technology-Foster\n",
            "   Saved: 2 author rows\n",
            "[712/800] https://misq.umn.edu/misq/article/43/3/951/1763/Developer-Centrality-and-the-Impact-of-Value\n",
            "   Saved: 3 author rows\n",
            "[713/800] https://misq.umn.edu/misq/article/43/3/977/1766/Contextual-Explanation-Alternative-Approaches-and\n",
            "   Saved: 1 author rows\n",
            "[714/800] https://misq.umn.edu/misq/article/43/3/1007/1776/Technology-as-Routine-Capability1\n",
            "   Saved: 1 author rows\n",
            "[715/800] https://misq.umn.edu/misq/article/43/3/1025/1780/Says-Who-The-Effects-of-Presentation-Format-and\n",
            "   Saved: 2 author rows\n",
            "[716/800] https://misq.umn.edu/misq/article/43/3/iii/1757/Editor-s-CommentsThe-First-Revision\n",
            "   Saved: 1 author rows\n",
            "[717/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory\n",
            "   Saved: 3 author rows\n",
            "[718/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and\n",
            "   Saved: 5 author rows\n",
            "[719/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence\n",
            "   Saved: 3 author rows\n",
            "[720/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An\n",
            "   Saved: 3 author rows\n",
            "[721/800] https://misq.umn.edu/misq/article/43/4/1041/1773/A-Time-Based-Dynamic-Synchronization-Policy-for\n",
            "   Saved: 2 author rows\n",
            "[722/800] https://misq.umn.edu/misq/article/43/4/1059/1777/Shared-or-Dedicated-Infrastructures-On-the-Impact\n",
            "   Saved: 4 author rows\n",
            "[723/800] https://misq.umn.edu/misq/article/43/4/1081/1786/Governance-and-ICT4D-Initiative-Success-A\n",
            "   Saved: 4 author rows\n",
            "[724/800] https://misq.umn.edu/misq/article/43/4/1105/1792/Using-Eye-Tracking-to-Expose-Cognitive-Processes\n",
            "   Saved: 3 author rows\n",
            "[725/800] https://misq.umn.edu/misq/article/43/4/1127/1795/Playing-to-the-Crowd-Digital-Visibility-and-the\n",
            "   Saved: 2 author rows\n",
            "[726/800] https://misq.umn.edu/misq/article/43/4/1143/1789/Do-Search-Engines-Influence-Media-Piracy-Evidence\n",
            "   Saved: 3 author rows\n",
            "[727/800] https://misq.umn.edu/misq/article/43/4/1155/1797/How-Technology-Afforded-Practices-at-the-Micro\n",
            "   Saved: 2 author rows\n",
            "[728/800] https://misq.umn.edu/misq/article/43/4/1177/1799/The-Role-of-Affordances-in-the\n",
            "   Saved: 3 author rows\n",
            "[729/800] https://misq.umn.edu/misq/article/43/4/1201/1793/Holistic-Archetypes-of-IT-Outsourcing-Strategy-A\n",
            "   Saved: 4 author rows\n",
            "[730/800] https://misq.umn.edu/misq/article/43/4/1227/1772/A-Potato-Salad-with-a-Lemon-Twist-Using-a-Supply\n",
            "   Saved: 3 author rows\n",
            "[731/800] https://misq.umn.edu/misq/article/43/4/1249/1774/Optimizing-and-Satisficing-The-Interplay-Between\n",
            "   Saved: 3 author rows\n",
            "[732/800] https://misq.umn.edu/misq/article/43/4/1279/1778/Theorizing-the-Digital-Object1\n",
            "   Saved: 2 author rows\n",
            "[733/800] https://misq.umn.edu/misq/article/43/4/1303/1783/Followership-in-an-Open-Source-Software-Project\n",
            "   Saved: 4 author rows\n",
            "[734/800] https://misq.umn.edu/misq/article/43/4/1321/1787/Reducing-Recommender-System-Biases-An\n",
            "Verification detected. Solve it in the opened browser window; I'll continue once the article loads.\n",
            "   Saved: 4 author rows\n",
            "[735/800] https://misq.umn.edu/misq/article/43/4/1343/1791/Fake-News-on-Social-Media-People-Believe-What-They\n",
            "   Saved: 3 author rows\n",
            "[736/800] https://misq.umn.edu/misq/article/43/4/iii/1782/Editor-s-CommentsDeveloping-Virtuous-Reviewers\n",
            "   Saved: 1 author rows\n",
            "[737/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory\n",
            "   Saved: 3 author rows\n",
            "[738/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and\n",
            "   Saved: 5 author rows\n",
            "[739/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence\n",
            "   Saved: 3 author rows\n",
            "[740/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An\n",
            "   Saved: 3 author rows\n",
            "[741/800] https://misq.umn.edu/misq/article/44/1/1/132/Complexity-and-Information-Systems-Research-in-the\n",
            "   Saved: 4 author rows\n",
            "[742/800] https://misq.umn.edu/misq/article/44/1/185/168/Connecting-Systems-Data-and-People-A\n",
            "   Saved: 3 author rows\n",
            "[743/800] https://misq.umn.edu/misq/article/44/1/iii/129/Editor-s-CommentsProactively-Attending-to\n",
            "   Saved: 1 author rows\n",
            "[744/800] https://misq.umn.edu/misq/article/44/1/19/171/The-Dynamics-of-Drift-in-Digitized-Processes1\n",
            "   Saved: 4 author rows\n",
            "[745/800] https://misq.umn.edu/misq/article/44/1/49/184/Taming-Complexity-in-Search-Matching-Two-Sided\n",
            "   Saved: 4 author rows\n",
            "[746/800] https://misq.umn.edu/misq/article/44/1/85/177/Organized-Complexity-of-Digital-Business-Strategy\n",
            "   Saved: 2 author rows\n",
            "[747/800] https://misq.umn.edu/misq/article/44/1/129/148/Digitization-and-Phase-Transitions-in-Platform\n",
            "   Saved: 3 author rows\n",
            "[748/800] https://misq.umn.edu/misq/article/44/1/155/192/The-Evolution-of-Information-Systems-Architecture\n",
            "   Saved: 4 author rows\n",
            "[749/800] https://misq.umn.edu/misq/article/44/1/201/170/Trajectories-of-Repeated-Readmissions-of-Chronic\n",
            "   Saved: 2 author rows\n",
            "[750/800] https://misq.umn.edu/misq/article/44/1/227/173/Chronic-Disease-Management-How-IT-and-Analytics\n",
            "   Saved: 4 author rows\n",
            "[751/800] https://misq.umn.edu/misq/article/44/1/257/128/Go-To-YouTube-and-Call-Me-in-the-Morning-Use-of\n",
            "   Saved: 4 author rows\n",
            "[752/800] https://misq.umn.edu/misq/article/44/1/285/130/A-Data-Analytics-Framework-for-Smart-Asthma\n",
            "   Saved: 3 author rows\n",
            "[753/800] https://misq.umn.edu/misq/article/44/1/305/135/A-Comprehensive-Analysis-of-Triggers-and-Risk\n",
            "   Saved: 2 author rows\n",
            "[754/800] https://misq.umn.edu/misq/article/44/1/351/153/Examining-How-Chronically-Ill-Patients-Reactions\n",
            "   Saved: 3 author rows\n",
            "[755/800] https://misq.umn.edu/misq/article/44/1/391/169/The-Effects-of-Participating-in-a-Physician-Driven\n",
            "   Saved: 3 author rows\n",
            "[756/800] https://misq.umn.edu/misq/article/44/1/421/188/Cascading-Feedback-A-Longitudinal-Study-of-a\n",
            "   Saved: 4 author rows\n",
            "[757/800] https://misq.umn.edu/misq/article/44/1/451/172/IT-Enabled-Self-Monitoring-for-Chronic-Disease\n",
            "   Saved: 2 author rows\n",
            "[758/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory\n",
            "   Saved: 3 author rows\n",
            "[759/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and\n",
            "   Saved: 5 author rows\n",
            "[760/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence\n",
            "   Saved: 3 author rows\n",
            "[761/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An\n",
            "   Saved: 3 author rows\n",
            "[762/800] https://misq.umn.edu/misq/article/44/2/509/433/Digital-First-The-Ontological-Reversal-and-New\n",
            "   Saved: 3 author rows\n",
            "[763/800] https://misq.umn.edu/misq/article/44/2/525/437/When-Statistical-Significance-Is-Not-Enough\n",
            "   Saved: 3 author rows\n",
            "[764/800] https://misq.umn.edu/misq/article/44/2/561/456/The-Impact-of-Customer-Valuation-Uncertainty-on\n",
            "   Saved: 1 author rows\n",
            "[765/800] https://misq.umn.edu/misq/article/44/2/605/446/ICT-for-Development-in-Rural-India-A-Longitudinal\n",
            "   Saved: 3 author rows\n",
            "[766/800] https://misq.umn.edu/misq/article/44/2/631/452/The-Effect-of-Piracy-Website-Blocking-on-Consumer\n",
            "   Saved: 4 author rows\n",
            "[767/800] https://misq.umn.edu/misq/article/44/2/661/458/For-Startups-Adaptability-and-Mentor-Network\n",
            "   Saved: 2 author rows\n",
            "[768/800] https://misq.umn.edu/misq/article/44/2/699/442/Patient-Provider-Engagement-and-its-Impact-on\n",
            "   Saved: 5 author rows\n",
            "[769/800] https://misq.umn.edu/misq/article/44/2/725/450/Separate-Versus-Joint-Evaluation-The-Roles-of\n",
            "   Saved: 4 author rows\n",
            "[770/800] https://misq.umn.edu/misq/article/44/2/747/454/Is-There-a-Genetic-Basis-for-Information-Search\n",
            "   Saved: 2 author rows\n",
            "[771/800] https://misq.umn.edu/misq/article/44/2/771/426/Framing-Communication-How-Agenda-Alignment-and\n",
            "   Saved: 2 author rows\n",
            "[772/800] https://misq.umn.edu/misq/article/44/2/809/430/The-Bright-and-Dark-Sides-of-Technostress-A-Mixed\n",
            "   Saved: 3 author rows\n",
            "[773/800] https://misq.umn.edu/misq/article/44/2/857/434/Optimal-Asset-Transfer-in-IT-Outsourcing\n",
            "   Saved: 3 author rows\n",
            "[774/800] https://misq.umn.edu/misq/article/44/2/907/439/Is-Cybersecurity-a-Team-Sport-A-Multilevel\n",
            "   Saved: 3 author rows\n",
            "[775/800] https://misq.umn.edu/misq/article/44/2/933/445/Finding-People-with-Emotional-Distress-in-Online\n",
            "   Saved: 6 author rows\n",
            "[776/800] https://misq.umn.edu/misq/article/44/2/957/449/Complementarity-and-Cannibalization-of-Offline-to\n",
            "   Saved: 4 author rows\n",
            "[777/800] https://misq.umn.edu/misq/article/44/2/iii/429/Editor-s-CommentsThe-COVID-19-Pandemic-Building\n",
            "   Saved: 1 author rows\n",
            "[778/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory\n",
            "   Saved: 3 author rows\n",
            "[779/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and\n",
            "   Saved: 5 author rows\n",
            "[780/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence\n",
            "   Saved: 3 author rows\n",
            "[781/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An\n",
            "   Saved: 3 author rows\n",
            "[782/800] https://misq.umn.edu/misq/article/44/3/983/1817/Information-Technology-Identity-A-Key-Determinant\n",
            "   Saved: 4 author rows\n",
            "[783/800] https://misq.umn.edu/misq/article/44/3/1023/1802/Achieving-Effective-Use-When-Digitalizing-Work-The\n",
            "   Saved: 4 author rows\n",
            "[784/800] https://misq.umn.edu/misq/article/44/3/1049/1800/Altruism-or-Shrewd-Business-Implications-of\n",
            "   Saved: 4 author rows\n",
            "[785/800] https://misq.umn.edu/misq/article/44/3/1073/1813/Value-of-Local-Showrooms-to-Online-Competitors1\n",
            "   Saved: 3 author rows\n",
            "[786/800] https://misq.umn.edu/misq/article/44/3/1107/1822/First-or-Second-Mover-Advantage-The-Case-of-IT\n",
            "   Saved: 4 author rows\n",
            "[787/800] https://misq.umn.edu/misq/article/44/3/1143/1806/The-Interplay-of-IT-Users-Coping-Strategies\n",
            "Verification detected. Solve it in the opened browser window; I'll continue once the article loads.\n",
            "   Saved: 3 author rows\n",
            "[788/800] https://misq.umn.edu/misq/article/44/3/1177/1811/Platform-Signaling-for-Generating-Platform\n",
            "   Saved: 4 author rows\n",
            "[789/800] https://misq.umn.edu/misq/article/44/3/1207/1820/Disentangling-the-Impact-of-Omnichannel\n",
            "   Saved: 3 author rows\n",
            "[790/800] https://misq.umn.edu/misq/article/44/3/1259/1814/A-Daily-Field-Investigation-of-Technology-Driven\n",
            "   Saved: 1 author rows\n",
            "[791/800] https://misq.umn.edu/misq/article/44/3/1301/1794/Social-Media-and-Selection-Political-Issue\n",
            "   Saved: 4 author rows\n",
            "[792/800] https://misq.umn.edu/misq/article/44/3/1359/1798/How-Information-Technology-Matters-in-Societal\n",
            "   Saved: 3 author rows\n",
            "[793/800] https://misq.umn.edu/misq/article/44/3/1391/1801/The-Effect-of-Shortening-Lock-in-Periods-in\n",
            "   Saved: 3 author rows\n",
            "[794/800] https://misq.umn.edu/misq/article/44/3/1411/1804/The-Multiplex-Nature-of-the-Customer\n",
            "   Saved: 2 author rows\n",
            "[795/800] https://misq.umn.edu/misq/article/44/3/1439/1809/The-Takeoff-of-Open-Source-Software-A-Signaling\n",
            "   Saved: 3 author rows\n",
            "[796/800] https://misq.umn.edu/misq/article/44/3/iii/1796/Editor-s-CommentsReplication-Crisis-or-Replication\n",
            "   Saved: 4 author rows\n",
            "[797/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18511/3786/Trustworthiness-in-Computational-Theory\n",
            "   Saved: 3 author rows\n",
            "[798/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/18708/3771/When-Bots-Evaluate-Humans-Delegation-to-Bots-and\n",
            "   Saved: 5 author rows\n",
            "[799/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2026/17971/3766/FAIR-A-Design-Theory-for-Artificial-Intelligence\n",
            "   Saved: 3 author rows\n",
            "[800/800] https://misq.umn.edu/misq/article/doi/10.25300/MISQ/2025/18897/3770/Symptoms-and-Their-Temporal-Distributions-An\n",
            "   Saved: 3 author rows\n",
            "\n",
            "Done. Output saved to: c:\\Users\\t276m996\\Downloads\\Journals_data\\Journals_data\\InformationSystems\\MIS Quarterly\\MISQ_article_data.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv, time, os, random, re\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "START_INDEX = 700\n",
        "END_INDEX = 800\n",
        "WAIT_SEC = 15\n",
        "\n",
        "CSV_PATH = os.path.join(os.getcwd(), \"MISQ_Issues.csv\")\n",
        "OUT_FILE = os.path.join(os.getcwd(), \"MISQ_article_data.csv\")\n",
        "JOURNAL_TITLE = \"MIS Quarterly\"\n",
        "\n",
        "def clean_text(s):\n",
        "    if not s:\n",
        "        return \"N/A\"\n",
        "    s = re.sub(r\"\\s+\", \" \", str(s).strip())\n",
        "    return s if s else \"N/A\"\n",
        "\n",
        "def parse_month_year(date_str):\n",
        "    if not date_str:\n",
        "        return \"N/A\"\n",
        "    date_str = date_str.strip()\n",
        "    for fmt in (\"%Y/%m/%d\", \"%Y-%m-%d\", \"%b %d, %Y\", \"%B %d, %Y\"):\n",
        "        try:\n",
        "            d = datetime.strptime(date_str, fmt)\n",
        "            return d.strftime(\"%B %Y\")\n",
        "        except:\n",
        "            pass\n",
        "    return clean_text(date_str)\n",
        "\n",
        "def get_driver():\n",
        "    opts = Options()\n",
        "\n",
        "    # Windows-safe persistent profile path (in your repo folder)\n",
        "    profile_dir = os.path.join(os.getcwd(), \"chrome_profile_misq\")\n",
        "    opts.add_argument(f\"--user-data-dir={profile_dir}\")\n",
        "\n",
        "    # IMPORTANT: allow images so captcha/verification can render\n",
        "    prefs = {\"profile.managed_default_content_settings.images\": 1}\n",
        "    opts.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "    return webdriver.Chrome(options=opts)\n",
        "\n",
        "def extract_article_details(html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    meta = {}\n",
        "    for m in soup.find_all(\"meta\"):\n",
        "        n = m.get(\"name\")\n",
        "        c = m.get(\"content\")\n",
        "        if n and c:\n",
        "            meta[n.strip()] = c.strip()\n",
        "\n",
        "    title = clean_text(meta.get(\"citation_title\"))\n",
        "    if title == \"N/A\":\n",
        "        h1 = soup.select_one(\"h1\")\n",
        "        title = clean_text(h1.get_text()) if h1 else \"N/A\"\n",
        "\n",
        "    abstract = \"N/A\"\n",
        "    if meta.get(\"citation_abstract\"):\n",
        "        abstract = clean_text(meta.get(\"citation_abstract\"))\n",
        "    else:\n",
        "        abs_el = soup.select_one(\".abstract, .abstract-text, #abstract, .article-abstract\")\n",
        "        if abs_el:\n",
        "            abstract = clean_text(abs_el.get_text())\n",
        "\n",
        "    kw = []\n",
        "    for k in soup.find_all(\"meta\", attrs={\"name\": \"citation_keywords\"}):\n",
        "        if k.get(\"content\"):\n",
        "            kw.append(clean_text(k.get(\"content\")))\n",
        "    keywords = clean_text(str(sorted(set([x for x in kw if x and x != \"N/A\"]))))\n",
        "\n",
        "    vol = clean_text(meta.get(\"citation_volume\"))\n",
        "    issue = clean_text(meta.get(\"citation_issue\"))\n",
        "    volume_issue = \"N/A\"\n",
        "    if vol != \"N/A\" and issue != \"N/A\":\n",
        "        volume_issue = f\"Volume {vol}, Issue {issue}\"\n",
        "    elif vol != \"N/A\":\n",
        "        volume_issue = f\"Volume {vol}\"\n",
        "    elif issue != \"N/A\":\n",
        "        volume_issue = f\"Issue {issue}\"\n",
        "\n",
        "    pub_date = meta.get(\"citation_publication_date\") or meta.get(\"citation_date\")\n",
        "    month_year = parse_month_year(pub_date)\n",
        "\n",
        "    return title, abstract, keywords, volume_issue, month_year\n",
        "\n",
        "def extract_authors_hover(driver):\n",
        "    def _extract_email(text):\n",
        "        if not text:\n",
        "            return \"N/A\"\n",
        "        m = re.search(r\"[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\", text, flags=re.I)\n",
        "        return clean_text(m.group(0)) if m else \"N/A\"\n",
        "\n",
        "    def _dedupe(items):\n",
        "        uniq, seen = [], set()\n",
        "        for a in items:\n",
        "            key = (a.get(\"name\"), a.get(\"email\"), a.get(\"address\"))\n",
        "            if key in seen:\n",
        "                continue\n",
        "            seen.add(key)\n",
        "            uniq.append(a)\n",
        "        return uniq\n",
        "\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    meta_authors = [m.get(\"content\", \"\").strip() for m in soup.select('meta[name=\"citation_author\"]') if m.get(\"content\")]\n",
        "    meta_insts = [m.get(\"content\", \"\").strip() for m in soup.select('meta[name=\"citation_author_institution\"]') if m.get(\"content\")]\n",
        "    meta_emails = [m.get(\"content\", \"\").strip() for m in soup.select('meta[name=\"citation_author_email\"]') if m.get(\"content\")]\n",
        "\n",
        "    if meta_authors and (meta_insts or meta_emails):\n",
        "        out = []\n",
        "        for i, name in enumerate(meta_authors):\n",
        "            inst = meta_insts[i] if i < len(meta_insts) else \"N/A\"\n",
        "            em = meta_emails[i] if i < len(meta_emails) else \"N/A\"\n",
        "            out.append({\"name\": clean_text(name), \"email\": clean_text(em), \"address\": clean_text(inst)})\n",
        "        return _dedupe([a for a in out if a[\"name\"] != \"N/A\"])\n",
        "\n",
        "    actions = ActionChains(driver)\n",
        "\n",
        "    author_els = driver.find_elements(\n",
        "        By.CSS_SELECTOR,\n",
        "        \"a[href*='/misq/author/'], .article-authors a, .authors a, a.article-author\"\n",
        "    )\n",
        "    if not author_els:\n",
        "        return []\n",
        "\n",
        "    tip_selectors = [\n",
        "        \"[role='tooltip']\",\n",
        "        \".tooltipster-base\",\n",
        "        \".tooltipster-content\",\n",
        "        \".tippy-box\",\n",
        "        \".tippy-content\",\n",
        "        \".ui-tooltip\",\n",
        "        \".tooltip\",\n",
        "        \".popover\",\n",
        "        \".popover-body\",\n",
        "    ]\n",
        "\n",
        "    def _get_visible_tooltip_text():\n",
        "        for sel in tip_selectors:\n",
        "            tips = driver.find_elements(By.CSS_SELECTOR, sel)\n",
        "            vis = [t for t in tips if t.is_displayed() and (t.text or \"\").strip()]\n",
        "            if vis:\n",
        "                return vis[-1].text.strip()\n",
        "        return \"\"\n",
        "\n",
        "    results = []\n",
        "    for el in author_els:\n",
        "        name = clean_text(el.text)\n",
        "        if name == \"N/A\":\n",
        "            continue\n",
        "\n",
        "        email = \"N/A\"\n",
        "        address = \"N/A\"\n",
        "\n",
        "        try:\n",
        "            href = el.get_attribute(\"href\") or \"\"\n",
        "            if \"mailto:\" in href:\n",
        "                email = clean_text(href.replace(\"mailto:\", \"\").split(\"?\")[0])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        trigger = None\n",
        "        try:\n",
        "            parent = el.find_element(By.XPATH, \"./parent::*\")\n",
        "            cand = parent.find_elements(\n",
        "                By.CSS_SELECTOR,\n",
        "                \"button[data-toggle='tooltip'], button[data-bs-toggle='tooltip'], [data-tippy-content], [data-original-title], a[title], button[title], span[title]\"\n",
        "            )\n",
        "            trigger = cand[0] if cand else None\n",
        "        except:\n",
        "            trigger = None\n",
        "\n",
        "        if trigger is None:\n",
        "            try:\n",
        "                gp = el.find_element(By.XPATH, \"./ancestor::*[self::li or self::div or self::span][1]\")\n",
        "                cand = gp.find_elements(By.CSS_SELECTOR, \"button, a, span, i, svg\")\n",
        "                for c in cand:\n",
        "                    attrs = [\n",
        "                        c.get_attribute(\"data-tippy-content\"),\n",
        "                        c.get_attribute(\"data-original-title\"),\n",
        "                        c.get_attribute(\"title\"),\n",
        "                        c.get_attribute(\"aria-label\"),\n",
        "                    ]\n",
        "                    if any(x and str(x).strip() for x in attrs):\n",
        "                        trigger = c\n",
        "                        break\n",
        "            except:\n",
        "                trigger = None\n",
        "\n",
        "        tip_text = \"\"\n",
        "\n",
        "        if trigger is not None:\n",
        "            for attr in [\"data-tippy-content\", \"data-original-title\", \"title\", \"aria-label\"]:\n",
        "                try:\n",
        "                    v = trigger.get_attribute(attr)\n",
        "                    if v and str(v).strip():\n",
        "                        tip_text = str(v).strip()\n",
        "                        break\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        if not tip_text:\n",
        "            try:\n",
        "                if trigger is not None:\n",
        "                    try:\n",
        "                        actions.move_to_element(trigger).pause(0.4).perform()\n",
        "                    except:\n",
        "                        actions.move_to_element(el).pause(0.4).perform()\n",
        "                    try:\n",
        "                        trigger.click()\n",
        "                        time.sleep(0.2)\n",
        "                    except:\n",
        "                        pass\n",
        "                else:\n",
        "                    actions.move_to_element(el).pause(0.4).perform()\n",
        "\n",
        "                WebDriverWait(driver, 2).until(lambda d: bool(_get_visible_tooltip_text()))\n",
        "                tip_text = _get_visible_tooltip_text()\n",
        "            except:\n",
        "                tip_text = _get_visible_tooltip_text()\n",
        "\n",
        "        if tip_text:\n",
        "            if \"<\" in tip_text and \">\" in tip_text:\n",
        "                try:\n",
        "                    tip_text = BeautifulSoup(tip_text, \"html.parser\").get_text(\"\\n\")\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            lines = [x.strip() for x in tip_text.split(\"\\n\") if x.strip()]\n",
        "            if lines and lines[0].lower() == name.lower():\n",
        "                lines = lines[1:]\n",
        "\n",
        "            joined = \" \".join(lines)\n",
        "            found_email = _extract_email(joined)\n",
        "            if email == \"N/A\" and found_email != \"N/A\":\n",
        "                email = found_email\n",
        "\n",
        "            joined = re.sub(r\"\\bSearch for other works\\b.*\", \"\", joined, flags=re.I).strip()\n",
        "            if found_email != \"N/A\":\n",
        "                joined = re.sub(re.escape(found_email), \"\", joined, flags=re.I).strip(\" -|,;\\t\\n\")\n",
        "\n",
        "            address = clean_text(joined) if joined else address\n",
        "\n",
        "        results.append({\"name\": name, \"email\": email, \"address\": address})\n",
        "\n",
        "    return _dedupe(results)\n",
        "\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    print(f\"Error: {CSV_PATH} not found.\")\n",
        "    raise SystemExit\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "if not os.path.exists(OUT_FILE):\n",
        "    with open(OUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\n",
        "            \"URL\",\n",
        "            \"Journal_Title\",\n",
        "            \"Article_Title\",\n",
        "            \"Volume_Issue\",\n",
        "            \"Month_Year\",\n",
        "            \"Abstract\",\n",
        "            \"Keywords\",\n",
        "            \"Author_name\",\n",
        "            \"Author_email\",\n",
        "            \"Author_Address\"\n",
        "        ])\n",
        "\n",
        "driver = get_driver()\n",
        "wait = WebDriverWait(driver, WAIT_SEC)\n",
        "\n",
        "try:\n",
        "    for i in range(START_INDEX, min(END_INDEX, len(df))):\n",
        "        row = df.iloc[i]\n",
        "        url = str(row.get(\"URL\", \"\")).strip()\n",
        "        if not url:\n",
        "            continue\n",
        "\n",
        "        print(f\"[{i+1}/{min(END_INDEX, len(df))}] {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        time.sleep(random.uniform(8, 15))\n",
        "\n",
        "        def _looks_like_cloudflare_challenge(d):\n",
        "            try:\n",
        "                u = (d.current_url or \"\").lower()\n",
        "                t = (d.title or \"\").lower()\n",
        "                if \"/cdn-cgi/\" in u:\n",
        "                    return True\n",
        "                if \"just a moment\" in t:\n",
        "                    return True\n",
        "                src = (d.page_source or \"\").lower()\n",
        "                # Common Cloudflare / Turnstile markers\n",
        "                if \"cf-turnstile\" in src:\n",
        "                    return True\n",
        "                if \"challenge-platform\" in src:\n",
        "                    return True\n",
        "                if \"cf_chl\" in src:\n",
        "                    return True\n",
        "            except:\n",
        "                return False\n",
        "            return False\n",
        "\n",
        "        # If verification shows up, let you solve it and continue automatically\n",
        "        if _looks_like_cloudflare_challenge(driver):\n",
        "            print(\"Verification detected. Solve it in the opened browser window; I'll continue once the article loads.\")\n",
        "            try:\n",
        "                WebDriverWait(driver, 300).until(lambda d: not _looks_like_cloudflare_challenge(d))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Proceed once the real article page is present\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"h1\")))\n",
        "\n",
        "        html = driver.page_source\n",
        "        article_title, abstract, keywords, volume_issue, month_year = extract_article_details(html)\n",
        "\n",
        "        authors = extract_authors_hover(driver)\n",
        "        if not authors:\n",
        "            authors = [{\"name\": \"N/A\", \"email\": \"N/A\", \"address\": \"N/A\"}]\n",
        "\n",
        "        with open(OUT_FILE, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            w = csv.writer(f)\n",
        "            for a in authors:\n",
        "                w.writerow([\n",
        "                    url,\n",
        "                    JOURNAL_TITLE,\n",
        "                    article_title,\n",
        "                    volume_issue,\n",
        "                    month_year,\n",
        "                    abstract,\n",
        "                    keywords,\n",
        "                    a[\"name\"],\n",
        "                    a[\"email\"],\n",
        "                    a[\"address\"]\n",
        "                ])\n",
        "\n",
        "        print(f\"   Saved: {len(authors)} author rows\")\n",
        "\n",
        "finally:\n",
        "    driver.quit()\n",
        "    print(f\"\\nDone. Output saved to: {OUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77b091bc",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0decfb17",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
